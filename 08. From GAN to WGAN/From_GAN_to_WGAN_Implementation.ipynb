{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From_GAN_to_WGAN_Implementation\n",
    "\n",
    "with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import*\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2,PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. View MNIST DATASET (Just See)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train),(X_test,y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32).reshape(-1, 28,28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28,28) / 255.0\n",
    "\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[5000:]\n",
    "X_valid = X_train[:5000]\n",
    "\n",
    "y_train = y_train[5000:]\n",
    "y_valid = y_train[:5000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prints image\n",
    "plt.rcParams['font.family'] = 'NanumBarunGothic'\n",
    "\n",
    "plt.imshow(X_train[1])\n",
    "plt.title(\"The number is : {0}\".format(y_train[1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 16)        160       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 32)          4640      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 100,097\n",
      "Trainable params: 99,649\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 64)        131136    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 1)         1025      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,028,673\n",
      "Trainable params: 1,028,289\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.999909] [G loss: 1.000171]\n",
      "1 [D loss: 0.999919] [G loss: 1.000168]\n",
      "2 [D loss: 0.999919] [G loss: 1.000177]\n",
      "3 [D loss: 0.999921] [G loss: 1.000172]\n",
      "4 [D loss: 0.999924] [G loss: 1.000174]\n",
      "5 [D loss: 0.999924] [G loss: 1.000180]\n",
      "6 [D loss: 0.999924] [G loss: 1.000179]\n",
      "7 [D loss: 0.999927] [G loss: 1.000171]\n",
      "8 [D loss: 0.999921] [G loss: 1.000183]\n",
      "9 [D loss: 0.999921] [G loss: 1.000190]\n",
      "10 [D loss: 0.999926] [G loss: 1.000189]\n",
      "11 [D loss: 0.999923] [G loss: 1.000180]\n",
      "12 [D loss: 0.999922] [G loss: 1.000181]\n",
      "13 [D loss: 0.999926] [G loss: 1.000186]\n",
      "14 [D loss: 0.999925] [G loss: 1.000187]\n",
      "15 [D loss: 0.999930] [G loss: 1.000197]\n",
      "16 [D loss: 0.999928] [G loss: 1.000195]\n",
      "17 [D loss: 0.999930] [G loss: 1.000198]\n",
      "18 [D loss: 0.999932] [G loss: 1.000186]\n",
      "19 [D loss: 0.999935] [G loss: 1.000190]\n",
      "20 [D loss: 0.999939] [G loss: 1.000180]\n",
      "21 [D loss: 0.999943] [G loss: 1.000160]\n",
      "22 [D loss: 0.999936] [G loss: 1.000157]\n",
      "23 [D loss: 0.999935] [G loss: 1.000147]\n",
      "24 [D loss: 0.999947] [G loss: 1.000165]\n",
      "25 [D loss: 0.999941] [G loss: 1.000155]\n",
      "26 [D loss: 0.999951] [G loss: 1.000139]\n",
      "27 [D loss: 0.999949] [G loss: 1.000145]\n",
      "28 [D loss: 0.999951] [G loss: 1.000129]\n",
      "29 [D loss: 0.999940] [G loss: 1.000144]\n",
      "30 [D loss: 0.999942] [G loss: 1.000138]\n",
      "31 [D loss: 0.999941] [G loss: 1.000129]\n",
      "32 [D loss: 0.999946] [G loss: 1.000148]\n",
      "33 [D loss: 0.999952] [G loss: 1.000137]\n",
      "34 [D loss: 0.999951] [G loss: 1.000142]\n",
      "35 [D loss: 0.999955] [G loss: 1.000109]\n",
      "36 [D loss: 0.999935] [G loss: 1.000124]\n",
      "37 [D loss: 0.999961] [G loss: 1.000136]\n",
      "38 [D loss: 0.999967] [G loss: 1.000137]\n",
      "39 [D loss: 0.999958] [G loss: 1.000133]\n",
      "40 [D loss: 0.999956] [G loss: 1.000136]\n",
      "41 [D loss: 0.999965] [G loss: 1.000126]\n",
      "42 [D loss: 0.999962] [G loss: 1.000137]\n",
      "43 [D loss: 0.999977] [G loss: 1.000125]\n",
      "44 [D loss: 0.999951] [G loss: 1.000127]\n",
      "45 [D loss: 0.999962] [G loss: 1.000142]\n",
      "46 [D loss: 0.999963] [G loss: 1.000128]\n",
      "47 [D loss: 0.999950] [G loss: 1.000123]\n",
      "48 [D loss: 0.999977] [G loss: 1.000118]\n",
      "49 [D loss: 0.999950] [G loss: 1.000133]\n",
      "50 [D loss: 0.999943] [G loss: 1.000132]\n",
      "51 [D loss: 0.999955] [G loss: 1.000156]\n",
      "52 [D loss: 0.999984] [G loss: 1.000120]\n",
      "53 [D loss: 0.999994] [G loss: 1.000105]\n",
      "54 [D loss: 0.999975] [G loss: 1.000084]\n",
      "55 [D loss: 0.999959] [G loss: 1.000134]\n",
      "56 [D loss: 0.999975] [G loss: 1.000127]\n",
      "57 [D loss: 0.999987] [G loss: 1.000116]\n",
      "58 [D loss: 0.999949] [G loss: 1.000109]\n",
      "59 [D loss: 0.999985] [G loss: 1.000139]\n",
      "60 [D loss: 0.999989] [G loss: 1.000130]\n",
      "61 [D loss: 0.999986] [G loss: 1.000123]\n",
      "62 [D loss: 0.999954] [G loss: 1.000138]\n",
      "63 [D loss: 0.999976] [G loss: 1.000114]\n",
      "64 [D loss: 0.999981] [G loss: 1.000147]\n",
      "65 [D loss: 0.999988] [G loss: 1.000165]\n",
      "66 [D loss: 0.999974] [G loss: 1.000125]\n",
      "67 [D loss: 0.999963] [G loss: 1.000143]\n",
      "68 [D loss: 0.999973] [G loss: 1.000144]\n",
      "69 [D loss: 0.999970] [G loss: 1.000154]\n",
      "70 [D loss: 0.999970] [G loss: 1.000160]\n",
      "71 [D loss: 0.999983] [G loss: 1.000137]\n",
      "72 [D loss: 0.999976] [G loss: 1.000158]\n",
      "73 [D loss: 0.999981] [G loss: 1.000158]\n",
      "74 [D loss: 0.999965] [G loss: 1.000157]\n",
      "75 [D loss: 0.999964] [G loss: 1.000160]\n",
      "76 [D loss: 0.999954] [G loss: 1.000229]\n",
      "77 [D loss: 0.999940] [G loss: 1.000207]\n",
      "78 [D loss: 0.999969] [G loss: 1.000226]\n",
      "79 [D loss: 0.999954] [G loss: 1.000206]\n",
      "80 [D loss: 0.999964] [G loss: 1.000213]\n",
      "81 [D loss: 0.999955] [G loss: 1.000202]\n",
      "82 [D loss: 0.999945] [G loss: 1.000217]\n",
      "83 [D loss: 0.999951] [G loss: 1.000169]\n",
      "84 [D loss: 0.999965] [G loss: 1.000204]\n",
      "85 [D loss: 0.999951] [G loss: 1.000226]\n",
      "86 [D loss: 0.999952] [G loss: 1.000170]\n",
      "87 [D loss: 0.999936] [G loss: 1.000192]\n",
      "88 [D loss: 0.999941] [G loss: 1.000172]\n",
      "89 [D loss: 0.999950] [G loss: 1.000182]\n",
      "90 [D loss: 0.999980] [G loss: 1.000216]\n",
      "91 [D loss: 0.999950] [G loss: 1.000174]\n",
      "92 [D loss: 0.999964] [G loss: 1.000174]\n",
      "93 [D loss: 0.999939] [G loss: 1.000238]\n",
      "94 [D loss: 0.999985] [G loss: 1.000221]\n",
      "95 [D loss: 0.999969] [G loss: 1.000180]\n",
      "96 [D loss: 0.999967] [G loss: 1.000177]\n",
      "97 [D loss: 0.999961] [G loss: 1.000207]\n",
      "98 [D loss: 0.999936] [G loss: 1.000129]\n",
      "99 [D loss: 0.999970] [G loss: 1.000199]\n",
      "100 [D loss: 0.999949] [G loss: 1.000132]\n",
      "101 [D loss: 0.999912] [G loss: 1.000195]\n",
      "102 [D loss: 0.999954] [G loss: 1.000201]\n",
      "103 [D loss: 0.999957] [G loss: 1.000136]\n",
      "104 [D loss: 0.999951] [G loss: 1.000181]\n",
      "105 [D loss: 0.999949] [G loss: 1.000187]\n",
      "106 [D loss: 0.999943] [G loss: 1.000131]\n",
      "107 [D loss: 0.999926] [G loss: 1.000101]\n",
      "108 [D loss: 0.999947] [G loss: 1.000136]\n",
      "109 [D loss: 0.999975] [G loss: 1.000094]\n",
      "110 [D loss: 0.999943] [G loss: 1.000079]\n",
      "111 [D loss: 0.999904] [G loss: 1.000138]\n",
      "112 [D loss: 0.999924] [G loss: 1.000091]\n",
      "113 [D loss: 0.999971] [G loss: 1.000148]\n",
      "114 [D loss: 0.999938] [G loss: 1.000143]\n",
      "115 [D loss: 0.999947] [G loss: 1.000124]\n",
      "116 [D loss: 0.999908] [G loss: 1.000113]\n",
      "117 [D loss: 0.999930] [G loss: 1.000085]\n",
      "118 [D loss: 0.999940] [G loss: 1.000125]\n",
      "119 [D loss: 0.999947] [G loss: 1.000089]\n",
      "120 [D loss: 0.999925] [G loss: 1.000135]\n",
      "121 [D loss: 0.999919] [G loss: 1.000076]\n",
      "122 [D loss: 0.999948] [G loss: 1.000110]\n",
      "123 [D loss: 0.999972] [G loss: 1.000129]\n",
      "124 [D loss: 0.999959] [G loss: 1.000055]\n",
      "125 [D loss: 0.999937] [G loss: 1.000120]\n",
      "126 [D loss: 0.999948] [G loss: 1.000068]\n",
      "127 [D loss: 0.999945] [G loss: 1.000026]\n",
      "128 [D loss: 0.999952] [G loss: 1.000105]\n",
      "129 [D loss: 0.999940] [G loss: 1.000113]\n",
      "130 [D loss: 0.999932] [G loss: 1.000092]\n",
      "131 [D loss: 0.999974] [G loss: 1.000120]\n",
      "132 [D loss: 1.000001] [G loss: 1.000079]\n",
      "133 [D loss: 0.999937] [G loss: 1.000039]\n",
      "134 [D loss: 0.999938] [G loss: 1.000064]\n",
      "135 [D loss: 0.999945] [G loss: 1.000086]\n",
      "136 [D loss: 0.999911] [G loss: 1.000078]\n",
      "137 [D loss: 0.999954] [G loss: 1.000062]\n",
      "138 [D loss: 0.999954] [G loss: 1.000082]\n",
      "139 [D loss: 0.999881] [G loss: 1.000038]\n",
      "140 [D loss: 0.999915] [G loss: 1.000059]\n",
      "141 [D loss: 0.999935] [G loss: 1.000032]\n",
      "142 [D loss: 0.999908] [G loss: 1.000116]\n",
      "143 [D loss: 0.999969] [G loss: 1.000067]\n",
      "144 [D loss: 0.999890] [G loss: 1.000074]\n",
      "145 [D loss: 0.999985] [G loss: 1.000083]\n",
      "146 [D loss: 0.999912] [G loss: 1.000025]\n",
      "147 [D loss: 0.999973] [G loss: 1.000055]\n",
      "148 [D loss: 0.999926] [G loss: 1.000060]\n",
      "149 [D loss: 0.999928] [G loss: 1.000100]\n",
      "150 [D loss: 0.999942] [G loss: 1.000062]\n",
      "151 [D loss: 0.999976] [G loss: 1.000063]\n",
      "152 [D loss: 0.999950] [G loss: 1.000053]\n",
      "153 [D loss: 0.999955] [G loss: 1.000055]\n",
      "154 [D loss: 0.999920] [G loss: 1.000078]\n",
      "155 [D loss: 0.999925] [G loss: 1.000041]\n",
      "156 [D loss: 0.999945] [G loss: 1.000031]\n",
      "157 [D loss: 0.999964] [G loss: 1.000040]\n",
      "158 [D loss: 0.999955] [G loss: 1.000105]\n",
      "159 [D loss: 0.999976] [G loss: 1.000062]\n",
      "160 [D loss: 0.999981] [G loss: 1.000090]\n",
      "161 [D loss: 0.999896] [G loss: 1.000049]\n",
      "162 [D loss: 0.999908] [G loss: 1.000058]\n",
      "163 [D loss: 0.999957] [G loss: 1.000045]\n",
      "164 [D loss: 0.999917] [G loss: 1.000056]\n",
      "165 [D loss: 0.999932] [G loss: 1.000020]\n",
      "166 [D loss: 0.999922] [G loss: 1.000080]\n",
      "167 [D loss: 0.999921] [G loss: 1.000042]\n",
      "168 [D loss: 0.999955] [G loss: 1.000075]\n",
      "169 [D loss: 0.999937] [G loss: 1.000047]\n",
      "170 [D loss: 0.999920] [G loss: 1.000003]\n",
      "171 [D loss: 0.999940] [G loss: 1.000010]\n",
      "172 [D loss: 0.999935] [G loss: 1.000018]\n",
      "173 [D loss: 0.999950] [G loss: 1.000034]\n",
      "174 [D loss: 0.999939] [G loss: 1.000006]\n",
      "175 [D loss: 0.999932] [G loss: 1.000057]\n",
      "176 [D loss: 0.999956] [G loss: 1.000045]\n",
      "177 [D loss: 0.999971] [G loss: 1.000026]\n",
      "178 [D loss: 0.999917] [G loss: 1.000094]\n",
      "179 [D loss: 0.999932] [G loss: 1.000038]\n",
      "180 [D loss: 0.999940] [G loss: 1.000049]\n",
      "181 [D loss: 0.999963] [G loss: 1.000064]\n",
      "182 [D loss: 0.999941] [G loss: 1.000078]\n",
      "183 [D loss: 0.999937] [G loss: 1.000007]\n",
      "184 [D loss: 0.999895] [G loss: 0.999992]\n",
      "185 [D loss: 0.999942] [G loss: 0.999926]\n",
      "186 [D loss: 0.999912] [G loss: 1.000112]\n",
      "187 [D loss: 0.999976] [G loss: 0.999980]\n",
      "188 [D loss: 0.999942] [G loss: 1.000103]\n",
      "189 [D loss: 0.999990] [G loss: 1.000068]\n",
      "190 [D loss: 0.999924] [G loss: 1.000077]\n",
      "191 [D loss: 0.999896] [G loss: 1.000054]\n",
      "192 [D loss: 0.999944] [G loss: 1.000001]\n",
      "193 [D loss: 0.999923] [G loss: 1.000048]\n",
      "194 [D loss: 0.999969] [G loss: 1.000086]\n",
      "195 [D loss: 0.999899] [G loss: 1.000041]\n",
      "196 [D loss: 0.999929] [G loss: 1.000011]\n",
      "197 [D loss: 0.999925] [G loss: 1.000075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 [D loss: 0.999926] [G loss: 0.999989]\n",
      "199 [D loss: 0.999923] [G loss: 1.000020]\n",
      "200 [D loss: 0.999990] [G loss: 1.000042]\n",
      "201 [D loss: 0.999965] [G loss: 1.000077]\n",
      "202 [D loss: 0.999924] [G loss: 1.000052]\n",
      "203 [D loss: 0.999978] [G loss: 1.000066]\n",
      "204 [D loss: 1.000022] [G loss: 1.000025]\n",
      "205 [D loss: 0.999900] [G loss: 1.000059]\n",
      "206 [D loss: 0.999940] [G loss: 1.000069]\n",
      "207 [D loss: 0.999959] [G loss: 1.000075]\n",
      "208 [D loss: 0.999940] [G loss: 1.000017]\n",
      "209 [D loss: 0.999970] [G loss: 1.000005]\n",
      "210 [D loss: 0.999944] [G loss: 1.000104]\n",
      "211 [D loss: 0.999936] [G loss: 1.000059]\n",
      "212 [D loss: 0.999919] [G loss: 1.000024]\n",
      "213 [D loss: 0.999969] [G loss: 1.000068]\n",
      "214 [D loss: 0.999987] [G loss: 1.000026]\n",
      "215 [D loss: 0.999938] [G loss: 0.999995]\n",
      "216 [D loss: 0.999973] [G loss: 1.000052]\n",
      "217 [D loss: 0.999943] [G loss: 1.000005]\n",
      "218 [D loss: 0.999938] [G loss: 1.000020]\n",
      "219 [D loss: 1.000023] [G loss: 1.000043]\n",
      "220 [D loss: 0.999941] [G loss: 1.000069]\n",
      "221 [D loss: 0.999906] [G loss: 1.000083]\n",
      "222 [D loss: 0.999943] [G loss: 1.000020]\n",
      "223 [D loss: 0.999984] [G loss: 1.000078]\n",
      "224 [D loss: 0.999907] [G loss: 0.999965]\n",
      "225 [D loss: 0.999930] [G loss: 1.000033]\n",
      "226 [D loss: 0.999973] [G loss: 1.000041]\n",
      "227 [D loss: 0.999908] [G loss: 1.000035]\n",
      "228 [D loss: 0.999951] [G loss: 1.000014]\n",
      "229 [D loss: 0.999933] [G loss: 1.000056]\n",
      "230 [D loss: 0.999972] [G loss: 1.000109]\n",
      "231 [D loss: 0.999975] [G loss: 1.000044]\n",
      "232 [D loss: 0.999977] [G loss: 0.999981]\n",
      "233 [D loss: 0.999931] [G loss: 1.000121]\n",
      "234 [D loss: 0.999950] [G loss: 1.000027]\n",
      "235 [D loss: 0.999967] [G loss: 1.000053]\n",
      "236 [D loss: 0.999907] [G loss: 1.000080]\n",
      "237 [D loss: 0.999925] [G loss: 1.000025]\n",
      "238 [D loss: 0.999954] [G loss: 1.000062]\n",
      "239 [D loss: 0.999949] [G loss: 1.000089]\n",
      "240 [D loss: 0.999930] [G loss: 1.000069]\n",
      "241 [D loss: 0.999941] [G loss: 1.000028]\n",
      "242 [D loss: 0.999965] [G loss: 1.000063]\n",
      "243 [D loss: 0.999921] [G loss: 1.000114]\n",
      "244 [D loss: 0.999950] [G loss: 1.000069]\n",
      "245 [D loss: 0.999900] [G loss: 1.000055]\n",
      "246 [D loss: 0.999941] [G loss: 1.000099]\n",
      "247 [D loss: 0.999905] [G loss: 1.000099]\n",
      "248 [D loss: 0.999958] [G loss: 1.000092]\n",
      "249 [D loss: 0.999959] [G loss: 1.000132]\n",
      "250 [D loss: 0.999969] [G loss: 1.000080]\n",
      "251 [D loss: 0.999959] [G loss: 1.000062]\n",
      "252 [D loss: 0.999934] [G loss: 1.000050]\n",
      "253 [D loss: 0.999942] [G loss: 1.000059]\n",
      "254 [D loss: 0.999968] [G loss: 1.000034]\n",
      "255 [D loss: 0.999952] [G loss: 1.000070]\n",
      "256 [D loss: 0.999930] [G loss: 1.000005]\n",
      "257 [D loss: 0.999948] [G loss: 1.000027]\n",
      "258 [D loss: 0.999958] [G loss: 1.000073]\n",
      "259 [D loss: 0.999930] [G loss: 1.000017]\n",
      "260 [D loss: 0.999929] [G loss: 1.000071]\n",
      "261 [D loss: 0.999929] [G loss: 1.000023]\n",
      "262 [D loss: 0.999974] [G loss: 1.000030]\n",
      "263 [D loss: 0.999963] [G loss: 1.000054]\n",
      "264 [D loss: 0.999931] [G loss: 1.000081]\n",
      "265 [D loss: 0.999949] [G loss: 1.000052]\n",
      "266 [D loss: 0.999905] [G loss: 1.000042]\n",
      "267 [D loss: 0.999942] [G loss: 1.000027]\n",
      "268 [D loss: 0.999949] [G loss: 1.000046]\n",
      "269 [D loss: 0.999926] [G loss: 1.000051]\n",
      "270 [D loss: 0.999986] [G loss: 1.000044]\n",
      "271 [D loss: 0.999981] [G loss: 1.000049]\n",
      "272 [D loss: 0.999962] [G loss: 1.000064]\n",
      "273 [D loss: 0.999954] [G loss: 1.000060]\n",
      "274 [D loss: 0.999952] [G loss: 1.000091]\n",
      "275 [D loss: 0.999934] [G loss: 1.000096]\n",
      "276 [D loss: 0.999924] [G loss: 1.000042]\n",
      "277 [D loss: 0.999968] [G loss: 1.000057]\n",
      "278 [D loss: 0.999951] [G loss: 1.000053]\n",
      "279 [D loss: 0.999979] [G loss: 1.000069]\n",
      "280 [D loss: 0.999952] [G loss: 1.000055]\n",
      "281 [D loss: 0.999946] [G loss: 1.000024]\n",
      "282 [D loss: 0.999957] [G loss: 1.000046]\n",
      "283 [D loss: 0.999952] [G loss: 1.000074]\n",
      "284 [D loss: 0.999972] [G loss: 1.000039]\n",
      "285 [D loss: 0.999940] [G loss: 1.000018]\n",
      "286 [D loss: 0.999977] [G loss: 1.000076]\n",
      "287 [D loss: 0.999957] [G loss: 1.000097]\n",
      "288 [D loss: 0.999940] [G loss: 1.000095]\n",
      "289 [D loss: 0.999950] [G loss: 1.000001]\n",
      "290 [D loss: 0.999905] [G loss: 1.000072]\n",
      "291 [D loss: 0.999904] [G loss: 1.000070]\n",
      "292 [D loss: 0.999945] [G loss: 1.000042]\n",
      "293 [D loss: 0.999919] [G loss: 1.000081]\n",
      "294 [D loss: 0.999898] [G loss: 1.000025]\n",
      "295 [D loss: 0.999952] [G loss: 1.000039]\n",
      "296 [D loss: 0.999890] [G loss: 1.000018]\n",
      "297 [D loss: 0.999991] [G loss: 1.000087]\n",
      "298 [D loss: 0.999973] [G loss: 1.000123]\n",
      "299 [D loss: 0.999957] [G loss: 1.000081]\n",
      "300 [D loss: 0.999944] [G loss: 1.000041]\n",
      "301 [D loss: 0.999971] [G loss: 1.000041]\n",
      "302 [D loss: 0.999875] [G loss: 1.000062]\n",
      "303 [D loss: 0.999967] [G loss: 1.000055]\n",
      "304 [D loss: 0.999959] [G loss: 1.000003]\n",
      "305 [D loss: 0.999970] [G loss: 1.000046]\n",
      "306 [D loss: 0.999888] [G loss: 1.000091]\n",
      "307 [D loss: 0.999904] [G loss: 1.000080]\n",
      "308 [D loss: 0.999929] [G loss: 1.000064]\n",
      "309 [D loss: 0.999980] [G loss: 1.000068]\n",
      "310 [D loss: 0.999922] [G loss: 0.999998]\n",
      "311 [D loss: 0.999931] [G loss: 1.000078]\n",
      "312 [D loss: 0.999954] [G loss: 1.000018]\n",
      "313 [D loss: 0.999973] [G loss: 1.000037]\n",
      "314 [D loss: 0.999955] [G loss: 1.000007]\n",
      "315 [D loss: 0.999933] [G loss: 1.000013]\n",
      "316 [D loss: 0.999921] [G loss: 1.000039]\n",
      "317 [D loss: 0.999986] [G loss: 1.000114]\n",
      "318 [D loss: 0.999930] [G loss: 1.000046]\n",
      "319 [D loss: 0.999928] [G loss: 1.000022]\n",
      "320 [D loss: 0.999936] [G loss: 0.999992]\n",
      "321 [D loss: 0.999918] [G loss: 1.000060]\n",
      "322 [D loss: 0.999932] [G loss: 0.999997]\n",
      "323 [D loss: 0.999964] [G loss: 1.000097]\n",
      "324 [D loss: 0.999963] [G loss: 1.000016]\n",
      "325 [D loss: 0.999977] [G loss: 1.000009]\n",
      "326 [D loss: 0.999928] [G loss: 0.999983]\n",
      "327 [D loss: 0.999919] [G loss: 1.000016]\n",
      "328 [D loss: 0.999980] [G loss: 1.000035]\n",
      "329 [D loss: 0.999939] [G loss: 1.000063]\n",
      "330 [D loss: 0.999958] [G loss: 1.000065]\n",
      "331 [D loss: 0.999924] [G loss: 1.000052]\n",
      "332 [D loss: 0.999949] [G loss: 1.000039]\n",
      "333 [D loss: 0.999959] [G loss: 0.999991]\n",
      "334 [D loss: 0.999976] [G loss: 1.000069]\n",
      "335 [D loss: 0.999941] [G loss: 1.000063]\n",
      "336 [D loss: 0.999983] [G loss: 1.000019]\n",
      "337 [D loss: 0.999939] [G loss: 1.000058]\n",
      "338 [D loss: 0.999947] [G loss: 1.000024]\n",
      "339 [D loss: 0.999920] [G loss: 0.999989]\n",
      "340 [D loss: 0.999977] [G loss: 1.000036]\n",
      "341 [D loss: 0.999949] [G loss: 1.000032]\n",
      "342 [D loss: 0.999920] [G loss: 1.000047]\n",
      "343 [D loss: 0.999961] [G loss: 1.000028]\n",
      "344 [D loss: 0.999951] [G loss: 1.000101]\n",
      "345 [D loss: 0.999931] [G loss: 1.000031]\n",
      "346 [D loss: 0.999907] [G loss: 1.000045]\n",
      "347 [D loss: 0.999937] [G loss: 1.000045]\n",
      "348 [D loss: 0.999993] [G loss: 1.000078]\n",
      "349 [D loss: 0.999950] [G loss: 1.000019]\n",
      "350 [D loss: 0.999920] [G loss: 1.000074]\n",
      "351 [D loss: 0.999876] [G loss: 1.000060]\n",
      "352 [D loss: 0.999926] [G loss: 1.000047]\n",
      "353 [D loss: 0.999955] [G loss: 1.000030]\n",
      "354 [D loss: 0.999944] [G loss: 1.000087]\n",
      "355 [D loss: 0.999949] [G loss: 1.000069]\n",
      "356 [D loss: 0.999993] [G loss: 1.000043]\n",
      "357 [D loss: 0.999975] [G loss: 1.000016]\n",
      "358 [D loss: 0.999936] [G loss: 1.000080]\n",
      "359 [D loss: 0.999934] [G loss: 1.000049]\n",
      "360 [D loss: 0.999922] [G loss: 1.000060]\n",
      "361 [D loss: 0.999944] [G loss: 1.000046]\n",
      "362 [D loss: 0.999978] [G loss: 1.000087]\n",
      "363 [D loss: 0.999945] [G loss: 1.000008]\n",
      "364 [D loss: 0.999954] [G loss: 1.000100]\n",
      "365 [D loss: 0.999919] [G loss: 1.000118]\n",
      "366 [D loss: 0.999996] [G loss: 1.000009]\n",
      "367 [D loss: 0.999919] [G loss: 1.000063]\n",
      "368 [D loss: 0.999934] [G loss: 1.000059]\n",
      "369 [D loss: 0.999934] [G loss: 1.000062]\n",
      "370 [D loss: 0.999906] [G loss: 1.000030]\n",
      "371 [D loss: 0.999969] [G loss: 1.000028]\n",
      "372 [D loss: 0.999966] [G loss: 1.000021]\n",
      "373 [D loss: 0.999926] [G loss: 1.000131]\n",
      "374 [D loss: 0.999947] [G loss: 1.000001]\n",
      "375 [D loss: 0.999940] [G loss: 1.000063]\n",
      "376 [D loss: 0.999954] [G loss: 1.000068]\n",
      "377 [D loss: 1.000004] [G loss: 1.000015]\n",
      "378 [D loss: 0.999952] [G loss: 1.000033]\n",
      "379 [D loss: 0.999972] [G loss: 1.000055]\n",
      "380 [D loss: 0.999949] [G loss: 1.000075]\n",
      "381 [D loss: 0.999929] [G loss: 1.000090]\n",
      "382 [D loss: 0.999944] [G loss: 1.000024]\n",
      "383 [D loss: 0.999950] [G loss: 1.000052]\n",
      "384 [D loss: 0.999953] [G loss: 1.000066]\n",
      "385 [D loss: 0.999998] [G loss: 1.000113]\n",
      "386 [D loss: 0.999949] [G loss: 1.000032]\n",
      "387 [D loss: 0.999984] [G loss: 1.000012]\n",
      "388 [D loss: 0.999950] [G loss: 1.000010]\n",
      "389 [D loss: 0.999941] [G loss: 1.000066]\n",
      "390 [D loss: 0.999912] [G loss: 1.000014]\n",
      "391 [D loss: 0.999957] [G loss: 1.000041]\n",
      "392 [D loss: 0.999964] [G loss: 1.000054]\n",
      "393 [D loss: 0.999917] [G loss: 1.000076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394 [D loss: 0.999950] [G loss: 1.000021]\n",
      "395 [D loss: 0.999965] [G loss: 1.000069]\n",
      "396 [D loss: 0.999968] [G loss: 1.000051]\n",
      "397 [D loss: 0.999957] [G loss: 1.000013]\n",
      "398 [D loss: 0.999945] [G loss: 1.000092]\n",
      "399 [D loss: 0.999928] [G loss: 1.000018]\n",
      "400 [D loss: 0.999972] [G loss: 1.000033]\n",
      "401 [D loss: 0.999964] [G loss: 1.000060]\n",
      "402 [D loss: 0.999974] [G loss: 1.000032]\n",
      "403 [D loss: 0.999944] [G loss: 1.000039]\n",
      "404 [D loss: 0.999909] [G loss: 1.000082]\n",
      "405 [D loss: 0.999936] [G loss: 1.000085]\n",
      "406 [D loss: 0.999947] [G loss: 1.000056]\n",
      "407 [D loss: 0.999955] [G loss: 1.000050]\n",
      "408 [D loss: 0.999975] [G loss: 1.000013]\n",
      "409 [D loss: 0.999970] [G loss: 1.000063]\n",
      "410 [D loss: 0.999957] [G loss: 0.999998]\n",
      "411 [D loss: 0.999919] [G loss: 1.000062]\n",
      "412 [D loss: 0.999948] [G loss: 1.000082]\n",
      "413 [D loss: 0.999936] [G loss: 1.000036]\n",
      "414 [D loss: 0.999959] [G loss: 0.999995]\n",
      "415 [D loss: 0.999959] [G loss: 1.000046]\n",
      "416 [D loss: 0.999982] [G loss: 1.000095]\n",
      "417 [D loss: 1.000001] [G loss: 1.000080]\n",
      "418 [D loss: 0.999969] [G loss: 1.000031]\n",
      "419 [D loss: 0.999927] [G loss: 1.000053]\n",
      "420 [D loss: 0.999928] [G loss: 1.000045]\n",
      "421 [D loss: 0.999927] [G loss: 1.000025]\n",
      "422 [D loss: 0.999941] [G loss: 1.000044]\n",
      "423 [D loss: 0.999957] [G loss: 1.000025]\n",
      "424 [D loss: 0.999950] [G loss: 1.000030]\n",
      "425 [D loss: 0.999953] [G loss: 1.000014]\n",
      "426 [D loss: 0.999961] [G loss: 1.000119]\n",
      "427 [D loss: 0.999950] [G loss: 1.000058]\n",
      "428 [D loss: 0.999952] [G loss: 1.000052]\n",
      "429 [D loss: 0.999972] [G loss: 1.000005]\n",
      "430 [D loss: 0.999916] [G loss: 1.000078]\n",
      "431 [D loss: 0.999928] [G loss: 1.000068]\n",
      "432 [D loss: 0.999974] [G loss: 0.999996]\n",
      "433 [D loss: 0.999943] [G loss: 1.000071]\n",
      "434 [D loss: 0.999909] [G loss: 1.000031]\n",
      "435 [D loss: 0.999951] [G loss: 1.000042]\n",
      "436 [D loss: 0.999916] [G loss: 1.000072]\n",
      "437 [D loss: 0.999948] [G loss: 1.000035]\n",
      "438 [D loss: 0.999954] [G loss: 1.000010]\n",
      "439 [D loss: 0.999976] [G loss: 1.000019]\n",
      "440 [D loss: 0.999956] [G loss: 1.000022]\n",
      "441 [D loss: 0.999926] [G loss: 1.000098]\n",
      "442 [D loss: 0.999970] [G loss: 1.000075]\n",
      "443 [D loss: 0.999945] [G loss: 1.000030]\n",
      "444 [D loss: 0.999945] [G loss: 1.000039]\n",
      "445 [D loss: 0.999920] [G loss: 1.000063]\n",
      "446 [D loss: 0.999928] [G loss: 1.000035]\n",
      "447 [D loss: 0.999921] [G loss: 1.000047]\n",
      "448 [D loss: 0.999943] [G loss: 1.000177]\n",
      "449 [D loss: 0.999969] [G loss: 1.000062]\n",
      "450 [D loss: 0.999948] [G loss: 1.000024]\n",
      "451 [D loss: 0.999933] [G loss: 1.000065]\n",
      "452 [D loss: 0.999915] [G loss: 1.000086]\n",
      "453 [D loss: 0.999939] [G loss: 1.000036]\n",
      "454 [D loss: 0.999947] [G loss: 1.000088]\n",
      "455 [D loss: 0.999967] [G loss: 1.000038]\n",
      "456 [D loss: 0.999965] [G loss: 1.000088]\n",
      "457 [D loss: 0.999954] [G loss: 1.000085]\n",
      "458 [D loss: 0.999973] [G loss: 1.000083]\n",
      "459 [D loss: 0.999931] [G loss: 1.000067]\n",
      "460 [D loss: 0.999964] [G loss: 1.000039]\n",
      "461 [D loss: 0.999978] [G loss: 1.000076]\n",
      "462 [D loss: 0.999948] [G loss: 1.000014]\n",
      "463 [D loss: 0.999972] [G loss: 1.000015]\n",
      "464 [D loss: 0.999950] [G loss: 1.000105]\n",
      "465 [D loss: 0.999938] [G loss: 1.000042]\n",
      "466 [D loss: 0.999944] [G loss: 0.999977]\n",
      "467 [D loss: 0.999946] [G loss: 1.000028]\n",
      "468 [D loss: 0.999989] [G loss: 1.000039]\n",
      "469 [D loss: 0.999945] [G loss: 1.000093]\n",
      "470 [D loss: 0.999942] [G loss: 0.999997]\n",
      "471 [D loss: 0.999944] [G loss: 1.000033]\n",
      "472 [D loss: 0.999991] [G loss: 1.000015]\n",
      "473 [D loss: 0.999973] [G loss: 0.999990]\n",
      "474 [D loss: 0.999921] [G loss: 1.000029]\n",
      "475 [D loss: 0.999950] [G loss: 1.000081]\n",
      "476 [D loss: 0.999981] [G loss: 1.000041]\n",
      "477 [D loss: 0.999948] [G loss: 1.000091]\n",
      "478 [D loss: 0.999976] [G loss: 1.000063]\n",
      "479 [D loss: 0.999986] [G loss: 1.000003]\n",
      "480 [D loss: 0.999932] [G loss: 1.000074]\n",
      "481 [D loss: 0.999942] [G loss: 1.000046]\n",
      "482 [D loss: 0.999910] [G loss: 1.000025]\n",
      "483 [D loss: 0.999933] [G loss: 1.000037]\n",
      "484 [D loss: 0.999958] [G loss: 1.000068]\n",
      "485 [D loss: 0.999969] [G loss: 1.000081]\n",
      "486 [D loss: 0.999963] [G loss: 1.000026]\n",
      "487 [D loss: 0.999942] [G loss: 1.000033]\n",
      "488 [D loss: 0.999941] [G loss: 1.000027]\n",
      "489 [D loss: 0.999966] [G loss: 1.000060]\n",
      "490 [D loss: 0.999918] [G loss: 1.000051]\n",
      "491 [D loss: 0.999952] [G loss: 1.000079]\n",
      "492 [D loss: 0.999964] [G loss: 1.000004]\n",
      "493 [D loss: 0.999978] [G loss: 1.000054]\n",
      "494 [D loss: 0.999953] [G loss: 1.000019]\n",
      "495 [D loss: 0.999964] [G loss: 1.000027]\n",
      "496 [D loss: 0.999962] [G loss: 1.000079]\n",
      "497 [D loss: 0.999983] [G loss: 1.000007]\n",
      "498 [D loss: 0.999953] [G loss: 1.000042]\n",
      "499 [D loss: 0.999912] [G loss: 1.000043]\n",
      "500 [D loss: 0.999950] [G loss: 1.000045]\n",
      "501 [D loss: 0.999990] [G loss: 1.000069]\n",
      "502 [D loss: 0.999952] [G loss: 1.000025]\n",
      "503 [D loss: 0.999963] [G loss: 1.000091]\n",
      "504 [D loss: 0.999961] [G loss: 0.999985]\n",
      "505 [D loss: 0.999933] [G loss: 1.000040]\n",
      "506 [D loss: 0.999963] [G loss: 1.000038]\n",
      "507 [D loss: 0.999948] [G loss: 1.000002]\n",
      "508 [D loss: 0.999940] [G loss: 1.000076]\n",
      "509 [D loss: 0.999964] [G loss: 1.000049]\n",
      "510 [D loss: 0.999950] [G loss: 1.000078]\n",
      "511 [D loss: 0.999937] [G loss: 1.000059]\n",
      "512 [D loss: 0.999980] [G loss: 0.999985]\n",
      "513 [D loss: 0.999944] [G loss: 1.000060]\n",
      "514 [D loss: 0.999950] [G loss: 1.000043]\n",
      "515 [D loss: 0.999956] [G loss: 1.000058]\n",
      "516 [D loss: 0.999938] [G loss: 1.000053]\n",
      "517 [D loss: 0.999917] [G loss: 0.999981]\n",
      "518 [D loss: 0.999926] [G loss: 1.000064]\n",
      "519 [D loss: 0.999967] [G loss: 1.000084]\n",
      "520 [D loss: 0.999909] [G loss: 1.000035]\n",
      "521 [D loss: 0.999946] [G loss: 1.000072]\n",
      "522 [D loss: 0.999974] [G loss: 1.000044]\n",
      "523 [D loss: 0.999906] [G loss: 1.000044]\n",
      "524 [D loss: 0.999945] [G loss: 1.000043]\n",
      "525 [D loss: 0.999937] [G loss: 1.000078]\n",
      "526 [D loss: 0.999934] [G loss: 1.000062]\n",
      "527 [D loss: 0.999978] [G loss: 0.999997]\n",
      "528 [D loss: 0.999959] [G loss: 1.000005]\n",
      "529 [D loss: 0.999899] [G loss: 1.000038]\n",
      "530 [D loss: 0.999916] [G loss: 1.000057]\n",
      "531 [D loss: 0.999958] [G loss: 1.000047]\n",
      "532 [D loss: 0.999953] [G loss: 1.000093]\n",
      "533 [D loss: 0.999933] [G loss: 1.000019]\n",
      "534 [D loss: 0.999997] [G loss: 1.000037]\n",
      "535 [D loss: 0.999958] [G loss: 1.000030]\n",
      "536 [D loss: 0.999968] [G loss: 1.000043]\n",
      "537 [D loss: 0.999948] [G loss: 1.000034]\n",
      "538 [D loss: 0.999974] [G loss: 1.000092]\n",
      "539 [D loss: 0.999964] [G loss: 1.000018]\n",
      "540 [D loss: 0.999910] [G loss: 1.000098]\n",
      "541 [D loss: 0.999966] [G loss: 1.000035]\n",
      "542 [D loss: 0.999948] [G loss: 1.000056]\n",
      "543 [D loss: 0.999961] [G loss: 1.000063]\n",
      "544 [D loss: 0.999894] [G loss: 1.000056]\n",
      "545 [D loss: 0.999958] [G loss: 1.000048]\n",
      "546 [D loss: 0.999944] [G loss: 1.000027]\n",
      "547 [D loss: 0.999968] [G loss: 1.000027]\n",
      "548 [D loss: 0.999984] [G loss: 1.000023]\n",
      "549 [D loss: 0.999935] [G loss: 1.000012]\n",
      "550 [D loss: 0.999939] [G loss: 1.000033]\n",
      "551 [D loss: 0.999934] [G loss: 1.000032]\n",
      "552 [D loss: 0.999997] [G loss: 1.000057]\n",
      "553 [D loss: 0.999943] [G loss: 1.000078]\n",
      "554 [D loss: 0.999967] [G loss: 1.000011]\n",
      "555 [D loss: 1.000004] [G loss: 1.000064]\n",
      "556 [D loss: 0.999908] [G loss: 1.000031]\n",
      "557 [D loss: 0.999936] [G loss: 1.000077]\n",
      "558 [D loss: 0.999943] [G loss: 1.000004]\n",
      "559 [D loss: 0.999934] [G loss: 0.999989]\n",
      "560 [D loss: 0.999928] [G loss: 1.000028]\n",
      "561 [D loss: 0.999944] [G loss: 1.000043]\n",
      "562 [D loss: 0.999980] [G loss: 1.000017]\n",
      "563 [D loss: 1.000002] [G loss: 1.000048]\n",
      "564 [D loss: 0.999956] [G loss: 1.000072]\n",
      "565 [D loss: 1.000002] [G loss: 1.000027]\n",
      "566 [D loss: 0.999969] [G loss: 1.000055]\n",
      "567 [D loss: 0.999932] [G loss: 1.000083]\n",
      "568 [D loss: 0.999935] [G loss: 1.000092]\n",
      "569 [D loss: 0.999965] [G loss: 1.000076]\n",
      "570 [D loss: 0.999955] [G loss: 1.000090]\n",
      "571 [D loss: 0.999962] [G loss: 0.999993]\n",
      "572 [D loss: 0.999925] [G loss: 1.000035]\n",
      "573 [D loss: 0.999931] [G loss: 1.000098]\n",
      "574 [D loss: 0.999961] [G loss: 1.000089]\n",
      "575 [D loss: 0.999965] [G loss: 1.000075]\n",
      "576 [D loss: 0.999922] [G loss: 1.000073]\n",
      "577 [D loss: 0.999963] [G loss: 0.999998]\n",
      "578 [D loss: 0.999923] [G loss: 1.000078]\n",
      "579 [D loss: 0.999960] [G loss: 1.000079]\n",
      "580 [D loss: 0.999980] [G loss: 1.000073]\n",
      "581 [D loss: 0.999987] [G loss: 1.000053]\n",
      "582 [D loss: 0.999956] [G loss: 1.000008]\n",
      "583 [D loss: 0.999938] [G loss: 1.000014]\n",
      "584 [D loss: 0.999975] [G loss: 1.000081]\n",
      "585 [D loss: 0.999981] [G loss: 1.000090]\n",
      "586 [D loss: 0.999966] [G loss: 1.000065]\n",
      "587 [D loss: 0.999963] [G loss: 1.000102]\n",
      "588 [D loss: 0.999980] [G loss: 1.000059]\n",
      "589 [D loss: 0.999950] [G loss: 1.000032]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590 [D loss: 0.999931] [G loss: 1.000055]\n",
      "591 [D loss: 0.999909] [G loss: 1.000033]\n",
      "592 [D loss: 0.999948] [G loss: 1.000065]\n",
      "593 [D loss: 0.999941] [G loss: 1.000021]\n",
      "594 [D loss: 0.999933] [G loss: 1.000050]\n",
      "595 [D loss: 0.999939] [G loss: 1.000063]\n",
      "596 [D loss: 0.999942] [G loss: 1.000042]\n",
      "597 [D loss: 0.999942] [G loss: 1.000002]\n",
      "598 [D loss: 0.999952] [G loss: 0.999981]\n",
      "599 [D loss: 0.999930] [G loss: 1.000082]\n",
      "600 [D loss: 0.999938] [G loss: 1.000014]\n",
      "601 [D loss: 0.999953] [G loss: 1.000021]\n",
      "602 [D loss: 0.999929] [G loss: 1.000074]\n",
      "603 [D loss: 0.999939] [G loss: 1.000077]\n",
      "604 [D loss: 0.999950] [G loss: 1.000039]\n",
      "605 [D loss: 0.999982] [G loss: 1.000062]\n",
      "606 [D loss: 0.999969] [G loss: 1.000048]\n",
      "607 [D loss: 0.999943] [G loss: 1.000097]\n",
      "608 [D loss: 0.999970] [G loss: 1.000061]\n",
      "609 [D loss: 0.999970] [G loss: 1.000006]\n",
      "610 [D loss: 0.999957] [G loss: 1.000060]\n",
      "611 [D loss: 0.999943] [G loss: 1.000044]\n",
      "612 [D loss: 0.999934] [G loss: 1.000085]\n",
      "613 [D loss: 0.999960] [G loss: 1.000019]\n",
      "614 [D loss: 0.999964] [G loss: 1.000055]\n",
      "615 [D loss: 0.999960] [G loss: 1.000028]\n",
      "616 [D loss: 0.999916] [G loss: 1.000044]\n",
      "617 [D loss: 0.999916] [G loss: 1.000049]\n",
      "618 [D loss: 0.999948] [G loss: 1.000037]\n",
      "619 [D loss: 0.999957] [G loss: 1.000071]\n",
      "620 [D loss: 0.999933] [G loss: 1.000064]\n",
      "621 [D loss: 0.999936] [G loss: 1.000066]\n",
      "622 [D loss: 0.999993] [G loss: 1.000047]\n",
      "623 [D loss: 0.999935] [G loss: 1.000067]\n",
      "624 [D loss: 0.999950] [G loss: 1.000064]\n",
      "625 [D loss: 0.999970] [G loss: 1.000002]\n",
      "626 [D loss: 0.999955] [G loss: 1.000105]\n",
      "627 [D loss: 0.999955] [G loss: 1.000039]\n",
      "628 [D loss: 0.999959] [G loss: 1.000036]\n",
      "629 [D loss: 0.999968] [G loss: 0.999999]\n",
      "630 [D loss: 0.999954] [G loss: 1.000014]\n",
      "631 [D loss: 0.999916] [G loss: 1.000024]\n",
      "632 [D loss: 0.999947] [G loss: 1.000018]\n",
      "633 [D loss: 0.999939] [G loss: 1.000073]\n",
      "634 [D loss: 0.999951] [G loss: 1.000076]\n",
      "635 [D loss: 0.999963] [G loss: 1.000012]\n",
      "636 [D loss: 0.999968] [G loss: 1.000030]\n",
      "637 [D loss: 0.999991] [G loss: 1.000054]\n",
      "638 [D loss: 0.999956] [G loss: 1.000070]\n",
      "639 [D loss: 0.999951] [G loss: 1.000013]\n",
      "640 [D loss: 0.999955] [G loss: 1.000098]\n",
      "641 [D loss: 0.999948] [G loss: 1.000045]\n",
      "642 [D loss: 0.999966] [G loss: 1.000080]\n",
      "643 [D loss: 0.999936] [G loss: 1.000031]\n",
      "644 [D loss: 0.999956] [G loss: 1.000027]\n",
      "645 [D loss: 0.999958] [G loss: 1.000066]\n",
      "646 [D loss: 0.999934] [G loss: 1.000052]\n",
      "647 [D loss: 0.999944] [G loss: 1.000043]\n",
      "648 [D loss: 0.999949] [G loss: 1.000029]\n",
      "649 [D loss: 0.999950] [G loss: 1.000052]\n",
      "650 [D loss: 0.999956] [G loss: 1.000079]\n",
      "651 [D loss: 0.999957] [G loss: 1.000068]\n",
      "652 [D loss: 0.999949] [G loss: 1.000036]\n",
      "653 [D loss: 0.999954] [G loss: 1.000061]\n",
      "654 [D loss: 0.999983] [G loss: 1.000049]\n",
      "655 [D loss: 0.999942] [G loss: 1.000067]\n",
      "656 [D loss: 0.999959] [G loss: 1.000037]\n",
      "657 [D loss: 0.999949] [G loss: 1.000022]\n",
      "658 [D loss: 0.999960] [G loss: 1.000035]\n",
      "659 [D loss: 0.999948] [G loss: 1.000036]\n",
      "660 [D loss: 0.999973] [G loss: 1.000062]\n",
      "661 [D loss: 0.999977] [G loss: 1.000059]\n",
      "662 [D loss: 0.999965] [G loss: 1.000029]\n",
      "663 [D loss: 0.999968] [G loss: 1.000067]\n",
      "664 [D loss: 0.999960] [G loss: 1.000083]\n",
      "665 [D loss: 0.999955] [G loss: 1.000041]\n",
      "666 [D loss: 0.999977] [G loss: 1.000078]\n",
      "667 [D loss: 0.999947] [G loss: 1.000035]\n",
      "668 [D loss: 0.999949] [G loss: 1.000056]\n",
      "669 [D loss: 0.999961] [G loss: 1.000060]\n",
      "670 [D loss: 0.999970] [G loss: 1.000040]\n",
      "671 [D loss: 0.999958] [G loss: 1.000083]\n",
      "672 [D loss: 0.999949] [G loss: 1.000025]\n",
      "673 [D loss: 0.999950] [G loss: 1.000001]\n",
      "674 [D loss: 0.999937] [G loss: 1.000036]\n",
      "675 [D loss: 0.999961] [G loss: 1.000013]\n",
      "676 [D loss: 0.999959] [G loss: 1.000034]\n",
      "677 [D loss: 0.999961] [G loss: 1.000071]\n",
      "678 [D loss: 0.999967] [G loss: 1.000034]\n",
      "679 [D loss: 0.999945] [G loss: 1.000069]\n",
      "680 [D loss: 0.999962] [G loss: 1.000096]\n",
      "681 [D loss: 0.999947] [G loss: 1.000025]\n",
      "682 [D loss: 0.999962] [G loss: 1.000058]\n",
      "683 [D loss: 0.999949] [G loss: 1.000033]\n",
      "684 [D loss: 0.999971] [G loss: 1.000036]\n",
      "685 [D loss: 0.999949] [G loss: 1.000031]\n",
      "686 [D loss: 0.999945] [G loss: 1.000061]\n",
      "687 [D loss: 0.999952] [G loss: 1.000078]\n",
      "688 [D loss: 0.999955] [G loss: 1.000080]\n",
      "689 [D loss: 0.999974] [G loss: 1.000038]\n",
      "690 [D loss: 0.999937] [G loss: 1.000062]\n",
      "691 [D loss: 0.999956] [G loss: 1.000093]\n",
      "692 [D loss: 0.999961] [G loss: 1.000103]\n",
      "693 [D loss: 0.999945] [G loss: 1.000065]\n",
      "694 [D loss: 0.999949] [G loss: 1.000044]\n",
      "695 [D loss: 0.999956] [G loss: 1.000053]\n",
      "696 [D loss: 0.999955] [G loss: 1.000030]\n",
      "697 [D loss: 0.999966] [G loss: 1.000070]\n",
      "698 [D loss: 0.999949] [G loss: 1.000029]\n",
      "699 [D loss: 0.999939] [G loss: 1.000077]\n",
      "700 [D loss: 0.999954] [G loss: 1.000020]\n",
      "701 [D loss: 0.999941] [G loss: 1.000045]\n",
      "702 [D loss: 0.999954] [G loss: 1.000062]\n",
      "703 [D loss: 0.999956] [G loss: 1.000062]\n",
      "704 [D loss: 0.999943] [G loss: 1.000088]\n",
      "705 [D loss: 0.999947] [G loss: 1.000052]\n",
      "706 [D loss: 0.999965] [G loss: 1.000051]\n",
      "707 [D loss: 0.999978] [G loss: 1.000065]\n",
      "708 [D loss: 0.999941] [G loss: 1.000049]\n",
      "709 [D loss: 0.999958] [G loss: 1.000084]\n",
      "710 [D loss: 0.999950] [G loss: 1.000051]\n",
      "711 [D loss: 1.000016] [G loss: 1.000040]\n",
      "712 [D loss: 0.999953] [G loss: 1.000070]\n",
      "713 [D loss: 0.999942] [G loss: 1.000056]\n",
      "714 [D loss: 0.999960] [G loss: 1.000061]\n",
      "715 [D loss: 0.999935] [G loss: 1.000052]\n",
      "716 [D loss: 0.999947] [G loss: 1.000081]\n",
      "717 [D loss: 0.999962] [G loss: 1.000055]\n",
      "718 [D loss: 0.999954] [G loss: 1.000070]\n",
      "719 [D loss: 0.999927] [G loss: 1.000052]\n",
      "720 [D loss: 0.999947] [G loss: 1.000023]\n",
      "721 [D loss: 0.999924] [G loss: 1.000053]\n",
      "722 [D loss: 0.999975] [G loss: 1.000075]\n",
      "723 [D loss: 0.999959] [G loss: 1.000052]\n",
      "724 [D loss: 0.999958] [G loss: 1.000076]\n",
      "725 [D loss: 0.999963] [G loss: 1.000051]\n",
      "726 [D loss: 0.999964] [G loss: 1.000047]\n",
      "727 [D loss: 0.999941] [G loss: 1.000017]\n",
      "728 [D loss: 0.999927] [G loss: 1.000060]\n",
      "729 [D loss: 0.999971] [G loss: 1.000005]\n",
      "730 [D loss: 0.999979] [G loss: 1.000047]\n",
      "731 [D loss: 0.999953] [G loss: 1.000072]\n",
      "732 [D loss: 0.999962] [G loss: 1.000060]\n",
      "733 [D loss: 0.999941] [G loss: 1.000028]\n",
      "734 [D loss: 0.999965] [G loss: 1.000026]\n",
      "735 [D loss: 0.999969] [G loss: 1.000056]\n",
      "736 [D loss: 0.999961] [G loss: 1.000054]\n",
      "737 [D loss: 0.999969] [G loss: 1.000043]\n",
      "738 [D loss: 0.999970] [G loss: 1.000096]\n",
      "739 [D loss: 0.999981] [G loss: 1.000056]\n",
      "740 [D loss: 0.999954] [G loss: 1.000031]\n",
      "741 [D loss: 0.999968] [G loss: 1.000057]\n",
      "742 [D loss: 0.999975] [G loss: 1.000081]\n",
      "743 [D loss: 0.999940] [G loss: 1.000076]\n",
      "744 [D loss: 0.999953] [G loss: 1.000046]\n",
      "745 [D loss: 0.999980] [G loss: 1.000061]\n",
      "746 [D loss: 0.999982] [G loss: 1.000054]\n",
      "747 [D loss: 0.999977] [G loss: 1.000027]\n",
      "748 [D loss: 0.999945] [G loss: 1.000085]\n",
      "749 [D loss: 0.999956] [G loss: 1.000069]\n",
      "750 [D loss: 0.999963] [G loss: 1.000044]\n",
      "751 [D loss: 0.999964] [G loss: 1.000086]\n",
      "752 [D loss: 0.999985] [G loss: 1.000065]\n",
      "753 [D loss: 0.999936] [G loss: 1.000051]\n",
      "754 [D loss: 0.999977] [G loss: 1.000015]\n",
      "755 [D loss: 0.999914] [G loss: 1.000072]\n",
      "756 [D loss: 0.999951] [G loss: 1.000035]\n",
      "757 [D loss: 0.999929] [G loss: 1.000028]\n",
      "758 [D loss: 0.999945] [G loss: 1.000083]\n",
      "759 [D loss: 0.999916] [G loss: 1.000084]\n",
      "760 [D loss: 0.999932] [G loss: 1.000061]\n",
      "761 [D loss: 0.999948] [G loss: 1.000083]\n",
      "762 [D loss: 0.999938] [G loss: 1.000029]\n",
      "763 [D loss: 0.999969] [G loss: 1.000043]\n",
      "764 [D loss: 0.999959] [G loss: 1.000049]\n",
      "765 [D loss: 0.999975] [G loss: 1.000021]\n",
      "766 [D loss: 0.999941] [G loss: 1.000081]\n",
      "767 [D loss: 0.999944] [G loss: 1.000061]\n",
      "768 [D loss: 0.999964] [G loss: 1.000076]\n",
      "769 [D loss: 0.999936] [G loss: 1.000068]\n",
      "770 [D loss: 0.999942] [G loss: 1.000045]\n",
      "771 [D loss: 0.999961] [G loss: 1.000011]\n",
      "772 [D loss: 0.999960] [G loss: 1.000060]\n",
      "773 [D loss: 0.999944] [G loss: 1.000043]\n",
      "774 [D loss: 0.999935] [G loss: 1.000062]\n",
      "775 [D loss: 0.999964] [G loss: 1.000041]\n",
      "776 [D loss: 0.999939] [G loss: 1.000054]\n",
      "777 [D loss: 0.999959] [G loss: 1.000061]\n",
      "778 [D loss: 0.999917] [G loss: 1.000076]\n",
      "779 [D loss: 0.999942] [G loss: 1.000022]\n",
      "780 [D loss: 0.999976] [G loss: 1.000028]\n",
      "781 [D loss: 0.999981] [G loss: 1.000035]\n",
      "782 [D loss: 0.999963] [G loss: 1.000075]\n",
      "783 [D loss: 0.999951] [G loss: 1.000071]\n",
      "784 [D loss: 0.999942] [G loss: 1.000082]\n",
      "785 [D loss: 0.999987] [G loss: 1.000055]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "786 [D loss: 0.999971] [G loss: 1.000063]\n",
      "787 [D loss: 0.999985] [G loss: 1.000053]\n",
      "788 [D loss: 0.999931] [G loss: 1.000071]\n",
      "789 [D loss: 0.999987] [G loss: 1.000031]\n",
      "790 [D loss: 0.999954] [G loss: 1.000047]\n",
      "791 [D loss: 0.999930] [G loss: 1.000019]\n",
      "792 [D loss: 0.999967] [G loss: 1.000029]\n",
      "793 [D loss: 0.999939] [G loss: 1.000040]\n",
      "794 [D loss: 0.999924] [G loss: 1.000056]\n",
      "795 [D loss: 0.999970] [G loss: 1.000016]\n",
      "796 [D loss: 0.999930] [G loss: 1.000055]\n",
      "797 [D loss: 0.999969] [G loss: 1.000037]\n",
      "798 [D loss: 0.999950] [G loss: 1.000058]\n",
      "799 [D loss: 0.999935] [G loss: 1.000059]\n",
      "800 [D loss: 0.999967] [G loss: 1.000064]\n",
      "801 [D loss: 0.999963] [G loss: 0.999983]\n",
      "802 [D loss: 0.999982] [G loss: 1.000054]\n",
      "803 [D loss: 0.999969] [G loss: 1.000053]\n",
      "804 [D loss: 0.999965] [G loss: 1.000089]\n",
      "805 [D loss: 0.999981] [G loss: 1.000063]\n",
      "806 [D loss: 0.999965] [G loss: 1.000057]\n",
      "807 [D loss: 0.999975] [G loss: 1.000094]\n",
      "808 [D loss: 0.999923] [G loss: 1.000061]\n",
      "809 [D loss: 0.999963] [G loss: 1.000054]\n",
      "810 [D loss: 0.999968] [G loss: 1.000022]\n",
      "811 [D loss: 0.999944] [G loss: 1.000048]\n",
      "812 [D loss: 0.999973] [G loss: 1.000055]\n",
      "813 [D loss: 0.999955] [G loss: 1.000045]\n",
      "814 [D loss: 0.999943] [G loss: 1.000046]\n",
      "815 [D loss: 0.999966] [G loss: 1.000049]\n",
      "816 [D loss: 0.999996] [G loss: 1.000047]\n",
      "817 [D loss: 0.999933] [G loss: 1.000034]\n",
      "818 [D loss: 0.999936] [G loss: 1.000069]\n",
      "819 [D loss: 0.999962] [G loss: 1.000060]\n",
      "820 [D loss: 0.999973] [G loss: 1.000068]\n",
      "821 [D loss: 0.999956] [G loss: 1.000093]\n",
      "822 [D loss: 0.999968] [G loss: 1.000054]\n",
      "823 [D loss: 0.999969] [G loss: 1.000041]\n",
      "824 [D loss: 0.999940] [G loss: 1.000030]\n",
      "825 [D loss: 0.999955] [G loss: 1.000038]\n",
      "826 [D loss: 0.999975] [G loss: 1.000061]\n",
      "827 [D loss: 0.999929] [G loss: 1.000052]\n",
      "828 [D loss: 0.999964] [G loss: 1.000058]\n",
      "829 [D loss: 0.999938] [G loss: 1.000069]\n",
      "830 [D loss: 0.999985] [G loss: 1.000075]\n",
      "831 [D loss: 0.999987] [G loss: 1.000037]\n",
      "832 [D loss: 0.999969] [G loss: 1.000082]\n",
      "833 [D loss: 0.999955] [G loss: 1.000086]\n",
      "834 [D loss: 0.999981] [G loss: 1.000028]\n",
      "835 [D loss: 0.999955] [G loss: 1.000070]\n",
      "836 [D loss: 0.999975] [G loss: 1.000042]\n",
      "837 [D loss: 0.999960] [G loss: 1.000049]\n",
      "838 [D loss: 0.999955] [G loss: 1.000014]\n",
      "839 [D loss: 0.999985] [G loss: 1.000040]\n",
      "840 [D loss: 0.999951] [G loss: 1.000086]\n",
      "841 [D loss: 0.999947] [G loss: 1.000078]\n",
      "842 [D loss: 0.999964] [G loss: 1.000025]\n",
      "843 [D loss: 0.999955] [G loss: 1.000044]\n",
      "844 [D loss: 0.999966] [G loss: 1.000059]\n",
      "845 [D loss: 0.999970] [G loss: 1.000094]\n",
      "846 [D loss: 0.999985] [G loss: 1.000047]\n",
      "847 [D loss: 0.999975] [G loss: 1.000053]\n",
      "848 [D loss: 0.999994] [G loss: 1.000029]\n",
      "849 [D loss: 0.999982] [G loss: 1.000032]\n",
      "850 [D loss: 0.999939] [G loss: 1.000076]\n",
      "851 [D loss: 0.999946] [G loss: 1.000052]\n",
      "852 [D loss: 0.999930] [G loss: 1.000057]\n",
      "853 [D loss: 0.999960] [G loss: 1.000057]\n",
      "854 [D loss: 0.999973] [G loss: 1.000089]\n",
      "855 [D loss: 0.999952] [G loss: 1.000063]\n",
      "856 [D loss: 0.999953] [G loss: 1.000075]\n",
      "857 [D loss: 0.999974] [G loss: 1.000060]\n",
      "858 [D loss: 0.999966] [G loss: 1.000051]\n",
      "859 [D loss: 0.999947] [G loss: 1.000076]\n",
      "860 [D loss: 0.999948] [G loss: 1.000066]\n",
      "861 [D loss: 0.999966] [G loss: 1.000091]\n",
      "862 [D loss: 0.999970] [G loss: 1.000036]\n",
      "863 [D loss: 0.999942] [G loss: 1.000054]\n",
      "864 [D loss: 0.999920] [G loss: 1.000080]\n",
      "865 [D loss: 0.999917] [G loss: 1.000067]\n",
      "866 [D loss: 0.999976] [G loss: 1.000032]\n",
      "867 [D loss: 0.999971] [G loss: 1.000051]\n",
      "868 [D loss: 0.999980] [G loss: 1.000051]\n",
      "869 [D loss: 0.999992] [G loss: 1.000067]\n",
      "870 [D loss: 0.999977] [G loss: 1.000025]\n",
      "871 [D loss: 0.999939] [G loss: 1.000086]\n",
      "872 [D loss: 0.999917] [G loss: 1.000052]\n",
      "873 [D loss: 0.999944] [G loss: 1.000058]\n",
      "874 [D loss: 0.999979] [G loss: 1.000051]\n",
      "875 [D loss: 0.999940] [G loss: 1.000054]\n",
      "876 [D loss: 0.999959] [G loss: 1.000018]\n",
      "877 [D loss: 0.999981] [G loss: 1.000014]\n",
      "878 [D loss: 0.999957] [G loss: 1.000030]\n",
      "879 [D loss: 0.999990] [G loss: 1.000033]\n",
      "880 [D loss: 0.999966] [G loss: 1.000036]\n",
      "881 [D loss: 0.999957] [G loss: 1.000040]\n",
      "882 [D loss: 0.999947] [G loss: 1.000046]\n",
      "883 [D loss: 0.999960] [G loss: 1.000027]\n",
      "884 [D loss: 0.999959] [G loss: 1.000067]\n",
      "885 [D loss: 0.999962] [G loss: 1.000044]\n",
      "886 [D loss: 0.999937] [G loss: 1.000096]\n",
      "887 [D loss: 0.999949] [G loss: 1.000063]\n",
      "888 [D loss: 0.999948] [G loss: 1.000023]\n",
      "889 [D loss: 0.999938] [G loss: 1.000065]\n",
      "890 [D loss: 0.999925] [G loss: 1.000051]\n",
      "891 [D loss: 0.999950] [G loss: 1.000047]\n",
      "892 [D loss: 0.999970] [G loss: 1.000016]\n",
      "893 [D loss: 0.999960] [G loss: 1.000075]\n",
      "894 [D loss: 0.999934] [G loss: 1.000059]\n",
      "895 [D loss: 0.999948] [G loss: 1.000052]\n",
      "896 [D loss: 0.999970] [G loss: 1.000060]\n",
      "897 [D loss: 0.999946] [G loss: 1.000074]\n",
      "898 [D loss: 0.999976] [G loss: 1.000031]\n",
      "899 [D loss: 0.999955] [G loss: 1.000070]\n",
      "900 [D loss: 0.999947] [G loss: 1.000044]\n",
      "901 [D loss: 0.999969] [G loss: 1.000024]\n",
      "902 [D loss: 0.999957] [G loss: 1.000021]\n",
      "903 [D loss: 0.999942] [G loss: 1.000033]\n",
      "904 [D loss: 0.999969] [G loss: 1.000078]\n",
      "905 [D loss: 0.999929] [G loss: 1.000072]\n",
      "906 [D loss: 0.999942] [G loss: 1.000057]\n",
      "907 [D loss: 0.999940] [G loss: 1.000050]\n",
      "908 [D loss: 0.999959] [G loss: 1.000089]\n",
      "909 [D loss: 0.999954] [G loss: 1.000031]\n",
      "910 [D loss: 0.999980] [G loss: 1.000051]\n",
      "911 [D loss: 0.999953] [G loss: 1.000045]\n",
      "912 [D loss: 0.999939] [G loss: 1.000030]\n",
      "913 [D loss: 0.999971] [G loss: 1.000040]\n",
      "914 [D loss: 0.999965] [G loss: 1.000019]\n",
      "915 [D loss: 0.999937] [G loss: 1.000085]\n",
      "916 [D loss: 0.999937] [G loss: 1.000081]\n",
      "917 [D loss: 0.999946] [G loss: 1.000057]\n",
      "918 [D loss: 0.999944] [G loss: 1.000067]\n",
      "919 [D loss: 0.999933] [G loss: 1.000015]\n",
      "920 [D loss: 0.999945] [G loss: 1.000093]\n",
      "921 [D loss: 0.999931] [G loss: 1.000047]\n",
      "922 [D loss: 0.999948] [G loss: 1.000076]\n",
      "923 [D loss: 0.999966] [G loss: 1.000081]\n",
      "924 [D loss: 0.999960] [G loss: 1.000045]\n",
      "925 [D loss: 0.999997] [G loss: 1.000094]\n",
      "926 [D loss: 0.999967] [G loss: 1.000082]\n",
      "927 [D loss: 0.999971] [G loss: 1.000045]\n",
      "928 [D loss: 0.999954] [G loss: 1.000061]\n",
      "929 [D loss: 0.999946] [G loss: 1.000050]\n",
      "930 [D loss: 0.999945] [G loss: 1.000045]\n",
      "931 [D loss: 0.999938] [G loss: 1.000018]\n",
      "932 [D loss: 0.999969] [G loss: 1.000047]\n",
      "933 [D loss: 0.999984] [G loss: 1.000027]\n",
      "934 [D loss: 0.999935] [G loss: 0.999997]\n",
      "935 [D loss: 0.999958] [G loss: 1.000028]\n",
      "936 [D loss: 0.999945] [G loss: 1.000056]\n",
      "937 [D loss: 0.999931] [G loss: 1.000111]\n",
      "938 [D loss: 0.999953] [G loss: 1.000068]\n",
      "939 [D loss: 0.999957] [G loss: 1.000024]\n",
      "940 [D loss: 0.999920] [G loss: 1.000030]\n",
      "941 [D loss: 0.999970] [G loss: 0.999991]\n",
      "942 [D loss: 0.999889] [G loss: 1.000016]\n",
      "943 [D loss: 0.999934] [G loss: 1.000029]\n",
      "944 [D loss: 0.999920] [G loss: 1.000047]\n",
      "945 [D loss: 0.999865] [G loss: 1.000065]\n",
      "946 [D loss: 0.999965] [G loss: 1.000041]\n",
      "947 [D loss: 0.999958] [G loss: 1.000036]\n",
      "948 [D loss: 0.999917] [G loss: 0.999974]\n",
      "949 [D loss: 0.999937] [G loss: 1.000069]\n",
      "950 [D loss: 0.999956] [G loss: 1.000053]\n",
      "951 [D loss: 0.999974] [G loss: 1.000025]\n",
      "952 [D loss: 1.000003] [G loss: 1.000067]\n",
      "953 [D loss: 0.999902] [G loss: 1.000065]\n",
      "954 [D loss: 0.999927] [G loss: 1.000030]\n",
      "955 [D loss: 0.999960] [G loss: 1.000020]\n",
      "956 [D loss: 0.999957] [G loss: 1.000030]\n",
      "957 [D loss: 0.999985] [G loss: 1.000075]\n",
      "958 [D loss: 0.999927] [G loss: 1.000040]\n",
      "959 [D loss: 0.999958] [G loss: 1.000047]\n",
      "960 [D loss: 0.999959] [G loss: 1.000080]\n",
      "961 [D loss: 0.999964] [G loss: 1.000040]\n",
      "962 [D loss: 0.999933] [G loss: 1.000070]\n",
      "963 [D loss: 0.999957] [G loss: 1.000039]\n",
      "964 [D loss: 0.999936] [G loss: 1.000005]\n",
      "965 [D loss: 0.999933] [G loss: 1.000016]\n",
      "966 [D loss: 0.999949] [G loss: 0.999996]\n",
      "967 [D loss: 0.999969] [G loss: 1.000106]\n",
      "968 [D loss: 0.999978] [G loss: 1.000016]\n",
      "969 [D loss: 0.999973] [G loss: 1.000056]\n",
      "970 [D loss: 0.999980] [G loss: 1.000058]\n",
      "971 [D loss: 0.999971] [G loss: 1.000051]\n",
      "972 [D loss: 0.999909] [G loss: 1.000014]\n",
      "973 [D loss: 0.999938] [G loss: 1.000071]\n",
      "974 [D loss: 1.000013] [G loss: 1.000024]\n",
      "975 [D loss: 0.999963] [G loss: 1.000017]\n",
      "976 [D loss: 1.000032] [G loss: 1.000048]\n",
      "977 [D loss: 0.999964] [G loss: 1.000067]\n",
      "978 [D loss: 0.999963] [G loss: 1.000026]\n",
      "979 [D loss: 0.999944] [G loss: 1.000032]\n",
      "980 [D loss: 0.999897] [G loss: 0.999932]\n",
      "981 [D loss: 0.999931] [G loss: 1.000028]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "982 [D loss: 0.999926] [G loss: 0.999996]\n",
      "983 [D loss: 0.999958] [G loss: 1.000013]\n",
      "984 [D loss: 0.999910] [G loss: 1.000073]\n",
      "985 [D loss: 0.999922] [G loss: 1.000050]\n",
      "986 [D loss: 0.999905] [G loss: 1.000033]\n",
      "987 [D loss: 0.999972] [G loss: 1.000116]\n",
      "988 [D loss: 0.999912] [G loss: 1.000045]\n",
      "989 [D loss: 0.999904] [G loss: 1.000047]\n",
      "990 [D loss: 0.999937] [G loss: 0.999981]\n",
      "991 [D loss: 0.999945] [G loss: 1.000028]\n",
      "992 [D loss: 0.999946] [G loss: 1.000070]\n",
      "993 [D loss: 0.999953] [G loss: 1.000067]\n",
      "994 [D loss: 0.999943] [G loss: 0.999995]\n",
      "995 [D loss: 0.999945] [G loss: 1.000084]\n",
      "996 [D loss: 0.999940] [G loss: 0.999977]\n",
      "997 [D loss: 0.999917] [G loss: 1.000020]\n",
      "998 [D loss: 0.999961] [G loss: 0.999979]\n",
      "999 [D loss: 0.999936] [G loss: 1.000057]\n",
      "1000 [D loss: 0.999940] [G loss: 1.000055]\n",
      "1001 [D loss: 0.999938] [G loss: 1.000039]\n",
      "1002 [D loss: 0.999994] [G loss: 1.000009]\n",
      "1003 [D loss: 0.999921] [G loss: 1.000062]\n",
      "1004 [D loss: 0.999982] [G loss: 1.000018]\n",
      "1005 [D loss: 0.999894] [G loss: 1.000015]\n",
      "1006 [D loss: 0.999944] [G loss: 0.999990]\n",
      "1007 [D loss: 0.999958] [G loss: 0.999980]\n",
      "1008 [D loss: 0.999977] [G loss: 1.000051]\n",
      "1009 [D loss: 0.999948] [G loss: 1.000014]\n",
      "1010 [D loss: 0.999926] [G loss: 1.000048]\n",
      "1011 [D loss: 0.999999] [G loss: 0.999898]\n",
      "1012 [D loss: 0.999967] [G loss: 0.999971]\n",
      "1013 [D loss: 0.999883] [G loss: 1.000035]\n",
      "1014 [D loss: 0.999901] [G loss: 1.000092]\n",
      "1015 [D loss: 0.999922] [G loss: 0.999964]\n",
      "1016 [D loss: 0.999915] [G loss: 1.000049]\n",
      "1017 [D loss: 0.999899] [G loss: 1.000001]\n",
      "1018 [D loss: 0.999923] [G loss: 1.000036]\n",
      "1019 [D loss: 0.999954] [G loss: 0.999970]\n",
      "1020 [D loss: 0.999951] [G loss: 0.999967]\n",
      "1021 [D loss: 0.999940] [G loss: 1.000041]\n",
      "1022 [D loss: 0.999988] [G loss: 1.000047]\n",
      "1023 [D loss: 0.999879] [G loss: 1.000008]\n",
      "1024 [D loss: 0.999925] [G loss: 1.000089]\n",
      "1025 [D loss: 0.999939] [G loss: 0.999974]\n",
      "1026 [D loss: 0.999952] [G loss: 1.000004]\n",
      "1027 [D loss: 0.999941] [G loss: 1.000013]\n",
      "1028 [D loss: 0.999903] [G loss: 1.000074]\n",
      "1029 [D loss: 0.999903] [G loss: 1.000001]\n",
      "1030 [D loss: 0.999904] [G loss: 1.000012]\n",
      "1031 [D loss: 0.999984] [G loss: 1.000002]\n",
      "1032 [D loss: 0.999939] [G loss: 1.000038]\n",
      "1033 [D loss: 0.999953] [G loss: 1.000047]\n",
      "1034 [D loss: 0.999930] [G loss: 1.000001]\n",
      "1035 [D loss: 0.999966] [G loss: 1.000003]\n",
      "1036 [D loss: 0.999962] [G loss: 0.999999]\n",
      "1037 [D loss: 0.999953] [G loss: 1.000037]\n",
      "1038 [D loss: 0.999899] [G loss: 1.000066]\n",
      "1039 [D loss: 0.999937] [G loss: 0.999982]\n",
      "1040 [D loss: 0.999908] [G loss: 0.999972]\n",
      "1041 [D loss: 0.999925] [G loss: 1.000016]\n",
      "1042 [D loss: 0.999935] [G loss: 1.000018]\n",
      "1043 [D loss: 0.999941] [G loss: 0.999994]\n",
      "1044 [D loss: 0.999927] [G loss: 1.000030]\n",
      "1045 [D loss: 0.999968] [G loss: 1.000036]\n",
      "1046 [D loss: 0.999933] [G loss: 1.000048]\n",
      "1047 [D loss: 0.999949] [G loss: 1.000033]\n",
      "1048 [D loss: 0.999945] [G loss: 1.000089]\n",
      "1049 [D loss: 0.999930] [G loss: 1.000001]\n",
      "1050 [D loss: 0.999931] [G loss: 1.000027]\n",
      "1051 [D loss: 0.999949] [G loss: 1.000046]\n",
      "1052 [D loss: 0.999937] [G loss: 1.000071]\n",
      "1053 [D loss: 0.999970] [G loss: 1.000059]\n",
      "1054 [D loss: 0.999944] [G loss: 1.000079]\n",
      "1055 [D loss: 0.999945] [G loss: 1.000020]\n",
      "1056 [D loss: 0.999912] [G loss: 0.999947]\n",
      "1057 [D loss: 0.999905] [G loss: 1.000074]\n",
      "1058 [D loss: 0.999917] [G loss: 0.999953]\n",
      "1059 [D loss: 0.999946] [G loss: 1.000054]\n",
      "1060 [D loss: 0.999924] [G loss: 1.000099]\n",
      "1061 [D loss: 0.999949] [G loss: 1.000012]\n",
      "1062 [D loss: 0.999961] [G loss: 1.000042]\n",
      "1063 [D loss: 0.999941] [G loss: 1.000005]\n",
      "1064 [D loss: 0.999908] [G loss: 0.999998]\n",
      "1065 [D loss: 0.999960] [G loss: 0.999994]\n",
      "1066 [D loss: 0.999929] [G loss: 1.000035]\n",
      "1067 [D loss: 0.999973] [G loss: 1.000058]\n",
      "1068 [D loss: 0.999920] [G loss: 1.000033]\n",
      "1069 [D loss: 0.999979] [G loss: 1.000037]\n",
      "1070 [D loss: 0.999953] [G loss: 1.000023]\n",
      "1071 [D loss: 0.999904] [G loss: 1.000079]\n",
      "1072 [D loss: 0.999952] [G loss: 1.000048]\n",
      "1073 [D loss: 0.999935] [G loss: 1.000004]\n",
      "1074 [D loss: 0.999973] [G loss: 1.000050]\n",
      "1075 [D loss: 0.999965] [G loss: 1.000041]\n",
      "1076 [D loss: 0.999992] [G loss: 1.000088]\n",
      "1077 [D loss: 0.999968] [G loss: 1.000048]\n",
      "1078 [D loss: 0.999956] [G loss: 1.000066]\n",
      "1079 [D loss: 0.999974] [G loss: 1.000011]\n",
      "1080 [D loss: 0.999932] [G loss: 1.000062]\n",
      "1081 [D loss: 0.999940] [G loss: 0.999960]\n",
      "1082 [D loss: 0.999959] [G loss: 1.000005]\n",
      "1083 [D loss: 0.999957] [G loss: 1.000037]\n",
      "1084 [D loss: 0.999930] [G loss: 1.000009]\n",
      "1085 [D loss: 0.999921] [G loss: 1.000016]\n",
      "1086 [D loss: 0.999902] [G loss: 1.000048]\n",
      "1087 [D loss: 0.999978] [G loss: 1.000026]\n",
      "1088 [D loss: 0.999904] [G loss: 1.000143]\n",
      "1089 [D loss: 0.999955] [G loss: 1.000033]\n",
      "1090 [D loss: 0.999878] [G loss: 1.000094]\n",
      "1091 [D loss: 0.999930] [G loss: 1.000043]\n",
      "1092 [D loss: 0.999906] [G loss: 1.000025]\n",
      "1093 [D loss: 0.999916] [G loss: 1.000074]\n",
      "1094 [D loss: 0.999985] [G loss: 1.000048]\n",
      "1095 [D loss: 0.999917] [G loss: 1.000031]\n",
      "1096 [D loss: 0.999960] [G loss: 1.000044]\n",
      "1097 [D loss: 0.999893] [G loss: 1.000075]\n",
      "1098 [D loss: 0.999989] [G loss: 1.000081]\n",
      "1099 [D loss: 0.999923] [G loss: 1.000027]\n",
      "1100 [D loss: 0.999913] [G loss: 1.000064]\n",
      "1101 [D loss: 0.999855] [G loss: 1.000020]\n",
      "1102 [D loss: 0.999902] [G loss: 1.000179]\n",
      "1103 [D loss: 0.999932] [G loss: 1.000027]\n",
      "1104 [D loss: 0.999913] [G loss: 1.000028]\n",
      "1105 [D loss: 0.999854] [G loss: 1.000075]\n",
      "1106 [D loss: 0.999987] [G loss: 0.999987]\n",
      "1107 [D loss: 0.999884] [G loss: 1.000061]\n",
      "1108 [D loss: 0.999957] [G loss: 1.000038]\n",
      "1109 [D loss: 0.999941] [G loss: 1.000128]\n",
      "1110 [D loss: 0.999917] [G loss: 1.000021]\n",
      "1111 [D loss: 0.999948] [G loss: 1.000005]\n",
      "1112 [D loss: 0.999957] [G loss: 1.000120]\n",
      "1113 [D loss: 0.999951] [G loss: 1.000122]\n",
      "1114 [D loss: 0.999942] [G loss: 1.000006]\n",
      "1115 [D loss: 0.999942] [G loss: 0.999978]\n",
      "1116 [D loss: 0.999915] [G loss: 1.000106]\n",
      "1117 [D loss: 0.999996] [G loss: 1.000057]\n",
      "1118 [D loss: 0.999947] [G loss: 1.000039]\n",
      "1119 [D loss: 0.999962] [G loss: 1.000087]\n",
      "1120 [D loss: 1.000013] [G loss: 1.000022]\n",
      "1121 [D loss: 0.999972] [G loss: 0.999995]\n",
      "1122 [D loss: 0.999967] [G loss: 1.000035]\n",
      "1123 [D loss: 0.999960] [G loss: 1.000070]\n",
      "1124 [D loss: 0.999930] [G loss: 1.000057]\n",
      "1125 [D loss: 0.999970] [G loss: 1.000054]\n",
      "1126 [D loss: 0.999967] [G loss: 1.000058]\n",
      "1127 [D loss: 0.999920] [G loss: 1.000040]\n",
      "1128 [D loss: 0.999945] [G loss: 1.000013]\n",
      "1129 [D loss: 0.999943] [G loss: 1.000067]\n",
      "1130 [D loss: 0.999884] [G loss: 1.000030]\n",
      "1131 [D loss: 0.999970] [G loss: 0.999968]\n",
      "1132 [D loss: 0.999935] [G loss: 0.999989]\n",
      "1133 [D loss: 0.999929] [G loss: 0.999958]\n",
      "1134 [D loss: 0.999924] [G loss: 1.000031]\n",
      "1135 [D loss: 0.999928] [G loss: 1.000069]\n",
      "1136 [D loss: 0.999906] [G loss: 1.000050]\n",
      "1137 [D loss: 0.999956] [G loss: 0.999997]\n",
      "1138 [D loss: 0.999923] [G loss: 1.000025]\n",
      "1139 [D loss: 0.999957] [G loss: 1.000036]\n",
      "1140 [D loss: 0.999867] [G loss: 0.999987]\n",
      "1141 [D loss: 0.999970] [G loss: 1.000103]\n",
      "1142 [D loss: 0.999899] [G loss: 0.999974]\n",
      "1143 [D loss: 0.999891] [G loss: 1.000025]\n",
      "1144 [D loss: 0.999924] [G loss: 0.999993]\n",
      "1145 [D loss: 1.000010] [G loss: 0.999986]\n",
      "1146 [D loss: 0.999948] [G loss: 1.000020]\n",
      "1147 [D loss: 0.999932] [G loss: 1.000018]\n",
      "1148 [D loss: 0.999958] [G loss: 1.000004]\n",
      "1149 [D loss: 0.999950] [G loss: 0.999988]\n",
      "1150 [D loss: 0.999964] [G loss: 1.000007]\n",
      "1151 [D loss: 0.999952] [G loss: 1.000018]\n",
      "1152 [D loss: 0.999947] [G loss: 0.999998]\n",
      "1153 [D loss: 0.999951] [G loss: 1.000036]\n",
      "1154 [D loss: 0.999912] [G loss: 1.000023]\n",
      "1155 [D loss: 0.999903] [G loss: 1.000033]\n",
      "1156 [D loss: 0.999957] [G loss: 1.000060]\n",
      "1157 [D loss: 0.999966] [G loss: 1.000114]\n",
      "1158 [D loss: 0.999939] [G loss: 0.999966]\n",
      "1159 [D loss: 0.999892] [G loss: 1.000043]\n",
      "1160 [D loss: 0.999952] [G loss: 0.999983]\n",
      "1161 [D loss: 0.999943] [G loss: 1.000043]\n",
      "1162 [D loss: 0.999892] [G loss: 1.000081]\n",
      "1163 [D loss: 0.999918] [G loss: 0.999988]\n",
      "1164 [D loss: 0.999909] [G loss: 1.000016]\n",
      "1165 [D loss: 0.999972] [G loss: 1.000146]\n",
      "1166 [D loss: 0.999903] [G loss: 0.999983]\n",
      "1167 [D loss: 0.999908] [G loss: 0.999992]\n",
      "1168 [D loss: 0.999977] [G loss: 1.000047]\n",
      "1169 [D loss: 0.999946] [G loss: 0.999986]\n",
      "1170 [D loss: 0.999920] [G loss: 1.000110]\n",
      "1171 [D loss: 0.999971] [G loss: 0.999943]\n",
      "1172 [D loss: 0.999913] [G loss: 0.999971]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1173 [D loss: 0.999917] [G loss: 1.000053]\n",
      "1174 [D loss: 0.999904] [G loss: 0.999980]\n",
      "1175 [D loss: 0.999955] [G loss: 1.000014]\n",
      "1176 [D loss: 0.999901] [G loss: 0.999974]\n",
      "1177 [D loss: 0.999959] [G loss: 1.000029]\n",
      "1178 [D loss: 0.999918] [G loss: 1.000060]\n",
      "1179 [D loss: 0.999941] [G loss: 0.999998]\n",
      "1180 [D loss: 0.999925] [G loss: 1.000016]\n",
      "1181 [D loss: 0.999926] [G loss: 1.000041]\n",
      "1182 [D loss: 0.999953] [G loss: 0.999910]\n",
      "1183 [D loss: 0.999934] [G loss: 1.000035]\n",
      "1184 [D loss: 0.999912] [G loss: 0.999917]\n",
      "1185 [D loss: 0.999976] [G loss: 1.000053]\n",
      "1186 [D loss: 0.999906] [G loss: 0.999982]\n",
      "1187 [D loss: 0.999938] [G loss: 1.000145]\n",
      "1188 [D loss: 0.999905] [G loss: 1.000030]\n",
      "1189 [D loss: 0.999964] [G loss: 1.000099]\n",
      "1190 [D loss: 0.999900] [G loss: 1.000139]\n",
      "1191 [D loss: 0.999897] [G loss: 1.000037]\n",
      "1192 [D loss: 0.999970] [G loss: 1.000026]\n",
      "1193 [D loss: 0.999939] [G loss: 0.999914]\n",
      "1194 [D loss: 0.999988] [G loss: 1.000018]\n",
      "1195 [D loss: 0.999946] [G loss: 0.999929]\n",
      "1196 [D loss: 0.999945] [G loss: 0.999893]\n",
      "1197 [D loss: 0.999863] [G loss: 0.999978]\n",
      "1198 [D loss: 0.999929] [G loss: 0.999998]\n",
      "1199 [D loss: 0.999983] [G loss: 0.999999]\n",
      "1200 [D loss: 0.999939] [G loss: 0.999984]\n",
      "1201 [D loss: 0.999927] [G loss: 1.000019]\n",
      "1202 [D loss: 0.999905] [G loss: 1.000042]\n",
      "1203 [D loss: 0.999894] [G loss: 1.000057]\n",
      "1204 [D loss: 1.000010] [G loss: 0.999982]\n",
      "1205 [D loss: 0.999966] [G loss: 1.000072]\n",
      "1206 [D loss: 0.999885] [G loss: 1.000061]\n",
      "1207 [D loss: 0.999934] [G loss: 0.999978]\n",
      "1208 [D loss: 0.999965] [G loss: 1.000083]\n",
      "1209 [D loss: 0.999953] [G loss: 0.999990]\n",
      "1210 [D loss: 0.999835] [G loss: 1.000017]\n",
      "1211 [D loss: 0.999870] [G loss: 1.000041]\n",
      "1212 [D loss: 0.999937] [G loss: 1.000011]\n",
      "1213 [D loss: 0.999938] [G loss: 0.999892]\n",
      "1214 [D loss: 0.999861] [G loss: 1.000122]\n",
      "1215 [D loss: 0.999952] [G loss: 0.999969]\n",
      "1216 [D loss: 1.000013] [G loss: 1.000092]\n",
      "1217 [D loss: 0.999788] [G loss: 0.999921]\n",
      "1218 [D loss: 0.999949] [G loss: 0.999970]\n",
      "1219 [D loss: 0.999874] [G loss: 0.999930]\n",
      "1220 [D loss: 0.999859] [G loss: 0.999993]\n",
      "1221 [D loss: 0.999838] [G loss: 1.000028]\n",
      "1222 [D loss: 0.999893] [G loss: 0.999935]\n",
      "1223 [D loss: 0.999914] [G loss: 1.000007]\n",
      "1224 [D loss: 0.999959] [G loss: 1.000034]\n",
      "1225 [D loss: 0.999936] [G loss: 1.000080]\n",
      "1226 [D loss: 0.999729] [G loss: 1.000074]\n",
      "1227 [D loss: 0.999960] [G loss: 1.000032]\n",
      "1228 [D loss: 0.999962] [G loss: 1.000043]\n",
      "1229 [D loss: 0.999935] [G loss: 1.000039]\n",
      "1230 [D loss: 0.999867] [G loss: 1.000035]\n",
      "1231 [D loss: 0.999872] [G loss: 1.000051]\n",
      "1232 [D loss: 0.999914] [G loss: 1.000011]\n",
      "1233 [D loss: 0.999993] [G loss: 0.999962]\n",
      "1234 [D loss: 0.999918] [G loss: 0.999982]\n",
      "1235 [D loss: 0.999919] [G loss: 1.000005]\n",
      "1236 [D loss: 0.999858] [G loss: 1.000135]\n",
      "1237 [D loss: 0.999943] [G loss: 0.999855]\n",
      "1238 [D loss: 0.999836] [G loss: 1.000076]\n",
      "1239 [D loss: 0.999914] [G loss: 0.999996]\n",
      "1240 [D loss: 0.999895] [G loss: 1.000051]\n",
      "1241 [D loss: 0.999882] [G loss: 1.000025]\n",
      "1242 [D loss: 0.999906] [G loss: 0.999886]\n",
      "1243 [D loss: 0.999922] [G loss: 1.000005]\n",
      "1244 [D loss: 0.999971] [G loss: 1.000013]\n",
      "1245 [D loss: 0.999836] [G loss: 0.999987]\n",
      "1246 [D loss: 0.999919] [G loss: 0.999935]\n",
      "1247 [D loss: 0.999952] [G loss: 0.999870]\n",
      "1248 [D loss: 0.999866] [G loss: 0.999944]\n",
      "1249 [D loss: 0.999848] [G loss: 0.999965]\n",
      "1250 [D loss: 0.999855] [G loss: 0.999878]\n",
      "1251 [D loss: 0.999892] [G loss: 0.999977]\n",
      "1252 [D loss: 0.999894] [G loss: 0.999966]\n",
      "1253 [D loss: 0.999933] [G loss: 0.999928]\n",
      "1254 [D loss: 0.999893] [G loss: 1.000042]\n",
      "1255 [D loss: 0.999933] [G loss: 1.000058]\n",
      "1256 [D loss: 0.999886] [G loss: 1.000036]\n",
      "1257 [D loss: 0.999854] [G loss: 1.000046]\n",
      "1258 [D loss: 0.999909] [G loss: 1.000035]\n",
      "1259 [D loss: 0.999919] [G loss: 0.999968]\n",
      "1260 [D loss: 0.999915] [G loss: 0.999847]\n",
      "1261 [D loss: 0.999869] [G loss: 1.000062]\n",
      "1262 [D loss: 0.999771] [G loss: 1.000077]\n",
      "1263 [D loss: 0.999878] [G loss: 0.999938]\n",
      "1264 [D loss: 0.999836] [G loss: 0.999967]\n",
      "1265 [D loss: 0.999880] [G loss: 0.999907]\n",
      "1266 [D loss: 0.999863] [G loss: 1.000035]\n",
      "1267 [D loss: 0.999943] [G loss: 1.000063]\n",
      "1268 [D loss: 0.999934] [G loss: 1.000049]\n",
      "1269 [D loss: 0.999819] [G loss: 0.999899]\n",
      "1270 [D loss: 0.999889] [G loss: 1.000041]\n",
      "1271 [D loss: 0.999884] [G loss: 0.999962]\n",
      "1272 [D loss: 0.999844] [G loss: 0.999909]\n",
      "1273 [D loss: 0.999975] [G loss: 1.000027]\n",
      "1274 [D loss: 0.999849] [G loss: 0.999973]\n",
      "1275 [D loss: 0.999831] [G loss: 0.999928]\n",
      "1276 [D loss: 0.999895] [G loss: 1.000096]\n",
      "1277 [D loss: 0.999792] [G loss: 1.000077]\n",
      "1278 [D loss: 0.999805] [G loss: 0.999933]\n",
      "1279 [D loss: 0.999884] [G loss: 1.000005]\n",
      "1280 [D loss: 0.999964] [G loss: 1.000061]\n",
      "1281 [D loss: 0.999830] [G loss: 0.999879]\n",
      "1282 [D loss: 0.999978] [G loss: 0.999981]\n",
      "1283 [D loss: 0.999937] [G loss: 0.999967]\n",
      "1284 [D loss: 0.999864] [G loss: 0.999952]\n",
      "1285 [D loss: 1.000025] [G loss: 0.999957]\n",
      "1286 [D loss: 0.999852] [G loss: 1.000003]\n",
      "1287 [D loss: 0.999917] [G loss: 1.000028]\n",
      "1288 [D loss: 0.999861] [G loss: 0.999972]\n",
      "1289 [D loss: 0.999857] [G loss: 0.999919]\n",
      "1290 [D loss: 0.999829] [G loss: 1.000048]\n",
      "1291 [D loss: 0.999852] [G loss: 0.999987]\n",
      "1292 [D loss: 0.999873] [G loss: 0.999904]\n",
      "1293 [D loss: 0.999937] [G loss: 0.999974]\n",
      "1294 [D loss: 0.999965] [G loss: 0.999829]\n",
      "1295 [D loss: 0.999897] [G loss: 0.999993]\n",
      "1296 [D loss: 0.999989] [G loss: 0.999998]\n",
      "1297 [D loss: 0.999938] [G loss: 0.999907]\n",
      "1298 [D loss: 0.999848] [G loss: 1.000064]\n",
      "1299 [D loss: 0.999952] [G loss: 0.999893]\n",
      "1300 [D loss: 0.999815] [G loss: 0.999968]\n",
      "1301 [D loss: 0.999939] [G loss: 1.000089]\n",
      "1302 [D loss: 0.999864] [G loss: 1.000004]\n",
      "1303 [D loss: 0.999873] [G loss: 1.000090]\n",
      "1304 [D loss: 0.999907] [G loss: 1.000039]\n",
      "1305 [D loss: 0.999894] [G loss: 1.000082]\n",
      "1306 [D loss: 0.999951] [G loss: 0.999891]\n",
      "1307 [D loss: 1.000007] [G loss: 1.000116]\n",
      "1308 [D loss: 0.999900] [G loss: 0.999887]\n",
      "1309 [D loss: 0.999999] [G loss: 0.999909]\n",
      "1310 [D loss: 0.999933] [G loss: 1.000015]\n",
      "1311 [D loss: 0.999928] [G loss: 0.999786]\n",
      "1312 [D loss: 0.999804] [G loss: 0.999901]\n",
      "1313 [D loss: 0.999947] [G loss: 0.999969]\n",
      "1314 [D loss: 0.999894] [G loss: 1.000125]\n",
      "1315 [D loss: 0.999898] [G loss: 0.999975]\n",
      "1316 [D loss: 0.999928] [G loss: 1.000088]\n",
      "1317 [D loss: 0.999936] [G loss: 0.999967]\n",
      "1318 [D loss: 0.999893] [G loss: 0.999975]\n",
      "1319 [D loss: 0.999839] [G loss: 1.000042]\n",
      "1320 [D loss: 0.999830] [G loss: 1.000045]\n",
      "1321 [D loss: 0.999870] [G loss: 0.999981]\n",
      "1322 [D loss: 0.999899] [G loss: 0.999991]\n",
      "1323 [D loss: 0.999778] [G loss: 0.999920]\n",
      "1324 [D loss: 0.999891] [G loss: 0.999921]\n",
      "1325 [D loss: 0.999908] [G loss: 0.999939]\n",
      "1326 [D loss: 1.000049] [G loss: 0.999979]\n",
      "1327 [D loss: 0.999959] [G loss: 1.000071]\n",
      "1328 [D loss: 0.999988] [G loss: 0.999989]\n",
      "1329 [D loss: 0.999980] [G loss: 1.000004]\n",
      "1330 [D loss: 0.999955] [G loss: 0.999899]\n",
      "1331 [D loss: 0.999978] [G loss: 0.999954]\n",
      "1332 [D loss: 0.999997] [G loss: 1.000036]\n",
      "1333 [D loss: 0.999918] [G loss: 0.999948]\n",
      "1334 [D loss: 0.999984] [G loss: 1.000005]\n",
      "1335 [D loss: 0.999815] [G loss: 1.000055]\n",
      "1336 [D loss: 0.999961] [G loss: 0.999982]\n",
      "1337 [D loss: 0.999829] [G loss: 0.999932]\n",
      "1338 [D loss: 0.999880] [G loss: 1.000078]\n",
      "1339 [D loss: 0.999899] [G loss: 0.999959]\n",
      "1340 [D loss: 0.999889] [G loss: 0.999965]\n",
      "1341 [D loss: 0.999959] [G loss: 0.999983]\n",
      "1342 [D loss: 0.999843] [G loss: 1.000050]\n",
      "1343 [D loss: 0.999880] [G loss: 0.999851]\n",
      "1344 [D loss: 0.999880] [G loss: 0.999943]\n",
      "1345 [D loss: 0.999895] [G loss: 1.000085]\n",
      "1346 [D loss: 0.999940] [G loss: 0.999901]\n",
      "1347 [D loss: 0.999812] [G loss: 1.000021]\n",
      "1348 [D loss: 0.999938] [G loss: 0.999995]\n",
      "1349 [D loss: 0.999942] [G loss: 0.999888]\n",
      "1350 [D loss: 0.999969] [G loss: 0.999719]\n",
      "1351 [D loss: 0.999916] [G loss: 0.999885]\n",
      "1352 [D loss: 0.999873] [G loss: 0.999954]\n",
      "1353 [D loss: 0.999941] [G loss: 1.000099]\n",
      "1354 [D loss: 0.999903] [G loss: 1.000007]\n",
      "1355 [D loss: 0.999911] [G loss: 0.999942]\n",
      "1356 [D loss: 0.999904] [G loss: 1.000116]\n",
      "1357 [D loss: 0.999909] [G loss: 1.000031]\n",
      "1358 [D loss: 0.999933] [G loss: 1.000021]\n",
      "1359 [D loss: 0.999901] [G loss: 0.999927]\n",
      "1360 [D loss: 0.999946] [G loss: 1.000169]\n",
      "1361 [D loss: 0.999784] [G loss: 0.999868]\n",
      "1362 [D loss: 0.999959] [G loss: 0.999991]\n",
      "1363 [D loss: 0.999892] [G loss: 1.000045]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1364 [D loss: 0.999786] [G loss: 0.999923]\n",
      "1365 [D loss: 0.999849] [G loss: 1.000060]\n",
      "1366 [D loss: 0.999850] [G loss: 0.999918]\n",
      "1367 [D loss: 0.999889] [G loss: 0.999981]\n",
      "1368 [D loss: 0.999851] [G loss: 0.999842]\n",
      "1369 [D loss: 0.999858] [G loss: 1.000082]\n",
      "1370 [D loss: 0.999997] [G loss: 0.999972]\n",
      "1371 [D loss: 0.999902] [G loss: 0.999784]\n",
      "1372 [D loss: 0.999812] [G loss: 0.999955]\n",
      "1373 [D loss: 0.999956] [G loss: 1.000128]\n",
      "1374 [D loss: 0.999926] [G loss: 0.999977]\n",
      "1375 [D loss: 0.999835] [G loss: 0.999858]\n",
      "1376 [D loss: 0.999953] [G loss: 1.000078]\n",
      "1377 [D loss: 0.999890] [G loss: 0.999939]\n",
      "1378 [D loss: 0.999821] [G loss: 1.000067]\n",
      "1379 [D loss: 0.999833] [G loss: 1.000048]\n",
      "1380 [D loss: 0.999888] [G loss: 1.000082]\n",
      "1381 [D loss: 1.000021] [G loss: 1.000018]\n",
      "1382 [D loss: 0.999842] [G loss: 0.999900]\n",
      "1383 [D loss: 0.999871] [G loss: 0.999928]\n",
      "1384 [D loss: 1.000047] [G loss: 1.000083]\n",
      "1385 [D loss: 0.999828] [G loss: 0.999884]\n",
      "1386 [D loss: 0.999860] [G loss: 0.999977]\n",
      "1387 [D loss: 0.999795] [G loss: 1.000047]\n",
      "1388 [D loss: 0.999829] [G loss: 0.999940]\n",
      "1389 [D loss: 0.999919] [G loss: 0.999861]\n",
      "1390 [D loss: 0.999943] [G loss: 1.000012]\n",
      "1391 [D loss: 0.999871] [G loss: 0.999852]\n",
      "1392 [D loss: 0.999958] [G loss: 0.999937]\n",
      "1393 [D loss: 0.999952] [G loss: 0.999906]\n",
      "1394 [D loss: 0.999915] [G loss: 0.999920]\n",
      "1395 [D loss: 0.999783] [G loss: 0.999988]\n",
      "1396 [D loss: 0.999965] [G loss: 0.999952]\n",
      "1397 [D loss: 0.999933] [G loss: 0.999901]\n",
      "1398 [D loss: 0.999893] [G loss: 1.000087]\n",
      "1399 [D loss: 0.999853] [G loss: 1.000092]\n",
      "1400 [D loss: 0.999935] [G loss: 0.999987]\n",
      "1401 [D loss: 0.999875] [G loss: 0.999846]\n",
      "1402 [D loss: 0.999969] [G loss: 1.000041]\n",
      "1403 [D loss: 0.999845] [G loss: 1.000028]\n",
      "1404 [D loss: 0.999865] [G loss: 1.000066]\n",
      "1405 [D loss: 0.999967] [G loss: 0.999943]\n",
      "1406 [D loss: 0.999957] [G loss: 0.999979]\n",
      "1407 [D loss: 0.999859] [G loss: 1.000099]\n",
      "1408 [D loss: 0.999853] [G loss: 0.999903]\n",
      "1409 [D loss: 0.999947] [G loss: 0.999938]\n",
      "1410 [D loss: 0.999980] [G loss: 0.999985]\n",
      "1411 [D loss: 0.999895] [G loss: 0.999923]\n",
      "1412 [D loss: 0.999915] [G loss: 1.000098]\n",
      "1413 [D loss: 0.999872] [G loss: 0.999912]\n",
      "1414 [D loss: 0.999995] [G loss: 0.999904]\n",
      "1415 [D loss: 0.999854] [G loss: 1.000017]\n",
      "1416 [D loss: 1.000024] [G loss: 1.000133]\n",
      "1417 [D loss: 0.999897] [G loss: 1.000022]\n",
      "1418 [D loss: 0.999933] [G loss: 1.000009]\n",
      "1419 [D loss: 0.999898] [G loss: 1.000048]\n",
      "1420 [D loss: 1.000002] [G loss: 0.999970]\n",
      "1421 [D loss: 0.999909] [G loss: 0.999970]\n",
      "1422 [D loss: 0.999895] [G loss: 0.999947]\n",
      "1423 [D loss: 0.999905] [G loss: 1.000076]\n",
      "1424 [D loss: 0.999867] [G loss: 1.000072]\n",
      "1425 [D loss: 0.999937] [G loss: 0.999960]\n",
      "1426 [D loss: 0.999897] [G loss: 0.999937]\n",
      "1427 [D loss: 0.999918] [G loss: 1.000157]\n",
      "1428 [D loss: 0.999940] [G loss: 0.999980]\n",
      "1429 [D loss: 0.999921] [G loss: 1.000016]\n",
      "1430 [D loss: 0.999933] [G loss: 1.000105]\n",
      "1431 [D loss: 0.999883] [G loss: 0.999966]\n",
      "1432 [D loss: 0.999869] [G loss: 1.000014]\n",
      "1433 [D loss: 0.999842] [G loss: 1.000040]\n",
      "1434 [D loss: 0.999980] [G loss: 0.999996]\n",
      "1435 [D loss: 0.999912] [G loss: 1.000044]\n",
      "1436 [D loss: 0.999951] [G loss: 1.000069]\n",
      "1437 [D loss: 0.999924] [G loss: 1.000007]\n",
      "1438 [D loss: 0.999832] [G loss: 1.000001]\n",
      "1439 [D loss: 0.999855] [G loss: 1.000040]\n",
      "1440 [D loss: 0.999948] [G loss: 0.999924]\n",
      "1441 [D loss: 0.999901] [G loss: 1.000048]\n",
      "1442 [D loss: 0.999882] [G loss: 1.000125]\n",
      "1443 [D loss: 0.999943] [G loss: 1.000039]\n",
      "1444 [D loss: 0.999899] [G loss: 1.000022]\n",
      "1445 [D loss: 0.999894] [G loss: 0.999954]\n",
      "1446 [D loss: 0.999881] [G loss: 1.000084]\n",
      "1447 [D loss: 0.999934] [G loss: 1.000072]\n",
      "1448 [D loss: 0.999936] [G loss: 1.000041]\n",
      "1449 [D loss: 0.999977] [G loss: 1.000045]\n",
      "1450 [D loss: 0.999947] [G loss: 0.999944]\n",
      "1451 [D loss: 0.999876] [G loss: 1.000010]\n",
      "1452 [D loss: 0.999880] [G loss: 0.999980]\n",
      "1453 [D loss: 0.999954] [G loss: 1.000078]\n",
      "1454 [D loss: 0.999891] [G loss: 0.999957]\n",
      "1455 [D loss: 0.999866] [G loss: 1.000026]\n",
      "1456 [D loss: 0.999919] [G loss: 1.000033]\n",
      "1457 [D loss: 0.999953] [G loss: 0.999994]\n",
      "1458 [D loss: 0.999847] [G loss: 0.999993]\n",
      "1459 [D loss: 0.999902] [G loss: 0.999985]\n",
      "1460 [D loss: 0.999952] [G loss: 1.000056]\n",
      "1461 [D loss: 0.999953] [G loss: 0.999896]\n",
      "1462 [D loss: 0.999957] [G loss: 1.000008]\n",
      "1463 [D loss: 0.999912] [G loss: 0.999997]\n",
      "1464 [D loss: 0.999954] [G loss: 0.999996]\n",
      "1465 [D loss: 0.999933] [G loss: 0.999989]\n",
      "1466 [D loss: 0.999940] [G loss: 1.000057]\n",
      "1467 [D loss: 0.999941] [G loss: 0.999934]\n",
      "1468 [D loss: 0.999926] [G loss: 1.000005]\n",
      "1469 [D loss: 0.999983] [G loss: 1.000005]\n",
      "1470 [D loss: 0.999895] [G loss: 1.000051]\n",
      "1471 [D loss: 0.999827] [G loss: 0.999992]\n",
      "1472 [D loss: 0.999941] [G loss: 0.999988]\n",
      "1473 [D loss: 1.000004] [G loss: 1.000086]\n",
      "1474 [D loss: 0.999907] [G loss: 1.000045]\n",
      "1475 [D loss: 0.999910] [G loss: 1.000054]\n",
      "1476 [D loss: 0.999979] [G loss: 0.999884]\n",
      "1477 [D loss: 0.999931] [G loss: 1.000128]\n",
      "1478 [D loss: 0.999971] [G loss: 0.999977]\n",
      "1479 [D loss: 0.999956] [G loss: 0.999979]\n",
      "1480 [D loss: 0.999915] [G loss: 0.999941]\n",
      "1481 [D loss: 0.999975] [G loss: 1.000013]\n",
      "1482 [D loss: 0.999968] [G loss: 0.999988]\n",
      "1483 [D loss: 0.999875] [G loss: 1.000015]\n",
      "1484 [D loss: 0.999881] [G loss: 1.000050]\n",
      "1485 [D loss: 0.999913] [G loss: 0.999961]\n",
      "1486 [D loss: 0.999870] [G loss: 1.000069]\n",
      "1487 [D loss: 0.999928] [G loss: 0.999988]\n",
      "1488 [D loss: 0.999882] [G loss: 0.999971]\n",
      "1489 [D loss: 0.999901] [G loss: 0.999942]\n",
      "1490 [D loss: 0.999924] [G loss: 1.000020]\n",
      "1491 [D loss: 0.999865] [G loss: 0.999977]\n",
      "1492 [D loss: 0.999990] [G loss: 1.000041]\n",
      "1493 [D loss: 0.999951] [G loss: 1.000059]\n",
      "1494 [D loss: 0.999982] [G loss: 0.999864]\n",
      "1495 [D loss: 0.999957] [G loss: 0.999975]\n",
      "1496 [D loss: 0.999863] [G loss: 0.999983]\n",
      "1497 [D loss: 0.999961] [G loss: 0.999952]\n",
      "1498 [D loss: 0.999890] [G loss: 1.000005]\n",
      "1499 [D loss: 0.999934] [G loss: 1.000056]\n",
      "1500 [D loss: 0.999942] [G loss: 1.000045]\n",
      "1501 [D loss: 0.999919] [G loss: 1.000045]\n",
      "1502 [D loss: 0.999907] [G loss: 0.999957]\n",
      "1503 [D loss: 0.999938] [G loss: 0.999954]\n",
      "1504 [D loss: 0.999860] [G loss: 1.000059]\n",
      "1505 [D loss: 0.999895] [G loss: 0.999904]\n",
      "1506 [D loss: 0.999966] [G loss: 0.999928]\n",
      "1507 [D loss: 0.999878] [G loss: 1.000088]\n",
      "1508 [D loss: 0.999955] [G loss: 0.999986]\n",
      "1509 [D loss: 0.999922] [G loss: 0.999968]\n",
      "1510 [D loss: 0.999981] [G loss: 1.000088]\n",
      "1511 [D loss: 0.999898] [G loss: 1.000049]\n",
      "1512 [D loss: 0.999921] [G loss: 1.000004]\n",
      "1513 [D loss: 0.999969] [G loss: 1.000064]\n",
      "1514 [D loss: 0.999887] [G loss: 1.000099]\n",
      "1515 [D loss: 0.999909] [G loss: 1.000086]\n",
      "1516 [D loss: 0.999942] [G loss: 0.999964]\n",
      "1517 [D loss: 0.999898] [G loss: 1.000048]\n",
      "1518 [D loss: 0.999916] [G loss: 0.999998]\n",
      "1519 [D loss: 0.999952] [G loss: 0.999939]\n",
      "1520 [D loss: 0.999932] [G loss: 1.000052]\n",
      "1521 [D loss: 0.999893] [G loss: 1.000018]\n",
      "1522 [D loss: 0.999916] [G loss: 0.999990]\n",
      "1523 [D loss: 0.999905] [G loss: 0.999922]\n",
      "1524 [D loss: 0.999886] [G loss: 0.999982]\n",
      "1525 [D loss: 0.999979] [G loss: 1.000064]\n",
      "1526 [D loss: 0.999873] [G loss: 0.999990]\n",
      "1527 [D loss: 0.999855] [G loss: 0.999930]\n",
      "1528 [D loss: 0.999928] [G loss: 1.000003]\n",
      "1529 [D loss: 0.999899] [G loss: 1.000072]\n",
      "1530 [D loss: 0.999925] [G loss: 1.000013]\n",
      "1531 [D loss: 0.999954] [G loss: 1.000120]\n",
      "1532 [D loss: 0.999945] [G loss: 1.000083]\n",
      "1533 [D loss: 0.999943] [G loss: 0.999903]\n",
      "1534 [D loss: 0.999916] [G loss: 1.000024]\n",
      "1535 [D loss: 0.999870] [G loss: 1.000066]\n",
      "1536 [D loss: 0.999913] [G loss: 1.000054]\n",
      "1537 [D loss: 0.999922] [G loss: 1.000047]\n",
      "1538 [D loss: 0.999915] [G loss: 0.999998]\n",
      "1539 [D loss: 0.999938] [G loss: 0.999997]\n",
      "1540 [D loss: 0.999947] [G loss: 1.000006]\n",
      "1541 [D loss: 0.999918] [G loss: 1.000043]\n",
      "1542 [D loss: 0.999926] [G loss: 0.999990]\n",
      "1543 [D loss: 0.999892] [G loss: 1.000168]\n",
      "1544 [D loss: 0.999943] [G loss: 0.999987]\n",
      "1545 [D loss: 0.999907] [G loss: 1.000097]\n",
      "1546 [D loss: 1.000022] [G loss: 1.000062]\n",
      "1547 [D loss: 0.999991] [G loss: 0.999982]\n",
      "1548 [D loss: 0.999927] [G loss: 1.000031]\n",
      "1549 [D loss: 0.999923] [G loss: 0.999969]\n",
      "1550 [D loss: 0.999906] [G loss: 1.000115]\n",
      "1551 [D loss: 0.999949] [G loss: 1.000004]\n",
      "1552 [D loss: 0.999923] [G loss: 0.999948]\n",
      "1553 [D loss: 0.999933] [G loss: 1.000030]\n",
      "1554 [D loss: 0.999976] [G loss: 0.999931]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1555 [D loss: 0.999810] [G loss: 0.999883]\n",
      "1556 [D loss: 1.000032] [G loss: 0.999956]\n",
      "1557 [D loss: 0.999926] [G loss: 1.000024]\n",
      "1558 [D loss: 0.999953] [G loss: 0.999868]\n",
      "1559 [D loss: 0.999924] [G loss: 0.999990]\n",
      "1560 [D loss: 0.999953] [G loss: 1.000037]\n",
      "1561 [D loss: 0.999904] [G loss: 1.000020]\n",
      "1562 [D loss: 0.999852] [G loss: 0.999964]\n",
      "1563 [D loss: 0.999907] [G loss: 1.000076]\n",
      "1564 [D loss: 0.999828] [G loss: 1.000040]\n",
      "1565 [D loss: 0.999932] [G loss: 1.000005]\n",
      "1566 [D loss: 0.999921] [G loss: 0.999801]\n",
      "1567 [D loss: 0.999882] [G loss: 0.999934]\n",
      "1568 [D loss: 1.000007] [G loss: 0.999983]\n",
      "1569 [D loss: 0.999835] [G loss: 1.000093]\n",
      "1570 [D loss: 0.999971] [G loss: 0.999977]\n",
      "1571 [D loss: 0.999868] [G loss: 1.000003]\n",
      "1572 [D loss: 1.000023] [G loss: 1.000054]\n",
      "1573 [D loss: 0.999966] [G loss: 1.000053]\n",
      "1574 [D loss: 0.999917] [G loss: 1.000099]\n",
      "1575 [D loss: 0.999991] [G loss: 1.000028]\n",
      "1576 [D loss: 1.000026] [G loss: 1.000036]\n",
      "1577 [D loss: 0.999907] [G loss: 1.000033]\n",
      "1578 [D loss: 0.999910] [G loss: 1.000045]\n",
      "1579 [D loss: 0.999911] [G loss: 1.000030]\n",
      "1580 [D loss: 0.999937] [G loss: 1.000020]\n",
      "1581 [D loss: 0.999902] [G loss: 0.999877]\n",
      "1582 [D loss: 0.999960] [G loss: 1.000008]\n",
      "1583 [D loss: 0.999929] [G loss: 1.000041]\n",
      "1584 [D loss: 0.999887] [G loss: 0.999986]\n",
      "1585 [D loss: 0.999913] [G loss: 1.000037]\n",
      "1586 [D loss: 0.999953] [G loss: 0.999989]\n",
      "1587 [D loss: 0.999893] [G loss: 0.999978]\n",
      "1588 [D loss: 0.999956] [G loss: 1.000059]\n",
      "1589 [D loss: 0.999971] [G loss: 0.999990]\n",
      "1590 [D loss: 0.999970] [G loss: 1.000038]\n",
      "1591 [D loss: 0.999931] [G loss: 1.000006]\n",
      "1592 [D loss: 0.999911] [G loss: 0.999995]\n",
      "1593 [D loss: 0.999984] [G loss: 0.999896]\n",
      "1594 [D loss: 0.999933] [G loss: 1.000013]\n",
      "1595 [D loss: 0.999989] [G loss: 0.999964]\n",
      "1596 [D loss: 0.999970] [G loss: 0.999995]\n",
      "1597 [D loss: 0.999937] [G loss: 0.999987]\n",
      "1598 [D loss: 0.999920] [G loss: 1.000043]\n",
      "1599 [D loss: 0.999980] [G loss: 0.999944]\n",
      "1600 [D loss: 0.999966] [G loss: 1.000024]\n",
      "1601 [D loss: 0.999919] [G loss: 1.000044]\n",
      "1602 [D loss: 0.999915] [G loss: 1.000011]\n",
      "1603 [D loss: 0.999911] [G loss: 0.999963]\n",
      "1604 [D loss: 1.000028] [G loss: 0.999918]\n",
      "1605 [D loss: 0.999832] [G loss: 0.999947]\n",
      "1606 [D loss: 0.999961] [G loss: 0.999897]\n",
      "1607 [D loss: 0.999866] [G loss: 1.000004]\n",
      "1608 [D loss: 0.999914] [G loss: 0.999939]\n",
      "1609 [D loss: 0.999873] [G loss: 1.000073]\n",
      "1610 [D loss: 0.999944] [G loss: 0.999902]\n",
      "1611 [D loss: 0.999934] [G loss: 0.999967]\n",
      "1612 [D loss: 0.999985] [G loss: 1.000056]\n",
      "1613 [D loss: 0.999913] [G loss: 0.999914]\n",
      "1614 [D loss: 0.999847] [G loss: 1.000005]\n",
      "1615 [D loss: 0.999941] [G loss: 0.999910]\n",
      "1616 [D loss: 0.999821] [G loss: 1.000066]\n",
      "1617 [D loss: 0.999895] [G loss: 0.999973]\n",
      "1618 [D loss: 0.999905] [G loss: 0.999975]\n",
      "1619 [D loss: 0.999894] [G loss: 0.999981]\n",
      "1620 [D loss: 0.999885] [G loss: 0.999886]\n",
      "1621 [D loss: 0.999897] [G loss: 0.999934]\n",
      "1622 [D loss: 0.999859] [G loss: 0.999993]\n",
      "1623 [D loss: 0.999948] [G loss: 1.000013]\n",
      "1624 [D loss: 0.999910] [G loss: 0.999953]\n",
      "1625 [D loss: 0.999881] [G loss: 1.000047]\n",
      "1626 [D loss: 0.999983] [G loss: 1.000120]\n",
      "1627 [D loss: 0.999906] [G loss: 1.000051]\n",
      "1628 [D loss: 0.999904] [G loss: 1.000046]\n",
      "1629 [D loss: 0.999955] [G loss: 0.999794]\n",
      "1630 [D loss: 0.999939] [G loss: 0.999965]\n",
      "1631 [D loss: 0.999955] [G loss: 0.999974]\n",
      "1632 [D loss: 0.999860] [G loss: 1.000049]\n",
      "1633 [D loss: 0.999887] [G loss: 0.999962]\n",
      "1634 [D loss: 0.999864] [G loss: 1.000116]\n",
      "1635 [D loss: 0.999934] [G loss: 1.000052]\n",
      "1636 [D loss: 1.000010] [G loss: 0.999906]\n",
      "1637 [D loss: 0.999957] [G loss: 0.999940]\n",
      "1638 [D loss: 1.000013] [G loss: 0.999966]\n",
      "1639 [D loss: 0.999855] [G loss: 0.999843]\n",
      "1640 [D loss: 0.999910] [G loss: 0.999963]\n",
      "1641 [D loss: 0.999869] [G loss: 0.999987]\n",
      "1642 [D loss: 0.999944] [G loss: 0.999998]\n",
      "1643 [D loss: 1.000055] [G loss: 0.999969]\n",
      "1644 [D loss: 0.999961] [G loss: 1.000089]\n",
      "1645 [D loss: 0.999897] [G loss: 0.999930]\n",
      "1646 [D loss: 0.999941] [G loss: 0.999943]\n",
      "1647 [D loss: 0.999897] [G loss: 0.999948]\n",
      "1648 [D loss: 0.999861] [G loss: 1.000006]\n",
      "1649 [D loss: 0.999951] [G loss: 0.999907]\n",
      "1650 [D loss: 0.999990] [G loss: 0.999975]\n",
      "1651 [D loss: 0.999943] [G loss: 0.999910]\n",
      "1652 [D loss: 0.999977] [G loss: 0.999927]\n",
      "1653 [D loss: 0.999918] [G loss: 0.999981]\n",
      "1654 [D loss: 0.999990] [G loss: 1.000024]\n",
      "1655 [D loss: 0.999926] [G loss: 0.999934]\n",
      "1656 [D loss: 0.999955] [G loss: 1.000044]\n",
      "1657 [D loss: 0.999919] [G loss: 0.999909]\n",
      "1658 [D loss: 1.000002] [G loss: 0.999855]\n",
      "1659 [D loss: 0.999906] [G loss: 0.999875]\n",
      "1660 [D loss: 0.999959] [G loss: 0.999936]\n",
      "1661 [D loss: 0.999889] [G loss: 0.999993]\n",
      "1662 [D loss: 0.999975] [G loss: 0.999956]\n",
      "1663 [D loss: 0.999994] [G loss: 1.000000]\n",
      "1664 [D loss: 0.999940] [G loss: 0.999974]\n",
      "1665 [D loss: 0.999948] [G loss: 0.999974]\n",
      "1666 [D loss: 0.999911] [G loss: 1.000032]\n",
      "1667 [D loss: 0.999910] [G loss: 0.999861]\n",
      "1668 [D loss: 0.999913] [G loss: 0.999948]\n",
      "1669 [D loss: 0.999891] [G loss: 0.999992]\n",
      "1670 [D loss: 0.999813] [G loss: 0.999958]\n",
      "1671 [D loss: 0.999926] [G loss: 0.999979]\n",
      "1672 [D loss: 0.999907] [G loss: 0.999906]\n",
      "1673 [D loss: 0.999919] [G loss: 1.000099]\n",
      "1674 [D loss: 0.999940] [G loss: 1.000089]\n",
      "1675 [D loss: 0.999892] [G loss: 1.000007]\n",
      "1676 [D loss: 0.999957] [G loss: 1.000127]\n",
      "1677 [D loss: 0.999936] [G loss: 1.000039]\n",
      "1678 [D loss: 0.999937] [G loss: 1.000008]\n",
      "1679 [D loss: 0.999897] [G loss: 1.000078]\n",
      "1680 [D loss: 0.999968] [G loss: 1.000007]\n",
      "1681 [D loss: 0.999968] [G loss: 0.999984]\n",
      "1682 [D loss: 0.999906] [G loss: 0.999994]\n",
      "1683 [D loss: 0.999948] [G loss: 0.999984]\n",
      "1684 [D loss: 0.999967] [G loss: 1.000069]\n",
      "1685 [D loss: 0.999902] [G loss: 1.000058]\n",
      "1686 [D loss: 0.999978] [G loss: 0.999969]\n",
      "1687 [D loss: 0.999896] [G loss: 1.000015]\n",
      "1688 [D loss: 0.999883] [G loss: 0.999985]\n",
      "1689 [D loss: 0.999885] [G loss: 1.000022]\n",
      "1690 [D loss: 0.999899] [G loss: 1.000033]\n",
      "1691 [D loss: 0.999901] [G loss: 1.000058]\n",
      "1692 [D loss: 0.999925] [G loss: 0.999845]\n",
      "1693 [D loss: 0.999963] [G loss: 1.000041]\n",
      "1694 [D loss: 0.999936] [G loss: 0.999981]\n",
      "1695 [D loss: 0.999798] [G loss: 0.999916]\n",
      "1696 [D loss: 0.999947] [G loss: 0.999986]\n",
      "1697 [D loss: 0.999892] [G loss: 1.000048]\n",
      "1698 [D loss: 0.999968] [G loss: 1.000057]\n",
      "1699 [D loss: 0.999923] [G loss: 1.000069]\n",
      "1700 [D loss: 0.999924] [G loss: 1.000061]\n",
      "1701 [D loss: 0.999936] [G loss: 1.000010]\n",
      "1702 [D loss: 0.999939] [G loss: 1.000022]\n",
      "1703 [D loss: 0.999982] [G loss: 0.999921]\n",
      "1704 [D loss: 0.999928] [G loss: 1.000042]\n",
      "1705 [D loss: 0.999977] [G loss: 1.000010]\n",
      "1706 [D loss: 0.999917] [G loss: 1.000058]\n",
      "1707 [D loss: 0.999882] [G loss: 1.000115]\n",
      "1708 [D loss: 0.999934] [G loss: 0.999940]\n",
      "1709 [D loss: 0.999929] [G loss: 1.000075]\n",
      "1710 [D loss: 0.999945] [G loss: 0.999979]\n",
      "1711 [D loss: 0.999903] [G loss: 1.000081]\n",
      "1712 [D loss: 0.999910] [G loss: 1.000115]\n",
      "1713 [D loss: 0.999936] [G loss: 1.000023]\n",
      "1714 [D loss: 0.999971] [G loss: 0.999964]\n",
      "1715 [D loss: 0.999950] [G loss: 1.000047]\n",
      "1716 [D loss: 0.999958] [G loss: 1.000022]\n",
      "1717 [D loss: 0.999887] [G loss: 1.000032]\n",
      "1718 [D loss: 0.999940] [G loss: 0.999990]\n",
      "1719 [D loss: 0.999858] [G loss: 1.000007]\n",
      "1720 [D loss: 0.999980] [G loss: 0.999994]\n",
      "1721 [D loss: 0.999930] [G loss: 0.999987]\n",
      "1722 [D loss: 0.999985] [G loss: 0.999996]\n",
      "1723 [D loss: 0.999925] [G loss: 1.000063]\n",
      "1724 [D loss: 0.999957] [G loss: 1.000092]\n",
      "1725 [D loss: 0.999938] [G loss: 1.000018]\n",
      "1726 [D loss: 0.999926] [G loss: 0.999999]\n",
      "1727 [D loss: 0.999927] [G loss: 1.000027]\n",
      "1728 [D loss: 0.999929] [G loss: 1.000014]\n",
      "1729 [D loss: 0.999986] [G loss: 1.000033]\n",
      "1730 [D loss: 0.999972] [G loss: 0.999925]\n",
      "1731 [D loss: 0.999913] [G loss: 0.999948]\n",
      "1732 [D loss: 0.999943] [G loss: 0.999983]\n",
      "1733 [D loss: 0.999931] [G loss: 0.999985]\n",
      "1734 [D loss: 0.999965] [G loss: 0.999992]\n",
      "1735 [D loss: 1.000007] [G loss: 1.000027]\n",
      "1736 [D loss: 0.999976] [G loss: 0.999954]\n",
      "1737 [D loss: 1.000018] [G loss: 1.000003]\n",
      "1738 [D loss: 0.999944] [G loss: 0.999998]\n",
      "1739 [D loss: 0.999938] [G loss: 0.999984]\n",
      "1740 [D loss: 0.999954] [G loss: 1.000101]\n",
      "1741 [D loss: 0.999896] [G loss: 0.999957]\n",
      "1742 [D loss: 0.999974] [G loss: 0.999939]\n",
      "1743 [D loss: 0.999918] [G loss: 1.000005]\n",
      "1744 [D loss: 0.999922] [G loss: 1.000074]\n",
      "1745 [D loss: 0.999965] [G loss: 0.999966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1746 [D loss: 0.999982] [G loss: 1.000079]\n",
      "1747 [D loss: 0.999966] [G loss: 1.000095]\n",
      "1748 [D loss: 0.999875] [G loss: 1.000047]\n",
      "1749 [D loss: 0.999941] [G loss: 0.999998]\n",
      "1750 [D loss: 0.999945] [G loss: 0.999990]\n",
      "1751 [D loss: 0.999892] [G loss: 0.999917]\n",
      "1752 [D loss: 0.999886] [G loss: 0.999972]\n",
      "1753 [D loss: 1.000005] [G loss: 1.000049]\n",
      "1754 [D loss: 0.999934] [G loss: 1.000035]\n",
      "1755 [D loss: 0.999950] [G loss: 0.999981]\n",
      "1756 [D loss: 0.999986] [G loss: 1.000117]\n",
      "1757 [D loss: 0.999935] [G loss: 0.999998]\n",
      "1758 [D loss: 0.999945] [G loss: 0.999976]\n",
      "1759 [D loss: 0.999948] [G loss: 1.000002]\n",
      "1760 [D loss: 0.999917] [G loss: 1.000032]\n",
      "1761 [D loss: 0.999938] [G loss: 1.000023]\n",
      "1762 [D loss: 0.999906] [G loss: 1.000090]\n",
      "1763 [D loss: 0.999948] [G loss: 0.999995]\n",
      "1764 [D loss: 0.999975] [G loss: 1.000034]\n",
      "1765 [D loss: 0.999881] [G loss: 1.000152]\n",
      "1766 [D loss: 0.999915] [G loss: 0.999918]\n",
      "1767 [D loss: 1.000008] [G loss: 1.000031]\n",
      "1768 [D loss: 0.999931] [G loss: 1.000069]\n",
      "1769 [D loss: 1.000025] [G loss: 0.999992]\n",
      "1770 [D loss: 0.999901] [G loss: 0.999952]\n",
      "1771 [D loss: 0.999934] [G loss: 1.000060]\n",
      "1772 [D loss: 0.999922] [G loss: 0.999921]\n",
      "1773 [D loss: 0.999880] [G loss: 1.000087]\n",
      "1774 [D loss: 0.999896] [G loss: 0.999937]\n",
      "1775 [D loss: 0.999999] [G loss: 1.000006]\n",
      "1776 [D loss: 0.999966] [G loss: 0.999982]\n",
      "1777 [D loss: 0.999926] [G loss: 0.999966]\n",
      "1778 [D loss: 0.999931] [G loss: 0.999947]\n",
      "1779 [D loss: 0.999974] [G loss: 0.999985]\n",
      "1780 [D loss: 0.999996] [G loss: 0.999891]\n",
      "1781 [D loss: 0.999975] [G loss: 0.999992]\n",
      "1782 [D loss: 0.999954] [G loss: 1.000073]\n",
      "1783 [D loss: 0.999909] [G loss: 1.000014]\n",
      "1784 [D loss: 0.999993] [G loss: 0.999930]\n",
      "1785 [D loss: 0.999869] [G loss: 1.000067]\n",
      "1786 [D loss: 0.999910] [G loss: 0.999992]\n",
      "1787 [D loss: 0.999963] [G loss: 1.000056]\n",
      "1788 [D loss: 0.999899] [G loss: 0.999983]\n",
      "1789 [D loss: 0.999980] [G loss: 0.999930]\n",
      "1790 [D loss: 0.999954] [G loss: 0.999942]\n",
      "1791 [D loss: 0.999964] [G loss: 0.999943]\n",
      "1792 [D loss: 0.999976] [G loss: 0.999987]\n",
      "1793 [D loss: 0.999939] [G loss: 0.999956]\n",
      "1794 [D loss: 0.999962] [G loss: 1.000023]\n",
      "1795 [D loss: 0.999902] [G loss: 0.999967]\n",
      "1796 [D loss: 0.999945] [G loss: 0.999967]\n",
      "1797 [D loss: 0.999908] [G loss: 1.000054]\n",
      "1798 [D loss: 0.999938] [G loss: 1.000035]\n",
      "1799 [D loss: 0.999900] [G loss: 1.000039]\n",
      "1800 [D loss: 0.999897] [G loss: 1.000045]\n",
      "1801 [D loss: 0.999920] [G loss: 1.000075]\n",
      "1802 [D loss: 0.999932] [G loss: 0.999938]\n",
      "1803 [D loss: 0.999910] [G loss: 0.999937]\n",
      "1804 [D loss: 0.999968] [G loss: 1.000052]\n",
      "1805 [D loss: 0.999912] [G loss: 1.000055]\n",
      "1806 [D loss: 0.999956] [G loss: 1.000022]\n",
      "1807 [D loss: 0.999926] [G loss: 1.000014]\n",
      "1808 [D loss: 0.999922] [G loss: 0.999939]\n",
      "1809 [D loss: 0.999954] [G loss: 1.000010]\n",
      "1810 [D loss: 0.999961] [G loss: 1.000021]\n",
      "1811 [D loss: 0.999924] [G loss: 1.000033]\n",
      "1812 [D loss: 0.999995] [G loss: 0.999971]\n",
      "1813 [D loss: 0.999941] [G loss: 1.000026]\n",
      "1814 [D loss: 0.999925] [G loss: 1.000031]\n",
      "1815 [D loss: 0.999927] [G loss: 1.000041]\n",
      "1816 [D loss: 0.999882] [G loss: 0.999945]\n",
      "1817 [D loss: 0.999905] [G loss: 1.000013]\n",
      "1818 [D loss: 1.000004] [G loss: 1.000028]\n",
      "1819 [D loss: 0.999916] [G loss: 1.000015]\n",
      "1820 [D loss: 0.999902] [G loss: 0.999965]\n",
      "1821 [D loss: 0.999982] [G loss: 1.000084]\n",
      "1822 [D loss: 0.999952] [G loss: 1.000001]\n",
      "1823 [D loss: 0.999993] [G loss: 1.000074]\n",
      "1824 [D loss: 0.999912] [G loss: 0.999995]\n",
      "1825 [D loss: 0.999909] [G loss: 1.000021]\n",
      "1826 [D loss: 0.999954] [G loss: 0.999981]\n",
      "1827 [D loss: 0.999883] [G loss: 0.999971]\n",
      "1828 [D loss: 0.999946] [G loss: 1.000033]\n",
      "1829 [D loss: 0.999948] [G loss: 1.000075]\n",
      "1830 [D loss: 0.999946] [G loss: 0.999927]\n",
      "1831 [D loss: 0.999889] [G loss: 0.999992]\n",
      "1832 [D loss: 0.999900] [G loss: 1.000037]\n",
      "1833 [D loss: 0.999898] [G loss: 0.999970]\n",
      "1834 [D loss: 0.999938] [G loss: 1.000056]\n",
      "1835 [D loss: 0.999922] [G loss: 1.000034]\n",
      "1836 [D loss: 0.999977] [G loss: 0.999992]\n",
      "1837 [D loss: 0.999941] [G loss: 0.999970]\n",
      "1838 [D loss: 0.999976] [G loss: 1.000063]\n",
      "1839 [D loss: 0.999903] [G loss: 1.000166]\n",
      "1840 [D loss: 1.000033] [G loss: 0.999922]\n",
      "1841 [D loss: 0.999934] [G loss: 0.999998]\n",
      "1842 [D loss: 0.999959] [G loss: 0.999908]\n",
      "1843 [D loss: 0.999953] [G loss: 0.999994]\n",
      "1844 [D loss: 0.999882] [G loss: 1.000033]\n",
      "1845 [D loss: 0.999925] [G loss: 0.999966]\n",
      "1846 [D loss: 0.999956] [G loss: 0.999995]\n",
      "1847 [D loss: 0.999906] [G loss: 0.999932]\n",
      "1848 [D loss: 0.999928] [G loss: 1.000009]\n",
      "1849 [D loss: 0.999898] [G loss: 1.000079]\n",
      "1850 [D loss: 0.999935] [G loss: 1.000009]\n",
      "1851 [D loss: 0.999966] [G loss: 0.999989]\n",
      "1852 [D loss: 0.999917] [G loss: 1.000084]\n",
      "1853 [D loss: 0.999987] [G loss: 1.000001]\n",
      "1854 [D loss: 0.999965] [G loss: 1.000030]\n",
      "1855 [D loss: 0.999931] [G loss: 1.000050]\n",
      "1856 [D loss: 0.999921] [G loss: 1.000055]\n",
      "1857 [D loss: 0.999881] [G loss: 1.000051]\n",
      "1858 [D loss: 0.999943] [G loss: 0.999998]\n",
      "1859 [D loss: 0.999898] [G loss: 1.000093]\n",
      "1860 [D loss: 0.999943] [G loss: 0.999936]\n",
      "1861 [D loss: 0.999961] [G loss: 1.000047]\n",
      "1862 [D loss: 0.999899] [G loss: 1.000076]\n",
      "1863 [D loss: 0.999940] [G loss: 0.999995]\n",
      "1864 [D loss: 0.999987] [G loss: 1.000013]\n",
      "1865 [D loss: 0.999934] [G loss: 1.000036]\n",
      "1866 [D loss: 0.999906] [G loss: 0.999943]\n",
      "1867 [D loss: 0.999979] [G loss: 0.999937]\n",
      "1868 [D loss: 0.999939] [G loss: 1.000015]\n",
      "1869 [D loss: 0.999886] [G loss: 1.000030]\n",
      "1870 [D loss: 0.999889] [G loss: 0.999997]\n",
      "1871 [D loss: 0.999921] [G loss: 1.000075]\n",
      "1872 [D loss: 0.999916] [G loss: 1.000049]\n",
      "1873 [D loss: 0.999834] [G loss: 0.999935]\n",
      "1874 [D loss: 0.999951] [G loss: 0.999975]\n",
      "1875 [D loss: 0.999955] [G loss: 0.999959]\n",
      "1876 [D loss: 0.999902] [G loss: 0.999998]\n",
      "1877 [D loss: 0.999896] [G loss: 1.000000]\n",
      "1878 [D loss: 0.999887] [G loss: 1.000031]\n",
      "1879 [D loss: 0.999897] [G loss: 1.000104]\n",
      "1880 [D loss: 0.999923] [G loss: 0.999997]\n",
      "1881 [D loss: 0.999951] [G loss: 1.000066]\n",
      "1882 [D loss: 0.999992] [G loss: 1.000127]\n",
      "1883 [D loss: 0.999908] [G loss: 1.000019]\n",
      "1884 [D loss: 0.999977] [G loss: 0.999969]\n",
      "1885 [D loss: 0.999969] [G loss: 0.999906]\n",
      "1886 [D loss: 0.999887] [G loss: 1.000079]\n",
      "1887 [D loss: 0.999935] [G loss: 0.999991]\n",
      "1888 [D loss: 0.999909] [G loss: 1.000021]\n",
      "1889 [D loss: 0.999975] [G loss: 1.000071]\n",
      "1890 [D loss: 0.999827] [G loss: 1.000066]\n",
      "1891 [D loss: 0.999888] [G loss: 1.000024]\n",
      "1892 [D loss: 0.999857] [G loss: 1.000028]\n",
      "1893 [D loss: 0.999992] [G loss: 0.999982]\n",
      "1894 [D loss: 0.999960] [G loss: 1.000070]\n",
      "1895 [D loss: 0.999937] [G loss: 1.000000]\n",
      "1896 [D loss: 0.999900] [G loss: 1.000078]\n",
      "1897 [D loss: 0.999961] [G loss: 1.000091]\n",
      "1898 [D loss: 0.999910] [G loss: 1.000047]\n",
      "1899 [D loss: 0.999914] [G loss: 1.000007]\n",
      "1900 [D loss: 0.999861] [G loss: 1.000055]\n",
      "1901 [D loss: 0.999968] [G loss: 0.999907]\n",
      "1902 [D loss: 0.999972] [G loss: 0.999984]\n",
      "1903 [D loss: 0.999937] [G loss: 0.999983]\n",
      "1904 [D loss: 0.999886] [G loss: 0.999937]\n",
      "1905 [D loss: 0.999923] [G loss: 1.000068]\n",
      "1906 [D loss: 1.000036] [G loss: 1.000104]\n",
      "1907 [D loss: 0.999928] [G loss: 1.000092]\n",
      "1908 [D loss: 0.999842] [G loss: 0.999997]\n",
      "1909 [D loss: 0.999866] [G loss: 1.000089]\n",
      "1910 [D loss: 0.999967] [G loss: 1.000133]\n",
      "1911 [D loss: 0.999885] [G loss: 0.999993]\n",
      "1912 [D loss: 0.999929] [G loss: 1.000054]\n",
      "1913 [D loss: 0.999918] [G loss: 0.999999]\n",
      "1914 [D loss: 0.999946] [G loss: 1.000067]\n",
      "1915 [D loss: 0.999937] [G loss: 0.999996]\n",
      "1916 [D loss: 0.999879] [G loss: 0.999824]\n",
      "1917 [D loss: 0.999901] [G loss: 1.000024]\n",
      "1918 [D loss: 0.999940] [G loss: 0.999969]\n",
      "1919 [D loss: 0.999818] [G loss: 1.000086]\n",
      "1920 [D loss: 0.999933] [G loss: 1.000066]\n",
      "1921 [D loss: 0.999981] [G loss: 1.000101]\n",
      "1922 [D loss: 0.999918] [G loss: 1.000021]\n",
      "1923 [D loss: 0.999896] [G loss: 1.000081]\n",
      "1924 [D loss: 0.999972] [G loss: 1.000061]\n",
      "1925 [D loss: 0.999887] [G loss: 1.000003]\n",
      "1926 [D loss: 0.999896] [G loss: 1.000015]\n",
      "1927 [D loss: 0.999864] [G loss: 0.999947]\n",
      "1928 [D loss: 0.999897] [G loss: 1.000062]\n",
      "1929 [D loss: 0.999941] [G loss: 0.999966]\n",
      "1930 [D loss: 0.999893] [G loss: 0.999957]\n",
      "1931 [D loss: 0.999918] [G loss: 0.999886]\n",
      "1932 [D loss: 0.999921] [G loss: 1.000027]\n",
      "1933 [D loss: 0.999853] [G loss: 1.000089]\n",
      "1934 [D loss: 0.999929] [G loss: 0.999992]\n",
      "1935 [D loss: 0.999921] [G loss: 1.000050]\n",
      "1936 [D loss: 0.999870] [G loss: 1.000029]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1937 [D loss: 0.999811] [G loss: 0.999883]\n",
      "1938 [D loss: 0.999885] [G loss: 0.999953]\n",
      "1939 [D loss: 0.999938] [G loss: 1.000070]\n",
      "1940 [D loss: 0.999907] [G loss: 1.000046]\n",
      "1941 [D loss: 0.999846] [G loss: 1.000092]\n",
      "1942 [D loss: 1.000007] [G loss: 1.000015]\n",
      "1943 [D loss: 0.999915] [G loss: 1.000007]\n",
      "1944 [D loss: 0.999953] [G loss: 0.999995]\n",
      "1945 [D loss: 0.999893] [G loss: 1.000024]\n",
      "1946 [D loss: 0.999926] [G loss: 1.000082]\n",
      "1947 [D loss: 0.999936] [G loss: 1.000053]\n",
      "1948 [D loss: 0.999962] [G loss: 0.999965]\n",
      "1949 [D loss: 0.999958] [G loss: 1.000103]\n",
      "1950 [D loss: 0.999964] [G loss: 1.000149]\n",
      "1951 [D loss: 0.999841] [G loss: 1.000068]\n",
      "1952 [D loss: 0.999979] [G loss: 0.999995]\n",
      "1953 [D loss: 0.999970] [G loss: 1.000085]\n",
      "1954 [D loss: 0.999895] [G loss: 0.999980]\n",
      "1955 [D loss: 0.999943] [G loss: 1.000020]\n",
      "1956 [D loss: 0.999926] [G loss: 0.999994]\n",
      "1957 [D loss: 0.999914] [G loss: 0.999918]\n",
      "1958 [D loss: 0.999884] [G loss: 0.999882]\n",
      "1959 [D loss: 0.999941] [G loss: 0.999954]\n",
      "1960 [D loss: 0.999956] [G loss: 0.999966]\n",
      "1961 [D loss: 0.999932] [G loss: 0.999958]\n",
      "1962 [D loss: 0.999946] [G loss: 0.999933]\n",
      "1963 [D loss: 0.999855] [G loss: 1.000102]\n",
      "1964 [D loss: 0.999924] [G loss: 1.000032]\n",
      "1965 [D loss: 0.999978] [G loss: 0.999876]\n",
      "1966 [D loss: 0.999842] [G loss: 1.000101]\n",
      "1967 [D loss: 0.999848] [G loss: 1.000082]\n",
      "1968 [D loss: 0.999904] [G loss: 0.999885]\n",
      "1969 [D loss: 0.999946] [G loss: 0.999994]\n",
      "1970 [D loss: 0.999859] [G loss: 0.999907]\n",
      "1971 [D loss: 0.999901] [G loss: 1.000108]\n",
      "1972 [D loss: 0.999902] [G loss: 1.000105]\n",
      "1973 [D loss: 0.999912] [G loss: 1.000057]\n",
      "1974 [D loss: 0.999943] [G loss: 0.999857]\n",
      "1975 [D loss: 0.999878] [G loss: 1.000089]\n",
      "1976 [D loss: 0.999891] [G loss: 1.000025]\n",
      "1977 [D loss: 0.999852] [G loss: 1.000108]\n",
      "1978 [D loss: 0.999858] [G loss: 1.000083]\n",
      "1979 [D loss: 0.999760] [G loss: 0.999947]\n",
      "1980 [D loss: 0.999896] [G loss: 0.999909]\n",
      "1981 [D loss: 0.999972] [G loss: 0.999909]\n",
      "1982 [D loss: 0.999993] [G loss: 0.999873]\n",
      "1983 [D loss: 0.999959] [G loss: 0.999908]\n",
      "1984 [D loss: 0.999902] [G loss: 1.000069]\n",
      "1985 [D loss: 0.999912] [G loss: 1.000128]\n",
      "1986 [D loss: 0.999857] [G loss: 1.000120]\n",
      "1987 [D loss: 0.999909] [G loss: 0.999814]\n",
      "1988 [D loss: 0.999991] [G loss: 0.999929]\n",
      "1989 [D loss: 0.999971] [G loss: 0.999937]\n",
      "1990 [D loss: 0.999905] [G loss: 0.999913]\n",
      "1991 [D loss: 0.999930] [G loss: 1.000114]\n",
      "1992 [D loss: 0.999790] [G loss: 1.000057]\n",
      "1993 [D loss: 0.999881] [G loss: 0.999839]\n",
      "1994 [D loss: 0.999823] [G loss: 1.000095]\n",
      "1995 [D loss: 0.999927] [G loss: 1.000082]\n",
      "1996 [D loss: 0.999821] [G loss: 0.999991]\n",
      "1997 [D loss: 0.999912] [G loss: 1.000106]\n",
      "1998 [D loss: 0.999850] [G loss: 0.999878]\n",
      "1999 [D loss: 0.999911] [G loss: 1.000011]\n",
      "2000 [D loss: 0.999940] [G loss: 0.999854]\n",
      "2001 [D loss: 0.999803] [G loss: 0.999834]\n",
      "2002 [D loss: 0.999861] [G loss: 0.999954]\n",
      "2003 [D loss: 0.999760] [G loss: 0.999942]\n",
      "2004 [D loss: 0.999846] [G loss: 0.999983]\n",
      "2005 [D loss: 0.999998] [G loss: 0.999870]\n",
      "2006 [D loss: 0.999909] [G loss: 0.999893]\n",
      "2007 [D loss: 0.999852] [G loss: 0.999946]\n",
      "2008 [D loss: 0.999921] [G loss: 1.000050]\n",
      "2009 [D loss: 0.999925] [G loss: 1.000088]\n",
      "2010 [D loss: 0.999988] [G loss: 0.999915]\n",
      "2011 [D loss: 0.999867] [G loss: 0.999915]\n",
      "2012 [D loss: 0.999998] [G loss: 0.999845]\n",
      "2013 [D loss: 0.999869] [G loss: 1.000098]\n",
      "2014 [D loss: 0.999941] [G loss: 0.999878]\n",
      "2015 [D loss: 0.999876] [G loss: 0.999845]\n",
      "2016 [D loss: 0.999698] [G loss: 1.000066]\n",
      "2017 [D loss: 0.999964] [G loss: 0.999948]\n",
      "2018 [D loss: 0.999903] [G loss: 0.999907]\n",
      "2019 [D loss: 1.000006] [G loss: 1.000015]\n",
      "2020 [D loss: 0.999860] [G loss: 0.999920]\n",
      "2021 [D loss: 0.999831] [G loss: 0.999856]\n",
      "2022 [D loss: 0.999936] [G loss: 1.000039]\n",
      "2023 [D loss: 0.999811] [G loss: 1.000020]\n",
      "2024 [D loss: 0.999843] [G loss: 1.000081]\n",
      "2025 [D loss: 0.999989] [G loss: 0.999807]\n",
      "2026 [D loss: 0.999779] [G loss: 0.999845]\n",
      "2027 [D loss: 0.999913] [G loss: 0.999868]\n",
      "2028 [D loss: 0.999695] [G loss: 1.000022]\n",
      "2029 [D loss: 0.999971] [G loss: 1.000049]\n",
      "2030 [D loss: 0.999633] [G loss: 1.000022]\n",
      "2031 [D loss: 0.999916] [G loss: 1.000061]\n",
      "2032 [D loss: 0.999881] [G loss: 0.999834]\n",
      "2033 [D loss: 0.999905] [G loss: 1.000005]\n",
      "2034 [D loss: 0.999874] [G loss: 0.999900]\n",
      "2035 [D loss: 0.999954] [G loss: 1.000004]\n",
      "2036 [D loss: 0.999818] [G loss: 1.000004]\n",
      "2037 [D loss: 0.999914] [G loss: 1.000052]\n",
      "2038 [D loss: 0.999936] [G loss: 1.000074]\n",
      "2039 [D loss: 0.999835] [G loss: 0.999937]\n",
      "2040 [D loss: 0.999899] [G loss: 0.999914]\n",
      "2041 [D loss: 0.999856] [G loss: 0.999859]\n",
      "2042 [D loss: 0.999736] [G loss: 0.999872]\n",
      "2043 [D loss: 0.999925] [G loss: 0.999994]\n",
      "2044 [D loss: 0.999948] [G loss: 0.999969]\n",
      "2045 [D loss: 0.999909] [G loss: 1.000012]\n",
      "2046 [D loss: 0.999834] [G loss: 1.000051]\n",
      "2047 [D loss: 0.999980] [G loss: 0.999927]\n",
      "2048 [D loss: 0.999854] [G loss: 1.000044]\n",
      "2049 [D loss: 0.999892] [G loss: 1.000077]\n",
      "2050 [D loss: 0.999948] [G loss: 0.999929]\n",
      "2051 [D loss: 0.999872] [G loss: 0.999828]\n",
      "2052 [D loss: 0.999852] [G loss: 0.999809]\n",
      "2053 [D loss: 1.000056] [G loss: 1.000113]\n",
      "2054 [D loss: 0.999776] [G loss: 0.999984]\n",
      "2055 [D loss: 0.999835] [G loss: 1.000002]\n",
      "2056 [D loss: 0.999964] [G loss: 0.999908]\n",
      "2057 [D loss: 0.999992] [G loss: 0.999919]\n",
      "2058 [D loss: 1.000043] [G loss: 0.999954]\n",
      "2059 [D loss: 0.999895] [G loss: 0.999904]\n",
      "2060 [D loss: 0.999878] [G loss: 0.999838]\n",
      "2061 [D loss: 0.999832] [G loss: 0.999974]\n",
      "2062 [D loss: 0.999781] [G loss: 0.999684]\n",
      "2063 [D loss: 0.999817] [G loss: 0.999870]\n",
      "2064 [D loss: 0.999925] [G loss: 1.000116]\n",
      "2065 [D loss: 0.999962] [G loss: 0.999860]\n",
      "2066 [D loss: 0.999787] [G loss: 0.999991]\n",
      "2067 [D loss: 0.999873] [G loss: 1.000174]\n",
      "2068 [D loss: 0.999792] [G loss: 0.999821]\n",
      "2069 [D loss: 0.999862] [G loss: 0.999939]\n",
      "2070 [D loss: 0.999876] [G loss: 0.999853]\n",
      "2071 [D loss: 0.999849] [G loss: 1.000060]\n",
      "2072 [D loss: 0.999880] [G loss: 1.000121]\n",
      "2073 [D loss: 0.999951] [G loss: 1.000057]\n",
      "2074 [D loss: 1.000041] [G loss: 0.999959]\n",
      "2075 [D loss: 1.000021] [G loss: 0.999966]\n",
      "2076 [D loss: 0.999832] [G loss: 1.000059]\n",
      "2077 [D loss: 0.999843] [G loss: 0.999982]\n",
      "2078 [D loss: 0.999850] [G loss: 0.999982]\n",
      "2079 [D loss: 0.999959] [G loss: 1.000065]\n",
      "2080 [D loss: 0.999853] [G loss: 0.999946]\n",
      "2081 [D loss: 0.999858] [G loss: 0.999716]\n",
      "2082 [D loss: 0.999863] [G loss: 1.000115]\n",
      "2083 [D loss: 0.999855] [G loss: 1.000180]\n",
      "2084 [D loss: 0.999913] [G loss: 1.000128]\n",
      "2085 [D loss: 0.999810] [G loss: 1.000060]\n",
      "2086 [D loss: 0.999921] [G loss: 1.000024]\n",
      "2087 [D loss: 0.999863] [G loss: 0.999985]\n",
      "2088 [D loss: 0.999792] [G loss: 0.999932]\n",
      "2089 [D loss: 0.999965] [G loss: 0.999891]\n",
      "2090 [D loss: 0.999900] [G loss: 1.000079]\n",
      "2091 [D loss: 0.999870] [G loss: 1.000166]\n",
      "2092 [D loss: 0.999903] [G loss: 1.000005]\n",
      "2093 [D loss: 1.000001] [G loss: 0.999942]\n",
      "2094 [D loss: 0.999833] [G loss: 0.999964]\n",
      "2095 [D loss: 0.999972] [G loss: 0.999984]\n",
      "2096 [D loss: 0.999974] [G loss: 0.999981]\n",
      "2097 [D loss: 0.999756] [G loss: 0.999834]\n",
      "2098 [D loss: 0.999783] [G loss: 0.999919]\n",
      "2099 [D loss: 0.999989] [G loss: 1.000144]\n",
      "2100 [D loss: 0.999882] [G loss: 0.999956]\n",
      "2101 [D loss: 0.999999] [G loss: 0.999966]\n",
      "2102 [D loss: 0.999892] [G loss: 1.000013]\n",
      "2103 [D loss: 0.999894] [G loss: 1.000068]\n",
      "2104 [D loss: 0.999856] [G loss: 0.999901]\n",
      "2105 [D loss: 0.999901] [G loss: 0.999961]\n",
      "2106 [D loss: 0.999918] [G loss: 0.999900]\n",
      "2107 [D loss: 0.999896] [G loss: 1.000004]\n",
      "2108 [D loss: 0.999862] [G loss: 1.000035]\n",
      "2109 [D loss: 0.999931] [G loss: 0.999862]\n",
      "2110 [D loss: 0.999866] [G loss: 0.999971]\n",
      "2111 [D loss: 0.999809] [G loss: 0.999900]\n",
      "2112 [D loss: 0.999953] [G loss: 1.000075]\n",
      "2113 [D loss: 0.999893] [G loss: 0.999938]\n",
      "2114 [D loss: 0.999926] [G loss: 1.000006]\n",
      "2115 [D loss: 0.999886] [G loss: 0.999975]\n",
      "2116 [D loss: 0.999926] [G loss: 0.999971]\n",
      "2117 [D loss: 0.999793] [G loss: 0.999998]\n",
      "2118 [D loss: 0.999942] [G loss: 1.000232]\n",
      "2119 [D loss: 0.999891] [G loss: 0.999980]\n",
      "2120 [D loss: 0.999980] [G loss: 1.000256]\n",
      "2121 [D loss: 0.999880] [G loss: 1.000001]\n",
      "2122 [D loss: 1.000015] [G loss: 1.000023]\n",
      "2123 [D loss: 0.999851] [G loss: 1.000000]\n",
      "2124 [D loss: 0.999933] [G loss: 1.000027]\n",
      "2125 [D loss: 0.999991] [G loss: 0.999892]\n",
      "2126 [D loss: 0.999787] [G loss: 0.999915]\n",
      "2127 [D loss: 0.999756] [G loss: 0.999967]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2128 [D loss: 0.999899] [G loss: 0.999868]\n",
      "2129 [D loss: 0.999926] [G loss: 0.999811]\n",
      "2130 [D loss: 0.999899] [G loss: 1.000115]\n",
      "2131 [D loss: 0.999806] [G loss: 1.000014]\n",
      "2132 [D loss: 0.999934] [G loss: 0.999772]\n",
      "2133 [D loss: 0.999843] [G loss: 0.999991]\n",
      "2134 [D loss: 0.999910] [G loss: 0.999921]\n",
      "2135 [D loss: 0.999888] [G loss: 1.000078]\n",
      "2136 [D loss: 0.999868] [G loss: 0.999992]\n",
      "2137 [D loss: 0.999813] [G loss: 1.000171]\n",
      "2138 [D loss: 0.999857] [G loss: 0.999990]\n",
      "2139 [D loss: 0.999840] [G loss: 0.999957]\n",
      "2140 [D loss: 0.999913] [G loss: 1.000003]\n",
      "2141 [D loss: 0.999928] [G loss: 0.999963]\n",
      "2142 [D loss: 0.999955] [G loss: 1.000002]\n",
      "2143 [D loss: 0.999833] [G loss: 1.000130]\n",
      "2144 [D loss: 0.999939] [G loss: 1.000056]\n",
      "2145 [D loss: 0.999885] [G loss: 0.999958]\n",
      "2146 [D loss: 0.999829] [G loss: 0.999985]\n",
      "2147 [D loss: 0.999906] [G loss: 0.999963]\n",
      "2148 [D loss: 0.999850] [G loss: 1.000030]\n",
      "2149 [D loss: 0.999892] [G loss: 1.000049]\n",
      "2150 [D loss: 0.999767] [G loss: 0.999950]\n",
      "2151 [D loss: 0.999981] [G loss: 0.999933]\n",
      "2152 [D loss: 0.999993] [G loss: 0.999981]\n",
      "2153 [D loss: 0.999890] [G loss: 0.999941]\n",
      "2154 [D loss: 0.999953] [G loss: 0.999941]\n",
      "2155 [D loss: 1.000015] [G loss: 1.000040]\n",
      "2156 [D loss: 0.999928] [G loss: 1.000057]\n",
      "2157 [D loss: 0.999896] [G loss: 1.000049]\n",
      "2158 [D loss: 0.999928] [G loss: 1.000000]\n",
      "2159 [D loss: 0.999951] [G loss: 0.999850]\n",
      "2160 [D loss: 0.999835] [G loss: 0.999959]\n",
      "2161 [D loss: 0.999883] [G loss: 0.999994]\n",
      "2162 [D loss: 1.000022] [G loss: 1.000062]\n",
      "2163 [D loss: 0.999889] [G loss: 0.999928]\n",
      "2164 [D loss: 0.999989] [G loss: 1.000037]\n",
      "2165 [D loss: 0.999980] [G loss: 0.999897]\n",
      "2166 [D loss: 1.000111] [G loss: 0.999995]\n",
      "2167 [D loss: 0.999896] [G loss: 0.999986]\n",
      "2168 [D loss: 0.999901] [G loss: 0.999919]\n",
      "2169 [D loss: 0.999850] [G loss: 1.000018]\n",
      "2170 [D loss: 1.000011] [G loss: 0.999837]\n",
      "2171 [D loss: 0.999799] [G loss: 1.000039]\n",
      "2172 [D loss: 0.999850] [G loss: 0.999882]\n",
      "2173 [D loss: 0.999986] [G loss: 0.999917]\n",
      "2174 [D loss: 0.999824] [G loss: 1.000019]\n",
      "2175 [D loss: 0.999891] [G loss: 0.999946]\n",
      "2176 [D loss: 0.999824] [G loss: 0.999922]\n",
      "2177 [D loss: 0.999875] [G loss: 1.000060]\n",
      "2178 [D loss: 0.999926] [G loss: 0.999800]\n",
      "2179 [D loss: 1.000004] [G loss: 0.999967]\n",
      "2180 [D loss: 0.999983] [G loss: 0.999980]\n",
      "2181 [D loss: 0.999925] [G loss: 0.999954]\n",
      "2182 [D loss: 0.999784] [G loss: 0.999934]\n",
      "2183 [D loss: 0.999903] [G loss: 1.000035]\n",
      "2184 [D loss: 0.999805] [G loss: 1.000030]\n",
      "2185 [D loss: 0.999978] [G loss: 1.000073]\n",
      "2186 [D loss: 0.999832] [G loss: 0.999933]\n",
      "2187 [D loss: 0.999881] [G loss: 0.999983]\n",
      "2188 [D loss: 0.999969] [G loss: 0.999993]\n",
      "2189 [D loss: 0.999882] [G loss: 1.000022]\n",
      "2190 [D loss: 0.999837] [G loss: 0.999987]\n",
      "2191 [D loss: 0.999948] [G loss: 0.999889]\n",
      "2192 [D loss: 0.999968] [G loss: 0.999990]\n",
      "2193 [D loss: 0.999947] [G loss: 0.999923]\n",
      "2194 [D loss: 0.999803] [G loss: 1.000124]\n",
      "2195 [D loss: 0.999899] [G loss: 1.000005]\n",
      "2196 [D loss: 0.999755] [G loss: 1.000033]\n",
      "2197 [D loss: 0.999976] [G loss: 1.000073]\n",
      "2198 [D loss: 0.999782] [G loss: 0.999953]\n",
      "2199 [D loss: 0.999976] [G loss: 1.000149]\n",
      "2200 [D loss: 0.999866] [G loss: 0.999946]\n",
      "2201 [D loss: 0.999857] [G loss: 0.999980]\n",
      "2202 [D loss: 0.999906] [G loss: 0.999756]\n",
      "2203 [D loss: 1.000003] [G loss: 1.000150]\n",
      "2204 [D loss: 0.999903] [G loss: 0.999927]\n",
      "2205 [D loss: 0.999891] [G loss: 0.999812]\n",
      "2206 [D loss: 0.999989] [G loss: 0.999912]\n",
      "2207 [D loss: 0.999900] [G loss: 1.000082]\n",
      "2208 [D loss: 0.999937] [G loss: 1.000026]\n",
      "2209 [D loss: 1.000086] [G loss: 0.999810]\n",
      "2210 [D loss: 0.999837] [G loss: 0.999985]\n",
      "2211 [D loss: 0.999893] [G loss: 0.999852]\n",
      "2212 [D loss: 0.999931] [G loss: 1.000009]\n",
      "2213 [D loss: 0.999860] [G loss: 0.999857]\n",
      "2214 [D loss: 0.999961] [G loss: 0.999996]\n",
      "2215 [D loss: 0.999886] [G loss: 0.999852]\n",
      "2216 [D loss: 0.999961] [G loss: 1.000038]\n",
      "2217 [D loss: 0.999981] [G loss: 0.999992]\n",
      "2218 [D loss: 0.999910] [G loss: 0.999975]\n",
      "2219 [D loss: 1.000023] [G loss: 0.999925]\n",
      "2220 [D loss: 0.999948] [G loss: 0.999891]\n",
      "2221 [D loss: 0.999868] [G loss: 0.999997]\n",
      "2222 [D loss: 0.999850] [G loss: 0.999977]\n",
      "2223 [D loss: 0.999913] [G loss: 0.999983]\n",
      "2224 [D loss: 0.999901] [G loss: 0.999881]\n",
      "2225 [D loss: 0.999889] [G loss: 1.000014]\n",
      "2226 [D loss: 1.000017] [G loss: 1.000068]\n",
      "2227 [D loss: 0.999789] [G loss: 0.999805]\n",
      "2228 [D loss: 0.999893] [G loss: 0.999919]\n",
      "2229 [D loss: 0.999861] [G loss: 0.999998]\n",
      "2230 [D loss: 0.999930] [G loss: 0.999989]\n",
      "2231 [D loss: 0.999910] [G loss: 1.000013]\n",
      "2232 [D loss: 0.999922] [G loss: 0.999867]\n",
      "2233 [D loss: 0.999876] [G loss: 1.000015]\n",
      "2234 [D loss: 0.999738] [G loss: 0.999915]\n",
      "2235 [D loss: 0.999898] [G loss: 0.999707]\n",
      "2236 [D loss: 0.999851] [G loss: 1.000044]\n",
      "2237 [D loss: 0.999863] [G loss: 0.999886]\n",
      "2238 [D loss: 0.999927] [G loss: 0.999940]\n",
      "2239 [D loss: 1.000004] [G loss: 1.000039]\n",
      "2240 [D loss: 0.999822] [G loss: 0.999669]\n",
      "2241 [D loss: 0.999914] [G loss: 0.999961]\n",
      "2242 [D loss: 0.999827] [G loss: 0.999999]\n",
      "2243 [D loss: 0.999843] [G loss: 0.999980]\n",
      "2244 [D loss: 0.999879] [G loss: 1.000052]\n",
      "2245 [D loss: 0.999894] [G loss: 0.999862]\n",
      "2246 [D loss: 0.999932] [G loss: 0.999788]\n",
      "2247 [D loss: 1.000004] [G loss: 1.000013]\n",
      "2248 [D loss: 0.999931] [G loss: 0.999819]\n",
      "2249 [D loss: 0.999941] [G loss: 0.999912]\n",
      "2250 [D loss: 0.999989] [G loss: 0.999978]\n",
      "2251 [D loss: 0.999840] [G loss: 1.000090]\n",
      "2252 [D loss: 0.999787] [G loss: 0.999940]\n",
      "2253 [D loss: 0.999839] [G loss: 0.999786]\n",
      "2254 [D loss: 0.999867] [G loss: 0.999943]\n",
      "2255 [D loss: 0.999846] [G loss: 0.999989]\n",
      "2256 [D loss: 0.999819] [G loss: 1.000075]\n",
      "2257 [D loss: 0.999940] [G loss: 1.000036]\n",
      "2258 [D loss: 0.999834] [G loss: 0.999840]\n",
      "2259 [D loss: 0.999861] [G loss: 0.999948]\n",
      "2260 [D loss: 1.000012] [G loss: 0.999849]\n",
      "2261 [D loss: 0.999893] [G loss: 1.000068]\n",
      "2262 [D loss: 0.999865] [G loss: 0.999914]\n",
      "2263 [D loss: 0.999825] [G loss: 1.000073]\n",
      "2264 [D loss: 0.999996] [G loss: 1.000033]\n",
      "2265 [D loss: 0.999744] [G loss: 0.999859]\n",
      "2266 [D loss: 0.999990] [G loss: 0.999941]\n",
      "2267 [D loss: 0.999735] [G loss: 1.000005]\n",
      "2268 [D loss: 0.999860] [G loss: 0.999947]\n",
      "2269 [D loss: 0.999837] [G loss: 1.000073]\n",
      "2270 [D loss: 0.999755] [G loss: 0.999910]\n",
      "2271 [D loss: 0.999898] [G loss: 0.999943]\n",
      "2272 [D loss: 0.999835] [G loss: 0.999776]\n",
      "2273 [D loss: 0.999889] [G loss: 1.000034]\n",
      "2274 [D loss: 0.999899] [G loss: 1.000228]\n",
      "2275 [D loss: 0.999818] [G loss: 0.999848]\n",
      "2276 [D loss: 0.999795] [G loss: 0.999868]\n",
      "2277 [D loss: 0.999877] [G loss: 0.999945]\n",
      "2278 [D loss: 0.999886] [G loss: 0.999943]\n",
      "2279 [D loss: 0.999959] [G loss: 1.000012]\n",
      "2280 [D loss: 0.999917] [G loss: 0.999982]\n",
      "2281 [D loss: 0.999822] [G loss: 1.000028]\n",
      "2282 [D loss: 0.999906] [G loss: 0.999995]\n",
      "2283 [D loss: 0.999853] [G loss: 0.999837]\n",
      "2284 [D loss: 0.999882] [G loss: 0.999855]\n",
      "2285 [D loss: 0.999893] [G loss: 0.999792]\n",
      "2286 [D loss: 0.999924] [G loss: 1.000169]\n",
      "2287 [D loss: 0.999843] [G loss: 0.999911]\n",
      "2288 [D loss: 0.999812] [G loss: 0.999961]\n",
      "2289 [D loss: 0.999826] [G loss: 0.999937]\n",
      "2290 [D loss: 0.999981] [G loss: 0.999880]\n",
      "2291 [D loss: 0.999849] [G loss: 1.000159]\n",
      "2292 [D loss: 0.999978] [G loss: 0.999969]\n",
      "2293 [D loss: 0.999759] [G loss: 0.999993]\n",
      "2294 [D loss: 0.999915] [G loss: 0.999887]\n",
      "2295 [D loss: 0.999922] [G loss: 1.000041]\n",
      "2296 [D loss: 0.999792] [G loss: 0.999813]\n",
      "2297 [D loss: 0.999864] [G loss: 1.000021]\n",
      "2298 [D loss: 1.000016] [G loss: 0.999884]\n",
      "2299 [D loss: 0.999964] [G loss: 0.999858]\n",
      "2300 [D loss: 0.999907] [G loss: 0.999964]\n",
      "2301 [D loss: 0.999976] [G loss: 0.999971]\n",
      "2302 [D loss: 0.999721] [G loss: 1.000007]\n",
      "2303 [D loss: 0.999915] [G loss: 0.999968]\n",
      "2304 [D loss: 0.999968] [G loss: 0.999908]\n",
      "2305 [D loss: 0.999826] [G loss: 1.000020]\n",
      "2306 [D loss: 0.999799] [G loss: 0.999980]\n",
      "2307 [D loss: 0.999967] [G loss: 0.999912]\n",
      "2308 [D loss: 0.999934] [G loss: 1.000028]\n",
      "2309 [D loss: 0.999839] [G loss: 1.000196]\n",
      "2310 [D loss: 0.999934] [G loss: 0.999929]\n",
      "2311 [D loss: 0.999984] [G loss: 1.000033]\n",
      "2312 [D loss: 0.999900] [G loss: 1.000006]\n",
      "2313 [D loss: 0.999931] [G loss: 0.999768]\n",
      "2314 [D loss: 0.999937] [G loss: 0.999712]\n",
      "2315 [D loss: 0.999796] [G loss: 0.999943]\n",
      "2316 [D loss: 0.999943] [G loss: 0.999877]\n",
      "2317 [D loss: 0.999958] [G loss: 0.999841]\n",
      "2318 [D loss: 1.000053] [G loss: 0.999860]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2319 [D loss: 1.000015] [G loss: 0.999977]\n",
      "2320 [D loss: 0.999872] [G loss: 0.999949]\n",
      "2321 [D loss: 0.999717] [G loss: 1.000081]\n",
      "2322 [D loss: 0.999835] [G loss: 1.000037]\n",
      "2323 [D loss: 0.999860] [G loss: 0.999788]\n",
      "2324 [D loss: 0.999836] [G loss: 0.999928]\n",
      "2325 [D loss: 0.999758] [G loss: 0.999961]\n",
      "2326 [D loss: 0.999748] [G loss: 0.999923]\n",
      "2327 [D loss: 0.999833] [G loss: 1.000090]\n",
      "2328 [D loss: 0.999923] [G loss: 0.999863]\n",
      "2329 [D loss: 0.999997] [G loss: 1.000023]\n",
      "2330 [D loss: 0.999872] [G loss: 1.000083]\n",
      "2331 [D loss: 0.999933] [G loss: 1.000157]\n",
      "2332 [D loss: 0.999921] [G loss: 0.999905]\n",
      "2333 [D loss: 0.999885] [G loss: 0.999911]\n",
      "2334 [D loss: 0.999888] [G loss: 0.999982]\n",
      "2335 [D loss: 0.999835] [G loss: 0.999987]\n",
      "2336 [D loss: 0.999962] [G loss: 1.000045]\n",
      "2337 [D loss: 0.999847] [G loss: 0.999919]\n",
      "2338 [D loss: 0.999891] [G loss: 1.000030]\n",
      "2339 [D loss: 0.999921] [G loss: 1.000057]\n",
      "2340 [D loss: 0.999886] [G loss: 0.999998]\n",
      "2341 [D loss: 0.999886] [G loss: 0.999973]\n",
      "2342 [D loss: 0.999916] [G loss: 0.999869]\n",
      "2343 [D loss: 0.999976] [G loss: 0.999945]\n",
      "2344 [D loss: 0.999881] [G loss: 0.999925]\n",
      "2345 [D loss: 0.999811] [G loss: 0.999922]\n",
      "2346 [D loss: 0.999865] [G loss: 0.999981]\n",
      "2347 [D loss: 0.999910] [G loss: 0.999923]\n",
      "2348 [D loss: 0.999904] [G loss: 1.000036]\n",
      "2349 [D loss: 0.999840] [G loss: 1.000058]\n",
      "2350 [D loss: 1.000018] [G loss: 0.999941]\n",
      "2351 [D loss: 0.999865] [G loss: 1.000014]\n",
      "2352 [D loss: 0.999912] [G loss: 0.999889]\n",
      "2353 [D loss: 0.999899] [G loss: 1.000012]\n",
      "2354 [D loss: 0.999840] [G loss: 0.999958]\n",
      "2355 [D loss: 0.999848] [G loss: 1.000029]\n",
      "2356 [D loss: 1.000011] [G loss: 0.999949]\n",
      "2357 [D loss: 0.999833] [G loss: 1.000006]\n",
      "2358 [D loss: 0.999986] [G loss: 0.999801]\n",
      "2359 [D loss: 0.999805] [G loss: 1.000023]\n",
      "2360 [D loss: 0.999952] [G loss: 0.999895]\n",
      "2361 [D loss: 1.000017] [G loss: 0.999978]\n",
      "2362 [D loss: 0.999861] [G loss: 1.000124]\n",
      "2363 [D loss: 0.999941] [G loss: 0.999957]\n",
      "2364 [D loss: 0.999987] [G loss: 1.000000]\n",
      "2365 [D loss: 0.999882] [G loss: 0.999887]\n",
      "2366 [D loss: 0.999924] [G loss: 0.999896]\n",
      "2367 [D loss: 0.999881] [G loss: 0.999994]\n",
      "2368 [D loss: 0.999791] [G loss: 1.000085]\n",
      "2369 [D loss: 0.999938] [G loss: 0.999928]\n",
      "2370 [D loss: 0.999900] [G loss: 0.999846]\n",
      "2371 [D loss: 0.999875] [G loss: 0.999823]\n",
      "2372 [D loss: 0.999774] [G loss: 1.000040]\n",
      "2373 [D loss: 0.999974] [G loss: 0.999839]\n",
      "2374 [D loss: 0.999884] [G loss: 1.000019]\n",
      "2375 [D loss: 0.999904] [G loss: 0.999969]\n",
      "2376 [D loss: 0.999952] [G loss: 1.000029]\n",
      "2377 [D loss: 0.999894] [G loss: 0.999943]\n",
      "2378 [D loss: 0.999921] [G loss: 1.000004]\n",
      "2379 [D loss: 0.999781] [G loss: 1.000009]\n",
      "2380 [D loss: 0.999872] [G loss: 0.999956]\n",
      "2381 [D loss: 0.999836] [G loss: 0.999968]\n",
      "2382 [D loss: 0.999967] [G loss: 0.999960]\n",
      "2383 [D loss: 0.999891] [G loss: 0.999923]\n",
      "2384 [D loss: 0.999826] [G loss: 1.000042]\n",
      "2385 [D loss: 1.000022] [G loss: 1.000075]\n",
      "2386 [D loss: 0.999884] [G loss: 0.999999]\n",
      "2387 [D loss: 0.999796] [G loss: 0.999870]\n",
      "2388 [D loss: 1.000014] [G loss: 0.999947]\n",
      "2389 [D loss: 0.999988] [G loss: 0.999926]\n",
      "2390 [D loss: 0.999944] [G loss: 0.999836]\n",
      "2391 [D loss: 0.999920] [G loss: 0.999893]\n",
      "2392 [D loss: 0.999937] [G loss: 1.000011]\n",
      "2393 [D loss: 0.999800] [G loss: 0.999934]\n",
      "2394 [D loss: 0.999845] [G loss: 1.000144]\n",
      "2395 [D loss: 0.999834] [G loss: 0.999880]\n",
      "2396 [D loss: 1.000011] [G loss: 0.999935]\n",
      "2397 [D loss: 0.999813] [G loss: 0.999853]\n",
      "2398 [D loss: 0.999920] [G loss: 0.999971]\n",
      "2399 [D loss: 0.999855] [G loss: 1.000112]\n",
      "2400 [D loss: 0.999998] [G loss: 1.000033]\n",
      "2401 [D loss: 0.999893] [G loss: 0.999940]\n",
      "2402 [D loss: 0.999829] [G loss: 0.999972]\n",
      "2403 [D loss: 0.999974] [G loss: 0.999998]\n",
      "2404 [D loss: 0.999929] [G loss: 1.000016]\n",
      "2405 [D loss: 0.999959] [G loss: 1.000057]\n",
      "2406 [D loss: 0.999859] [G loss: 0.999918]\n",
      "2407 [D loss: 0.999903] [G loss: 0.999888]\n",
      "2408 [D loss: 0.999845] [G loss: 1.000088]\n",
      "2409 [D loss: 0.999848] [G loss: 0.999909]\n",
      "2410 [D loss: 0.999884] [G loss: 1.000002]\n",
      "2411 [D loss: 1.000006] [G loss: 0.999961]\n",
      "2412 [D loss: 0.999879] [G loss: 0.999835]\n",
      "2413 [D loss: 0.999888] [G loss: 0.999987]\n",
      "2414 [D loss: 0.999889] [G loss: 0.999942]\n",
      "2415 [D loss: 0.999894] [G loss: 1.000022]\n",
      "2416 [D loss: 0.999790] [G loss: 1.000037]\n",
      "2417 [D loss: 0.999910] [G loss: 1.000078]\n",
      "2418 [D loss: 0.999896] [G loss: 0.999863]\n",
      "2419 [D loss: 0.999912] [G loss: 0.999968]\n",
      "2420 [D loss: 1.000025] [G loss: 1.000024]\n",
      "2421 [D loss: 0.999888] [G loss: 1.000147]\n",
      "2422 [D loss: 0.999927] [G loss: 0.999830]\n",
      "2423 [D loss: 0.999930] [G loss: 1.000263]\n",
      "2424 [D loss: 0.999952] [G loss: 0.999960]\n",
      "2425 [D loss: 0.999950] [G loss: 1.000087]\n",
      "2426 [D loss: 0.999993] [G loss: 0.999844]\n",
      "2427 [D loss: 0.999900] [G loss: 0.999914]\n",
      "2428 [D loss: 0.999931] [G loss: 1.000033]\n",
      "2429 [D loss: 0.999847] [G loss: 0.999900]\n",
      "2430 [D loss: 0.999941] [G loss: 1.000010]\n",
      "2431 [D loss: 0.999990] [G loss: 0.999963]\n",
      "2432 [D loss: 0.999899] [G loss: 0.999994]\n",
      "2433 [D loss: 0.999861] [G loss: 0.999823]\n",
      "2434 [D loss: 0.999905] [G loss: 0.999961]\n",
      "2435 [D loss: 0.999836] [G loss: 1.000102]\n",
      "2436 [D loss: 0.999864] [G loss: 0.999941]\n",
      "2437 [D loss: 0.999808] [G loss: 0.999948]\n",
      "2438 [D loss: 0.999887] [G loss: 1.000027]\n",
      "2439 [D loss: 1.000061] [G loss: 0.999869]\n",
      "2440 [D loss: 0.999779] [G loss: 1.000065]\n",
      "2441 [D loss: 0.999765] [G loss: 0.999948]\n",
      "2442 [D loss: 0.999834] [G loss: 0.999884]\n",
      "2443 [D loss: 0.999874] [G loss: 0.999996]\n",
      "2444 [D loss: 0.999859] [G loss: 1.000039]\n",
      "2445 [D loss: 0.999859] [G loss: 1.000031]\n",
      "2446 [D loss: 0.999821] [G loss: 0.999986]\n",
      "2447 [D loss: 0.999880] [G loss: 1.000017]\n",
      "2448 [D loss: 0.999838] [G loss: 0.999987]\n",
      "2449 [D loss: 0.999893] [G loss: 0.999997]\n",
      "2450 [D loss: 0.999958] [G loss: 1.000079]\n",
      "2451 [D loss: 0.999878] [G loss: 0.999994]\n",
      "2452 [D loss: 0.999849] [G loss: 0.999943]\n",
      "2453 [D loss: 0.999907] [G loss: 1.000065]\n",
      "2454 [D loss: 0.999905] [G loss: 0.999979]\n",
      "2455 [D loss: 0.999881] [G loss: 0.999988]\n",
      "2456 [D loss: 0.999858] [G loss: 0.999931]\n",
      "2457 [D loss: 0.999786] [G loss: 0.999917]\n",
      "2458 [D loss: 0.999842] [G loss: 0.999973]\n",
      "2459 [D loss: 0.999884] [G loss: 0.999958]\n",
      "2460 [D loss: 0.999914] [G loss: 1.000197]\n",
      "2461 [D loss: 0.999920] [G loss: 0.999935]\n",
      "2462 [D loss: 0.999850] [G loss: 1.000087]\n",
      "2463 [D loss: 0.999875] [G loss: 1.000024]\n",
      "2464 [D loss: 0.999948] [G loss: 0.999981]\n",
      "2465 [D loss: 0.999869] [G loss: 0.999852]\n",
      "2466 [D loss: 0.999778] [G loss: 0.999822]\n",
      "2467 [D loss: 0.999885] [G loss: 0.999946]\n",
      "2468 [D loss: 0.999923] [G loss: 1.000105]\n",
      "2469 [D loss: 0.999810] [G loss: 0.999754]\n",
      "2470 [D loss: 0.999821] [G loss: 0.999933]\n",
      "2471 [D loss: 0.999805] [G loss: 0.999988]\n",
      "2472 [D loss: 0.999966] [G loss: 1.000094]\n",
      "2473 [D loss: 0.999860] [G loss: 1.000028]\n",
      "2474 [D loss: 0.999897] [G loss: 1.000168]\n",
      "2475 [D loss: 0.999880] [G loss: 0.999963]\n",
      "2476 [D loss: 0.999938] [G loss: 0.999930]\n",
      "2477 [D loss: 0.999833] [G loss: 1.000000]\n",
      "2478 [D loss: 0.999997] [G loss: 0.999759]\n",
      "2479 [D loss: 0.999825] [G loss: 0.999937]\n",
      "2480 [D loss: 1.000017] [G loss: 0.999933]\n",
      "2481 [D loss: 0.999999] [G loss: 0.999944]\n",
      "2482 [D loss: 0.999835] [G loss: 0.999914]\n",
      "2483 [D loss: 0.999883] [G loss: 0.999873]\n",
      "2484 [D loss: 0.999766] [G loss: 1.000041]\n",
      "2485 [D loss: 0.999954] [G loss: 1.000105]\n",
      "2486 [D loss: 0.999847] [G loss: 0.999944]\n",
      "2487 [D loss: 0.999946] [G loss: 1.000023]\n",
      "2488 [D loss: 0.999897] [G loss: 1.000123]\n",
      "2489 [D loss: 0.999896] [G loss: 1.000058]\n",
      "2490 [D loss: 0.999897] [G loss: 0.999851]\n",
      "2491 [D loss: 0.999868] [G loss: 1.000079]\n",
      "2492 [D loss: 0.999840] [G loss: 1.000008]\n",
      "2493 [D loss: 0.999795] [G loss: 0.999939]\n",
      "2494 [D loss: 0.999852] [G loss: 1.000030]\n",
      "2495 [D loss: 0.999946] [G loss: 0.999916]\n",
      "2496 [D loss: 0.999909] [G loss: 0.999987]\n",
      "2497 [D loss: 0.999961] [G loss: 0.999997]\n",
      "2498 [D loss: 0.999890] [G loss: 0.999942]\n",
      "2499 [D loss: 0.999886] [G loss: 0.999860]\n",
      "2500 [D loss: 1.000004] [G loss: 1.000045]\n",
      "2501 [D loss: 0.999899] [G loss: 0.999868]\n",
      "2502 [D loss: 1.000045] [G loss: 1.000034]\n",
      "2503 [D loss: 0.999989] [G loss: 1.000165]\n",
      "2504 [D loss: 0.999818] [G loss: 1.000020]\n",
      "2505 [D loss: 0.999859] [G loss: 1.000243]\n",
      "2506 [D loss: 0.999954] [G loss: 1.000038]\n",
      "2507 [D loss: 0.999869] [G loss: 1.000166]\n",
      "2508 [D loss: 1.000004] [G loss: 1.000050]\n",
      "2509 [D loss: 0.999882] [G loss: 0.999888]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2510 [D loss: 0.999878] [G loss: 1.000001]\n",
      "2511 [D loss: 1.000047] [G loss: 1.000031]\n",
      "2512 [D loss: 0.999937] [G loss: 1.000027]\n",
      "2513 [D loss: 0.999911] [G loss: 1.000004]\n",
      "2514 [D loss: 0.999821] [G loss: 1.000186]\n",
      "2515 [D loss: 0.999815] [G loss: 1.000151]\n",
      "2516 [D loss: 0.999983] [G loss: 0.999933]\n",
      "2517 [D loss: 0.999824] [G loss: 0.999945]\n",
      "2518 [D loss: 0.999918] [G loss: 0.999993]\n",
      "2519 [D loss: 0.999842] [G loss: 1.000069]\n",
      "2520 [D loss: 0.999829] [G loss: 1.000062]\n",
      "2521 [D loss: 0.999867] [G loss: 0.999852]\n",
      "2522 [D loss: 0.999934] [G loss: 0.999971]\n",
      "2523 [D loss: 0.999952] [G loss: 0.999890]\n",
      "2524 [D loss: 0.999960] [G loss: 0.999959]\n",
      "2525 [D loss: 0.999921] [G loss: 1.000056]\n",
      "2526 [D loss: 0.999911] [G loss: 0.999872]\n",
      "2527 [D loss: 0.999846] [G loss: 0.999953]\n",
      "2528 [D loss: 1.000030] [G loss: 1.000057]\n",
      "2529 [D loss: 0.999848] [G loss: 0.999950]\n",
      "2530 [D loss: 0.999832] [G loss: 1.000039]\n",
      "2531 [D loss: 0.999893] [G loss: 1.000228]\n",
      "2532 [D loss: 0.999932] [G loss: 1.000061]\n",
      "2533 [D loss: 0.999900] [G loss: 1.000046]\n",
      "2534 [D loss: 0.999961] [G loss: 1.000185]\n",
      "2535 [D loss: 0.999900] [G loss: 1.000049]\n",
      "2536 [D loss: 0.999936] [G loss: 0.999933]\n",
      "2537 [D loss: 0.999895] [G loss: 0.999858]\n",
      "2538 [D loss: 0.999934] [G loss: 0.999922]\n",
      "2539 [D loss: 0.999840] [G loss: 0.999915]\n",
      "2540 [D loss: 0.999849] [G loss: 0.999902]\n",
      "2541 [D loss: 0.999901] [G loss: 1.000142]\n",
      "2542 [D loss: 0.999993] [G loss: 1.000085]\n",
      "2543 [D loss: 0.999772] [G loss: 1.000036]\n",
      "2544 [D loss: 0.999903] [G loss: 0.999878]\n",
      "2545 [D loss: 0.999874] [G loss: 1.000042]\n",
      "2546 [D loss: 0.999983] [G loss: 1.000125]\n",
      "2547 [D loss: 0.999866] [G loss: 1.000079]\n",
      "2548 [D loss: 0.999916] [G loss: 0.999981]\n",
      "2549 [D loss: 0.999908] [G loss: 1.000008]\n",
      "2550 [D loss: 0.999915] [G loss: 1.000072]\n",
      "2551 [D loss: 0.999893] [G loss: 1.000107]\n",
      "2552 [D loss: 0.999882] [G loss: 0.999993]\n",
      "2553 [D loss: 0.999904] [G loss: 1.000004]\n",
      "2554 [D loss: 0.999924] [G loss: 1.000095]\n",
      "2555 [D loss: 1.000030] [G loss: 0.999995]\n",
      "2556 [D loss: 0.999832] [G loss: 0.999946]\n",
      "2557 [D loss: 0.999959] [G loss: 0.999956]\n",
      "2558 [D loss: 0.999861] [G loss: 0.999950]\n",
      "2559 [D loss: 0.999938] [G loss: 1.000007]\n",
      "2560 [D loss: 0.999877] [G loss: 1.000071]\n",
      "2561 [D loss: 0.999878] [G loss: 1.000016]\n",
      "2562 [D loss: 0.999926] [G loss: 1.000024]\n",
      "2563 [D loss: 0.999867] [G loss: 0.999879]\n",
      "2564 [D loss: 0.999912] [G loss: 1.000039]\n",
      "2565 [D loss: 0.999921] [G loss: 0.999938]\n",
      "2566 [D loss: 0.999826] [G loss: 0.999877]\n",
      "2567 [D loss: 0.999935] [G loss: 1.000074]\n",
      "2568 [D loss: 0.999968] [G loss: 1.000064]\n",
      "2569 [D loss: 0.999909] [G loss: 0.999979]\n",
      "2570 [D loss: 0.999901] [G loss: 0.999976]\n",
      "2571 [D loss: 0.999855] [G loss: 0.999878]\n",
      "2572 [D loss: 0.999972] [G loss: 0.999926]\n",
      "2573 [D loss: 0.999969] [G loss: 1.000208]\n",
      "2574 [D loss: 0.999909] [G loss: 1.000008]\n",
      "2575 [D loss: 0.999929] [G loss: 0.999905]\n",
      "2576 [D loss: 0.999917] [G loss: 1.000037]\n",
      "2577 [D loss: 0.999878] [G loss: 0.999957]\n",
      "2578 [D loss: 0.999881] [G loss: 0.999871]\n",
      "2579 [D loss: 0.999869] [G loss: 1.000150]\n",
      "2580 [D loss: 0.999850] [G loss: 1.000048]\n",
      "2581 [D loss: 0.999849] [G loss: 0.999948]\n",
      "2582 [D loss: 0.999919] [G loss: 0.999905]\n",
      "2583 [D loss: 0.999863] [G loss: 0.999941]\n",
      "2584 [D loss: 0.999855] [G loss: 0.999929]\n",
      "2585 [D loss: 0.999845] [G loss: 0.999883]\n",
      "2586 [D loss: 0.999885] [G loss: 1.000063]\n",
      "2587 [D loss: 0.999946] [G loss: 1.000041]\n",
      "2588 [D loss: 0.999846] [G loss: 0.999895]\n",
      "2589 [D loss: 0.999883] [G loss: 0.999960]\n",
      "2590 [D loss: 0.999906] [G loss: 0.999878]\n",
      "2591 [D loss: 0.999778] [G loss: 1.000086]\n",
      "2592 [D loss: 1.000025] [G loss: 1.000184]\n",
      "2593 [D loss: 0.999935] [G loss: 0.999916]\n",
      "2594 [D loss: 0.999890] [G loss: 0.999937]\n",
      "2595 [D loss: 0.999937] [G loss: 0.999899]\n",
      "2596 [D loss: 0.999826] [G loss: 1.000012]\n",
      "2597 [D loss: 0.999925] [G loss: 0.999940]\n",
      "2598 [D loss: 0.999807] [G loss: 1.000056]\n",
      "2599 [D loss: 0.999863] [G loss: 1.000016]\n",
      "2600 [D loss: 0.999927] [G loss: 0.999978]\n",
      "2601 [D loss: 0.999813] [G loss: 0.999899]\n",
      "2602 [D loss: 0.999912] [G loss: 0.999921]\n",
      "2603 [D loss: 0.999945] [G loss: 0.999794]\n",
      "2604 [D loss: 0.999908] [G loss: 0.999847]\n",
      "2605 [D loss: 0.999861] [G loss: 0.999802]\n",
      "2606 [D loss: 0.999839] [G loss: 0.999935]\n",
      "2607 [D loss: 0.999931] [G loss: 1.000153]\n",
      "2608 [D loss: 0.999917] [G loss: 1.000023]\n",
      "2609 [D loss: 0.999880] [G loss: 0.999873]\n",
      "2610 [D loss: 0.999830] [G loss: 0.999897]\n",
      "2611 [D loss: 0.999974] [G loss: 0.999993]\n",
      "2612 [D loss: 0.999891] [G loss: 1.000160]\n",
      "2613 [D loss: 0.999815] [G loss: 1.000132]\n",
      "2614 [D loss: 0.999808] [G loss: 1.000004]\n",
      "2615 [D loss: 0.999910] [G loss: 1.000012]\n",
      "2616 [D loss: 0.999938] [G loss: 1.000081]\n",
      "2617 [D loss: 0.999855] [G loss: 0.999901]\n",
      "2618 [D loss: 0.999932] [G loss: 0.999956]\n",
      "2619 [D loss: 0.999909] [G loss: 1.000090]\n",
      "2620 [D loss: 0.999927] [G loss: 1.000036]\n",
      "2621 [D loss: 0.999976] [G loss: 1.000063]\n",
      "2622 [D loss: 0.999739] [G loss: 1.000072]\n",
      "2623 [D loss: 0.999902] [G loss: 1.000008]\n",
      "2624 [D loss: 0.999856] [G loss: 1.000024]\n",
      "2625 [D loss: 0.999856] [G loss: 1.000002]\n",
      "2626 [D loss: 0.999916] [G loss: 1.000035]\n",
      "2627 [D loss: 0.999876] [G loss: 0.999922]\n",
      "2628 [D loss: 0.999936] [G loss: 1.000057]\n",
      "2629 [D loss: 0.999801] [G loss: 0.999973]\n",
      "2630 [D loss: 0.999922] [G loss: 0.999943]\n",
      "2631 [D loss: 1.000018] [G loss: 0.999965]\n",
      "2632 [D loss: 0.999858] [G loss: 1.000029]\n",
      "2633 [D loss: 0.999961] [G loss: 1.000084]\n",
      "2634 [D loss: 0.999934] [G loss: 1.000199]\n",
      "2635 [D loss: 0.999819] [G loss: 0.999971]\n",
      "2636 [D loss: 0.999889] [G loss: 1.000129]\n",
      "2637 [D loss: 0.999893] [G loss: 0.999798]\n",
      "2638 [D loss: 0.999883] [G loss: 1.000055]\n",
      "2639 [D loss: 0.999874] [G loss: 1.000099]\n",
      "2640 [D loss: 1.000014] [G loss: 1.000021]\n",
      "2641 [D loss: 0.999846] [G loss: 0.999925]\n",
      "2642 [D loss: 0.999859] [G loss: 0.999937]\n",
      "2643 [D loss: 0.999888] [G loss: 0.999960]\n",
      "2644 [D loss: 0.999944] [G loss: 0.999998]\n",
      "2645 [D loss: 0.999960] [G loss: 0.999984]\n",
      "2646 [D loss: 0.999890] [G loss: 0.999953]\n",
      "2647 [D loss: 0.999851] [G loss: 1.000108]\n",
      "2648 [D loss: 0.999861] [G loss: 0.999924]\n",
      "2649 [D loss: 0.999945] [G loss: 1.000011]\n",
      "2650 [D loss: 0.999854] [G loss: 1.000013]\n",
      "2651 [D loss: 0.999860] [G loss: 0.999931]\n",
      "2652 [D loss: 0.999838] [G loss: 0.999999]\n",
      "2653 [D loss: 0.999943] [G loss: 0.999934]\n",
      "2654 [D loss: 0.999952] [G loss: 0.999939]\n",
      "2655 [D loss: 0.999925] [G loss: 1.000072]\n",
      "2656 [D loss: 0.999860] [G loss: 0.999934]\n",
      "2657 [D loss: 0.999828] [G loss: 0.999714]\n",
      "2658 [D loss: 0.999983] [G loss: 1.000011]\n",
      "2659 [D loss: 0.999890] [G loss: 1.000137]\n",
      "2660 [D loss: 0.999936] [G loss: 1.000022]\n",
      "2661 [D loss: 0.999921] [G loss: 1.000015]\n",
      "2662 [D loss: 0.999902] [G loss: 1.000008]\n",
      "2663 [D loss: 0.999981] [G loss: 1.000066]\n",
      "2664 [D loss: 0.999908] [G loss: 0.999965]\n",
      "2665 [D loss: 0.999925] [G loss: 1.000019]\n",
      "2666 [D loss: 0.999947] [G loss: 1.000073]\n",
      "2667 [D loss: 0.999961] [G loss: 1.000046]\n",
      "2668 [D loss: 0.999980] [G loss: 1.000114]\n",
      "2669 [D loss: 0.999896] [G loss: 0.999934]\n",
      "2670 [D loss: 1.000037] [G loss: 0.999858]\n",
      "2671 [D loss: 0.999959] [G loss: 1.000059]\n",
      "2672 [D loss: 0.999896] [G loss: 0.999901]\n",
      "2673 [D loss: 0.999934] [G loss: 1.000036]\n",
      "2674 [D loss: 0.999953] [G loss: 1.000036]\n",
      "2675 [D loss: 0.999940] [G loss: 0.999981]\n",
      "2676 [D loss: 0.999969] [G loss: 1.000042]\n",
      "2677 [D loss: 0.999971] [G loss: 0.999986]\n",
      "2678 [D loss: 0.999965] [G loss: 1.000066]\n",
      "2679 [D loss: 0.999920] [G loss: 0.999864]\n",
      "2680 [D loss: 0.999873] [G loss: 1.000040]\n",
      "2681 [D loss: 0.999903] [G loss: 0.999965]\n",
      "2682 [D loss: 0.999876] [G loss: 1.000040]\n",
      "2683 [D loss: 0.999924] [G loss: 1.000079]\n",
      "2684 [D loss: 1.000017] [G loss: 0.999907]\n",
      "2685 [D loss: 0.999945] [G loss: 0.999977]\n",
      "2686 [D loss: 0.999907] [G loss: 1.000118]\n",
      "2687 [D loss: 0.999868] [G loss: 1.000073]\n",
      "2688 [D loss: 0.999840] [G loss: 0.999902]\n",
      "2689 [D loss: 0.999823] [G loss: 1.000103]\n",
      "2690 [D loss: 0.999884] [G loss: 1.000135]\n",
      "2691 [D loss: 0.999853] [G loss: 1.000054]\n",
      "2692 [D loss: 0.999998] [G loss: 0.999956]\n",
      "2693 [D loss: 0.999898] [G loss: 0.999958]\n",
      "2694 [D loss: 0.999957] [G loss: 0.999890]\n",
      "2695 [D loss: 0.999905] [G loss: 0.999968]\n",
      "2696 [D loss: 0.999870] [G loss: 1.000109]\n",
      "2697 [D loss: 0.999960] [G loss: 1.000174]\n",
      "2698 [D loss: 0.999885] [G loss: 0.999963]\n",
      "2699 [D loss: 0.999858] [G loss: 0.999940]\n",
      "2700 [D loss: 0.999894] [G loss: 1.000061]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2701 [D loss: 0.999973] [G loss: 1.000079]\n",
      "2702 [D loss: 0.999888] [G loss: 0.999947]\n",
      "2703 [D loss: 0.999922] [G loss: 1.000000]\n",
      "2704 [D loss: 0.999989] [G loss: 0.999956]\n",
      "2705 [D loss: 0.999969] [G loss: 1.000011]\n",
      "2706 [D loss: 0.999884] [G loss: 1.000128]\n",
      "2707 [D loss: 0.999942] [G loss: 1.000121]\n",
      "2708 [D loss: 0.999897] [G loss: 1.000083]\n",
      "2709 [D loss: 0.999915] [G loss: 0.999924]\n",
      "2710 [D loss: 0.999958] [G loss: 1.000055]\n",
      "2711 [D loss: 0.999936] [G loss: 1.000049]\n",
      "2712 [D loss: 0.999912] [G loss: 0.999854]\n",
      "2713 [D loss: 0.999882] [G loss: 1.000055]\n",
      "2714 [D loss: 0.999940] [G loss: 0.999944]\n",
      "2715 [D loss: 0.999935] [G loss: 0.999972]\n",
      "2716 [D loss: 0.999934] [G loss: 1.000006]\n",
      "2717 [D loss: 0.999869] [G loss: 0.999997]\n",
      "2718 [D loss: 1.000031] [G loss: 1.000049]\n",
      "2719 [D loss: 0.999867] [G loss: 0.999994]\n",
      "2720 [D loss: 0.999855] [G loss: 0.999852]\n",
      "2721 [D loss: 0.999918] [G loss: 0.999925]\n",
      "2722 [D loss: 0.999911] [G loss: 0.999944]\n",
      "2723 [D loss: 0.999990] [G loss: 0.999966]\n",
      "2724 [D loss: 0.999986] [G loss: 1.000083]\n",
      "2725 [D loss: 0.999852] [G loss: 0.999928]\n",
      "2726 [D loss: 0.999905] [G loss: 1.000008]\n",
      "2727 [D loss: 0.999877] [G loss: 0.999995]\n",
      "2728 [D loss: 0.999842] [G loss: 0.999995]\n",
      "2729 [D loss: 0.999904] [G loss: 1.000039]\n",
      "2730 [D loss: 0.999803] [G loss: 1.000051]\n",
      "2731 [D loss: 0.999903] [G loss: 1.000086]\n",
      "2732 [D loss: 0.999879] [G loss: 0.999998]\n",
      "2733 [D loss: 0.999964] [G loss: 0.999967]\n",
      "2734 [D loss: 0.999961] [G loss: 1.000122]\n",
      "2735 [D loss: 0.999871] [G loss: 1.000022]\n",
      "2736 [D loss: 0.999947] [G loss: 0.999857]\n",
      "2737 [D loss: 0.999857] [G loss: 1.000016]\n",
      "2738 [D loss: 0.999832] [G loss: 0.999981]\n",
      "2739 [D loss: 1.000027] [G loss: 0.999912]\n",
      "2740 [D loss: 0.999899] [G loss: 1.000160]\n",
      "2741 [D loss: 0.999909] [G loss: 0.999954]\n",
      "2742 [D loss: 0.999889] [G loss: 0.999947]\n",
      "2743 [D loss: 1.000025] [G loss: 0.999942]\n",
      "2744 [D loss: 0.999908] [G loss: 0.999945]\n",
      "2745 [D loss: 0.999915] [G loss: 1.000058]\n",
      "2746 [D loss: 0.999916] [G loss: 1.000016]\n",
      "2747 [D loss: 0.999952] [G loss: 0.999972]\n",
      "2748 [D loss: 0.999998] [G loss: 1.000048]\n",
      "2749 [D loss: 0.999868] [G loss: 1.000025]\n",
      "2750 [D loss: 0.999943] [G loss: 0.999919]\n",
      "2751 [D loss: 0.999931] [G loss: 1.000070]\n",
      "2752 [D loss: 0.999856] [G loss: 0.999951]\n",
      "2753 [D loss: 1.000087] [G loss: 0.999989]\n",
      "2754 [D loss: 0.999864] [G loss: 1.000085]\n",
      "2755 [D loss: 0.999971] [G loss: 1.000113]\n",
      "2756 [D loss: 0.999936] [G loss: 1.000110]\n",
      "2757 [D loss: 0.999954] [G loss: 1.000001]\n",
      "2758 [D loss: 0.999974] [G loss: 1.000015]\n",
      "2759 [D loss: 0.999857] [G loss: 0.999934]\n",
      "2760 [D loss: 0.999932] [G loss: 1.000035]\n",
      "2761 [D loss: 0.999946] [G loss: 1.000009]\n",
      "2762 [D loss: 0.999921] [G loss: 0.999985]\n",
      "2763 [D loss: 0.999928] [G loss: 1.000089]\n",
      "2764 [D loss: 0.999946] [G loss: 1.000010]\n",
      "2765 [D loss: 0.999902] [G loss: 1.000050]\n",
      "2766 [D loss: 0.999888] [G loss: 0.999918]\n",
      "2767 [D loss: 0.999952] [G loss: 1.000097]\n",
      "2768 [D loss: 0.999959] [G loss: 1.000040]\n",
      "2769 [D loss: 0.999893] [G loss: 1.000103]\n",
      "2770 [D loss: 0.999888] [G loss: 0.999984]\n",
      "2771 [D loss: 0.999914] [G loss: 0.999909]\n",
      "2772 [D loss: 0.999900] [G loss: 1.000093]\n",
      "2773 [D loss: 1.000004] [G loss: 1.000071]\n",
      "2774 [D loss: 0.999841] [G loss: 1.000151]\n",
      "2775 [D loss: 1.000006] [G loss: 1.000023]\n",
      "2776 [D loss: 0.999868] [G loss: 1.000193]\n",
      "2777 [D loss: 0.999894] [G loss: 1.000206]\n",
      "2778 [D loss: 0.999895] [G loss: 0.999869]\n",
      "2779 [D loss: 1.000026] [G loss: 1.000040]\n",
      "2780 [D loss: 0.999834] [G loss: 0.999979]\n",
      "2781 [D loss: 0.999875] [G loss: 1.000044]\n",
      "2782 [D loss: 0.999949] [G loss: 0.999958]\n",
      "2783 [D loss: 0.999788] [G loss: 0.999814]\n",
      "2784 [D loss: 0.999846] [G loss: 1.000034]\n",
      "2785 [D loss: 0.999928] [G loss: 0.999929]\n",
      "2786 [D loss: 0.999872] [G loss: 0.999982]\n",
      "2787 [D loss: 0.999929] [G loss: 0.999905]\n",
      "2788 [D loss: 0.999934] [G loss: 1.000155]\n",
      "2789 [D loss: 0.999895] [G loss: 0.999839]\n",
      "2790 [D loss: 0.999890] [G loss: 0.999973]\n",
      "2791 [D loss: 0.999867] [G loss: 0.999849]\n",
      "2792 [D loss: 0.999964] [G loss: 0.999977]\n",
      "2793 [D loss: 0.999952] [G loss: 1.000047]\n",
      "2794 [D loss: 0.999802] [G loss: 1.000029]\n",
      "2795 [D loss: 0.999879] [G loss: 1.000125]\n",
      "2796 [D loss: 0.999853] [G loss: 0.999971]\n",
      "2797 [D loss: 0.999902] [G loss: 1.000041]\n",
      "2798 [D loss: 0.999812] [G loss: 0.999929]\n",
      "2799 [D loss: 0.999894] [G loss: 1.000078]\n",
      "2800 [D loss: 0.999852] [G loss: 1.000045]\n",
      "2801 [D loss: 0.999927] [G loss: 1.000063]\n",
      "2802 [D loss: 0.999933] [G loss: 1.000025]\n",
      "2803 [D loss: 0.999849] [G loss: 0.999957]\n",
      "2804 [D loss: 0.999946] [G loss: 0.999881]\n",
      "2805 [D loss: 0.999842] [G loss: 0.999980]\n",
      "2806 [D loss: 0.999941] [G loss: 0.999968]\n",
      "2807 [D loss: 0.999813] [G loss: 0.999905]\n",
      "2808 [D loss: 0.999956] [G loss: 1.000092]\n",
      "2809 [D loss: 0.999857] [G loss: 0.999916]\n",
      "2810 [D loss: 0.999951] [G loss: 1.000091]\n",
      "2811 [D loss: 0.999923] [G loss: 0.999883]\n",
      "2812 [D loss: 0.999895] [G loss: 1.000050]\n",
      "2813 [D loss: 0.999971] [G loss: 0.999805]\n",
      "2814 [D loss: 0.999830] [G loss: 1.000057]\n",
      "2815 [D loss: 0.999814] [G loss: 1.000042]\n",
      "2816 [D loss: 0.999924] [G loss: 1.000103]\n",
      "2817 [D loss: 0.999987] [G loss: 0.999915]\n",
      "2818 [D loss: 0.999964] [G loss: 0.999974]\n",
      "2819 [D loss: 0.999919] [G loss: 0.999981]\n",
      "2820 [D loss: 0.999805] [G loss: 0.999966]\n",
      "2821 [D loss: 0.999963] [G loss: 1.000001]\n",
      "2822 [D loss: 0.999996] [G loss: 0.999936]\n",
      "2823 [D loss: 0.999895] [G loss: 1.000020]\n",
      "2824 [D loss: 0.999810] [G loss: 0.999829]\n",
      "2825 [D loss: 0.999860] [G loss: 0.999947]\n",
      "2826 [D loss: 0.999964] [G loss: 0.999907]\n",
      "2827 [D loss: 0.999882] [G loss: 0.999938]\n",
      "2828 [D loss: 0.999870] [G loss: 1.000061]\n",
      "2829 [D loss: 0.999977] [G loss: 0.999950]\n",
      "2830 [D loss: 0.999808] [G loss: 0.999754]\n",
      "2831 [D loss: 0.999879] [G loss: 0.999984]\n",
      "2832 [D loss: 0.999976] [G loss: 0.999931]\n",
      "2833 [D loss: 0.999854] [G loss: 0.999796]\n",
      "2834 [D loss: 0.999807] [G loss: 1.000052]\n",
      "2835 [D loss: 0.999773] [G loss: 0.999754]\n",
      "2836 [D loss: 0.999897] [G loss: 0.999862]\n",
      "2837 [D loss: 0.999697] [G loss: 0.999878]\n",
      "2838 [D loss: 0.999864] [G loss: 0.999911]\n",
      "2839 [D loss: 0.999775] [G loss: 0.999894]\n",
      "2840 [D loss: 0.999881] [G loss: 0.999867]\n",
      "2841 [D loss: 0.999853] [G loss: 0.999827]\n",
      "2842 [D loss: 0.999854] [G loss: 0.999973]\n",
      "2843 [D loss: 0.999838] [G loss: 1.000073]\n",
      "2844 [D loss: 0.999865] [G loss: 0.999941]\n",
      "2845 [D loss: 0.999822] [G loss: 0.999758]\n",
      "2846 [D loss: 0.999838] [G loss: 1.000001]\n",
      "2847 [D loss: 0.999869] [G loss: 1.000107]\n",
      "2848 [D loss: 0.999930] [G loss: 1.000009]\n",
      "2849 [D loss: 0.999881] [G loss: 0.999834]\n",
      "2850 [D loss: 0.999887] [G loss: 0.999813]\n",
      "2851 [D loss: 0.999880] [G loss: 1.000023]\n",
      "2852 [D loss: 0.999804] [G loss: 1.000067]\n",
      "2853 [D loss: 0.999834] [G loss: 1.000102]\n",
      "2854 [D loss: 0.999967] [G loss: 1.000123]\n",
      "2855 [D loss: 0.999976] [G loss: 0.999957]\n",
      "2856 [D loss: 0.999841] [G loss: 1.000066]\n",
      "2857 [D loss: 0.999894] [G loss: 1.000011]\n",
      "2858 [D loss: 0.999851] [G loss: 0.999926]\n",
      "2859 [D loss: 0.999945] [G loss: 0.999968]\n",
      "2860 [D loss: 0.999798] [G loss: 0.999722]\n",
      "2861 [D loss: 0.999850] [G loss: 0.999961]\n",
      "2862 [D loss: 1.000098] [G loss: 1.000037]\n",
      "2863 [D loss: 0.999863] [G loss: 0.999791]\n",
      "2864 [D loss: 0.999991] [G loss: 0.999821]\n",
      "2865 [D loss: 0.999873] [G loss: 1.000126]\n",
      "2866 [D loss: 0.999963] [G loss: 1.000046]\n",
      "2867 [D loss: 0.999766] [G loss: 0.999876]\n",
      "2868 [D loss: 0.999968] [G loss: 1.000004]\n",
      "2869 [D loss: 0.999923] [G loss: 0.999856]\n",
      "2870 [D loss: 0.999887] [G loss: 0.999811]\n",
      "2871 [D loss: 0.999789] [G loss: 0.999907]\n",
      "2872 [D loss: 1.000007] [G loss: 1.000210]\n",
      "2873 [D loss: 1.000058] [G loss: 1.000024]\n",
      "2874 [D loss: 0.999917] [G loss: 0.999845]\n",
      "2875 [D loss: 0.999893] [G loss: 0.999943]\n",
      "2876 [D loss: 0.999896] [G loss: 0.999829]\n",
      "2877 [D loss: 0.999905] [G loss: 0.999827]\n",
      "2878 [D loss: 0.999977] [G loss: 0.999880]\n",
      "2879 [D loss: 0.999678] [G loss: 1.000154]\n",
      "2880 [D loss: 0.999873] [G loss: 1.000056]\n",
      "2881 [D loss: 0.999973] [G loss: 0.999779]\n",
      "2882 [D loss: 0.999907] [G loss: 1.000025]\n",
      "2883 [D loss: 0.999826] [G loss: 1.000015]\n",
      "2884 [D loss: 1.000030] [G loss: 0.999925]\n",
      "2885 [D loss: 0.999805] [G loss: 0.999767]\n",
      "2886 [D loss: 0.999756] [G loss: 1.000195]\n",
      "2887 [D loss: 0.999946] [G loss: 1.000066]\n",
      "2888 [D loss: 0.999823] [G loss: 1.000010]\n",
      "2889 [D loss: 0.999853] [G loss: 1.000007]\n",
      "2890 [D loss: 1.000075] [G loss: 1.000019]\n",
      "2891 [D loss: 0.999846] [G loss: 0.999951]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2892 [D loss: 0.999855] [G loss: 1.000108]\n",
      "2893 [D loss: 0.999952] [G loss: 1.000023]\n",
      "2894 [D loss: 0.999856] [G loss: 1.000038]\n",
      "2895 [D loss: 0.999792] [G loss: 1.000090]\n",
      "2896 [D loss: 0.999834] [G loss: 0.999875]\n",
      "2897 [D loss: 0.999931] [G loss: 0.999810]\n",
      "2898 [D loss: 0.999905] [G loss: 0.999913]\n",
      "2899 [D loss: 0.999864] [G loss: 0.999884]\n",
      "2900 [D loss: 0.999861] [G loss: 1.000005]\n",
      "2901 [D loss: 0.999817] [G loss: 0.999891]\n",
      "2902 [D loss: 0.999885] [G loss: 0.999986]\n",
      "2903 [D loss: 0.999932] [G loss: 1.000036]\n",
      "2904 [D loss: 0.999949] [G loss: 0.999965]\n",
      "2905 [D loss: 0.999976] [G loss: 0.999865]\n",
      "2906 [D loss: 0.999891] [G loss: 0.999909]\n",
      "2907 [D loss: 0.999781] [G loss: 1.000089]\n",
      "2908 [D loss: 0.999940] [G loss: 0.999897]\n",
      "2909 [D loss: 0.999878] [G loss: 0.999943]\n",
      "2910 [D loss: 0.999938] [G loss: 1.000093]\n",
      "2911 [D loss: 0.999839] [G loss: 0.999845]\n",
      "2912 [D loss: 0.999794] [G loss: 0.999820]\n",
      "2913 [D loss: 0.999887] [G loss: 0.999887]\n",
      "2914 [D loss: 0.999792] [G loss: 0.999926]\n",
      "2915 [D loss: 0.999954] [G loss: 0.999908]\n",
      "2916 [D loss: 0.999861] [G loss: 0.999993]\n",
      "2917 [D loss: 0.999819] [G loss: 1.000030]\n",
      "2918 [D loss: 0.999877] [G loss: 1.000079]\n",
      "2919 [D loss: 0.999762] [G loss: 1.000100]\n",
      "2920 [D loss: 0.999809] [G loss: 0.999968]\n",
      "2921 [D loss: 0.999927] [G loss: 1.000104]\n",
      "2922 [D loss: 0.999800] [G loss: 0.999887]\n",
      "2923 [D loss: 0.999879] [G loss: 1.000069]\n",
      "2924 [D loss: 0.999813] [G loss: 0.999988]\n",
      "2925 [D loss: 1.000033] [G loss: 0.999952]\n",
      "2926 [D loss: 0.999775] [G loss: 0.999833]\n",
      "2927 [D loss: 0.999920] [G loss: 0.999978]\n",
      "2928 [D loss: 0.999974] [G loss: 0.999907]\n",
      "2929 [D loss: 0.999914] [G loss: 0.999922]\n",
      "2930 [D loss: 0.999985] [G loss: 0.999943]\n",
      "2931 [D loss: 0.999951] [G loss: 1.000175]\n",
      "2932 [D loss: 1.000054] [G loss: 0.999805]\n",
      "2933 [D loss: 0.999761] [G loss: 0.999957]\n",
      "2934 [D loss: 0.999638] [G loss: 0.999801]\n",
      "2935 [D loss: 0.999907] [G loss: 1.000034]\n",
      "2936 [D loss: 0.999887] [G loss: 0.999904]\n",
      "2937 [D loss: 0.999754] [G loss: 0.999961]\n",
      "2938 [D loss: 0.999873] [G loss: 1.000092]\n",
      "2939 [D loss: 0.999859] [G loss: 0.999874]\n",
      "2940 [D loss: 0.999966] [G loss: 1.000031]\n",
      "2941 [D loss: 0.999833] [G loss: 0.999968]\n",
      "2942 [D loss: 0.999936] [G loss: 0.999924]\n",
      "2943 [D loss: 0.999945] [G loss: 0.999975]\n",
      "2944 [D loss: 0.999711] [G loss: 0.999971]\n",
      "2945 [D loss: 0.999865] [G loss: 0.999863]\n",
      "2946 [D loss: 0.999815] [G loss: 0.999988]\n",
      "2947 [D loss: 1.000025] [G loss: 1.000029]\n",
      "2948 [D loss: 0.999794] [G loss: 0.999990]\n",
      "2949 [D loss: 0.999938] [G loss: 1.000306]\n",
      "2950 [D loss: 0.999911] [G loss: 0.999799]\n",
      "2951 [D loss: 0.999798] [G loss: 0.999754]\n",
      "2952 [D loss: 1.000008] [G loss: 0.999942]\n",
      "2953 [D loss: 0.999843] [G loss: 1.000044]\n",
      "2954 [D loss: 0.999752] [G loss: 0.999899]\n",
      "2955 [D loss: 0.999902] [G loss: 1.000105]\n",
      "2956 [D loss: 0.999919] [G loss: 1.000023]\n",
      "2957 [D loss: 0.999753] [G loss: 1.000002]\n",
      "2958 [D loss: 0.999635] [G loss: 0.999918]\n",
      "2959 [D loss: 0.999895] [G loss: 0.999974]\n",
      "2960 [D loss: 0.999858] [G loss: 1.000069]\n",
      "2961 [D loss: 0.999801] [G loss: 0.999839]\n",
      "2962 [D loss: 0.999978] [G loss: 0.999927]\n",
      "2963 [D loss: 0.999855] [G loss: 0.999811]\n",
      "2964 [D loss: 0.999983] [G loss: 1.000042]\n",
      "2965 [D loss: 0.999816] [G loss: 1.000163]\n",
      "2966 [D loss: 0.999927] [G loss: 0.999891]\n",
      "2967 [D loss: 0.999835] [G loss: 1.000088]\n",
      "2968 [D loss: 0.999828] [G loss: 0.999938]\n",
      "2969 [D loss: 0.999936] [G loss: 1.000159]\n",
      "2970 [D loss: 0.999890] [G loss: 0.999868]\n",
      "2971 [D loss: 0.999839] [G loss: 0.999912]\n",
      "2972 [D loss: 0.999960] [G loss: 1.000023]\n",
      "2973 [D loss: 0.999986] [G loss: 0.999865]\n",
      "2974 [D loss: 0.999895] [G loss: 1.000216]\n",
      "2975 [D loss: 0.999905] [G loss: 1.000050]\n",
      "2976 [D loss: 0.999916] [G loss: 0.999836]\n",
      "2977 [D loss: 0.999787] [G loss: 1.000038]\n",
      "2978 [D loss: 0.999797] [G loss: 0.999910]\n",
      "2979 [D loss: 0.999842] [G loss: 1.000007]\n",
      "2980 [D loss: 0.999936] [G loss: 0.999951]\n",
      "2981 [D loss: 1.000004] [G loss: 1.000017]\n",
      "2982 [D loss: 0.999918] [G loss: 0.999977]\n",
      "2983 [D loss: 0.999903] [G loss: 1.000065]\n",
      "2984 [D loss: 0.999804] [G loss: 0.999772]\n",
      "2985 [D loss: 0.999949] [G loss: 0.999976]\n",
      "2986 [D loss: 0.999713] [G loss: 0.999912]\n",
      "2987 [D loss: 0.999957] [G loss: 1.000134]\n",
      "2988 [D loss: 0.999880] [G loss: 1.000012]\n",
      "2989 [D loss: 0.999809] [G loss: 0.999983]\n",
      "2990 [D loss: 0.999860] [G loss: 0.999971]\n",
      "2991 [D loss: 0.999761] [G loss: 0.999863]\n",
      "2992 [D loss: 0.999710] [G loss: 0.999893]\n",
      "2993 [D loss: 0.999665] [G loss: 1.000017]\n",
      "2994 [D loss: 0.999948] [G loss: 0.999893]\n",
      "2995 [D loss: 0.999896] [G loss: 1.000025]\n",
      "2996 [D loss: 0.999698] [G loss: 1.000055]\n",
      "2997 [D loss: 0.999929] [G loss: 0.999859]\n",
      "2998 [D loss: 0.999697] [G loss: 0.999952]\n",
      "2999 [D loss: 0.999668] [G loss: 1.000027]\n",
      "3000 [D loss: 0.999725] [G loss: 1.000176]\n",
      "3001 [D loss: 0.999891] [G loss: 0.999867]\n",
      "3002 [D loss: 0.999966] [G loss: 1.000093]\n",
      "3003 [D loss: 0.999783] [G loss: 0.999948]\n",
      "3004 [D loss: 0.999856] [G loss: 1.000049]\n",
      "3005 [D loss: 0.999879] [G loss: 1.000124]\n",
      "3006 [D loss: 0.999711] [G loss: 0.999883]\n",
      "3007 [D loss: 0.999870] [G loss: 0.999988]\n",
      "3008 [D loss: 0.999926] [G loss: 1.000087]\n",
      "3009 [D loss: 0.999827] [G loss: 0.999861]\n",
      "3010 [D loss: 0.999769] [G loss: 1.000146]\n",
      "3011 [D loss: 0.999829] [G loss: 1.000107]\n",
      "3012 [D loss: 0.999944] [G loss: 0.999639]\n",
      "3013 [D loss: 0.999962] [G loss: 0.999929]\n",
      "3014 [D loss: 0.999885] [G loss: 1.000016]\n",
      "3015 [D loss: 0.999991] [G loss: 0.999661]\n",
      "3016 [D loss: 0.999943] [G loss: 0.999852]\n",
      "3017 [D loss: 0.999982] [G loss: 1.000112]\n",
      "3018 [D loss: 0.999747] [G loss: 1.000023]\n",
      "3019 [D loss: 0.999780] [G loss: 1.000114]\n",
      "3020 [D loss: 0.999815] [G loss: 1.000272]\n",
      "3021 [D loss: 0.999790] [G loss: 1.000085]\n",
      "3022 [D loss: 0.999838] [G loss: 1.000050]\n",
      "3023 [D loss: 0.999738] [G loss: 1.000127]\n",
      "3024 [D loss: 0.999910] [G loss: 0.999882]\n",
      "3025 [D loss: 0.999928] [G loss: 0.999907]\n",
      "3026 [D loss: 0.999678] [G loss: 0.999666]\n",
      "3027 [D loss: 0.999956] [G loss: 1.000145]\n",
      "3028 [D loss: 0.999845] [G loss: 0.999958]\n",
      "3029 [D loss: 0.999944] [G loss: 1.000025]\n",
      "3030 [D loss: 0.999977] [G loss: 1.000160]\n",
      "3031 [D loss: 0.999825] [G loss: 0.999902]\n",
      "3032 [D loss: 0.999795] [G loss: 0.999855]\n",
      "3033 [D loss: 0.999822] [G loss: 0.999989]\n",
      "3034 [D loss: 0.999761] [G loss: 1.000108]\n",
      "3035 [D loss: 0.999818] [G loss: 1.000053]\n",
      "3036 [D loss: 0.999868] [G loss: 1.000022]\n",
      "3037 [D loss: 0.999751] [G loss: 1.000021]\n",
      "3038 [D loss: 0.999826] [G loss: 1.000087]\n",
      "3039 [D loss: 0.999799] [G loss: 0.999913]\n",
      "3040 [D loss: 0.999859] [G loss: 0.999829]\n",
      "3041 [D loss: 0.999880] [G loss: 1.000023]\n",
      "3042 [D loss: 0.999872] [G loss: 0.999633]\n",
      "3043 [D loss: 0.999755] [G loss: 0.999991]\n",
      "3044 [D loss: 0.999945] [G loss: 0.999851]\n",
      "3045 [D loss: 0.999871] [G loss: 1.000010]\n",
      "3046 [D loss: 1.000009] [G loss: 0.999901]\n",
      "3047 [D loss: 0.999900] [G loss: 1.000055]\n",
      "3048 [D loss: 0.999874] [G loss: 0.999939]\n",
      "3049 [D loss: 0.999890] [G loss: 0.999968]\n",
      "3050 [D loss: 0.999902] [G loss: 0.999843]\n",
      "3051 [D loss: 1.000014] [G loss: 0.999747]\n",
      "3052 [D loss: 0.999864] [G loss: 1.000056]\n",
      "3053 [D loss: 0.999858] [G loss: 0.999986]\n",
      "3054 [D loss: 0.999725] [G loss: 0.999966]\n",
      "3055 [D loss: 0.999792] [G loss: 0.999931]\n",
      "3056 [D loss: 0.999844] [G loss: 0.999985]\n",
      "3057 [D loss: 0.999936] [G loss: 1.000092]\n",
      "3058 [D loss: 0.999871] [G loss: 0.999969]\n",
      "3059 [D loss: 0.999760] [G loss: 1.000100]\n",
      "3060 [D loss: 0.999790] [G loss: 0.999849]\n",
      "3061 [D loss: 0.999707] [G loss: 0.999845]\n",
      "3062 [D loss: 0.999730] [G loss: 1.000054]\n",
      "3063 [D loss: 0.999876] [G loss: 0.999795]\n",
      "3064 [D loss: 0.999964] [G loss: 1.000024]\n",
      "3065 [D loss: 0.999823] [G loss: 1.000058]\n",
      "3066 [D loss: 0.999690] [G loss: 0.999774]\n",
      "3067 [D loss: 0.999734] [G loss: 0.999899]\n",
      "3068 [D loss: 0.999788] [G loss: 0.999819]\n",
      "3069 [D loss: 0.999929] [G loss: 0.999923]\n",
      "3070 [D loss: 1.000043] [G loss: 0.999841]\n",
      "3071 [D loss: 0.999878] [G loss: 1.000129]\n",
      "3072 [D loss: 0.999827] [G loss: 0.999915]\n",
      "3073 [D loss: 0.999959] [G loss: 0.999935]\n",
      "3074 [D loss: 0.999716] [G loss: 0.999893]\n",
      "3075 [D loss: 0.999846] [G loss: 0.999855]\n",
      "3076 [D loss: 0.999819] [G loss: 0.999730]\n",
      "3077 [D loss: 0.999882] [G loss: 0.999964]\n",
      "3078 [D loss: 0.999843] [G loss: 0.999971]\n",
      "3079 [D loss: 0.999878] [G loss: 0.999979]\n",
      "3080 [D loss: 0.999787] [G loss: 0.999888]\n",
      "3081 [D loss: 0.999690] [G loss: 1.000054]\n",
      "3082 [D loss: 0.999819] [G loss: 0.999827]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3083 [D loss: 0.999736] [G loss: 0.999848]\n",
      "3084 [D loss: 0.999788] [G loss: 1.000009]\n",
      "3085 [D loss: 0.999805] [G loss: 1.000045]\n",
      "3086 [D loss: 0.999753] [G loss: 1.000120]\n",
      "3087 [D loss: 0.999860] [G loss: 1.000070]\n",
      "3088 [D loss: 1.000016] [G loss: 1.000178]\n",
      "3089 [D loss: 0.999794] [G loss: 0.999826]\n",
      "3090 [D loss: 0.999868] [G loss: 0.999736]\n",
      "3091 [D loss: 0.999946] [G loss: 0.999727]\n",
      "3092 [D loss: 0.999933] [G loss: 0.999948]\n",
      "3093 [D loss: 0.999886] [G loss: 1.000033]\n",
      "3094 [D loss: 0.999959] [G loss: 1.000092]\n",
      "3095 [D loss: 0.999895] [G loss: 1.000017]\n",
      "3096 [D loss: 0.999611] [G loss: 0.999883]\n",
      "3097 [D loss: 0.999934] [G loss: 0.999868]\n",
      "3098 [D loss: 0.999840] [G loss: 1.000069]\n",
      "3099 [D loss: 0.999789] [G loss: 1.000010]\n",
      "3100 [D loss: 0.999954] [G loss: 0.999925]\n",
      "3101 [D loss: 1.000054] [G loss: 0.999955]\n",
      "3102 [D loss: 0.999988] [G loss: 0.999898]\n",
      "3103 [D loss: 0.999893] [G loss: 0.999709]\n",
      "3104 [D loss: 0.999905] [G loss: 1.000066]\n",
      "3105 [D loss: 0.999970] [G loss: 1.000017]\n",
      "3106 [D loss: 0.999912] [G loss: 0.999788]\n",
      "3107 [D loss: 0.999681] [G loss: 0.999975]\n",
      "3108 [D loss: 0.999959] [G loss: 0.999655]\n",
      "3109 [D loss: 0.999839] [G loss: 0.999721]\n",
      "3110 [D loss: 0.999777] [G loss: 0.999944]\n",
      "3111 [D loss: 0.999824] [G loss: 0.999837]\n",
      "3112 [D loss: 1.000058] [G loss: 0.999913]\n",
      "3113 [D loss: 0.999977] [G loss: 0.999787]\n",
      "3114 [D loss: 0.999810] [G loss: 0.999993]\n",
      "3115 [D loss: 0.999883] [G loss: 0.999861]\n",
      "3116 [D loss: 0.999833] [G loss: 1.000040]\n",
      "3117 [D loss: 0.999762] [G loss: 0.999660]\n",
      "3118 [D loss: 0.999873] [G loss: 1.000086]\n",
      "3119 [D loss: 0.999812] [G loss: 0.999978]\n",
      "3120 [D loss: 0.999879] [G loss: 1.000158]\n",
      "3121 [D loss: 0.999894] [G loss: 1.000022]\n",
      "3122 [D loss: 0.999803] [G loss: 1.000042]\n",
      "3123 [D loss: 0.999790] [G loss: 1.000065]\n",
      "3124 [D loss: 0.999901] [G loss: 0.999965]\n",
      "3125 [D loss: 0.999867] [G loss: 0.999741]\n",
      "3126 [D loss: 0.999837] [G loss: 0.999856]\n",
      "3127 [D loss: 0.999857] [G loss: 0.999738]\n",
      "3128 [D loss: 0.999925] [G loss: 0.999893]\n",
      "3129 [D loss: 0.999843] [G loss: 0.999825]\n",
      "3130 [D loss: 0.999911] [G loss: 0.999963]\n",
      "3131 [D loss: 0.999742] [G loss: 0.999864]\n",
      "3132 [D loss: 0.999869] [G loss: 0.999809]\n",
      "3133 [D loss: 1.000049] [G loss: 0.999999]\n",
      "3134 [D loss: 0.999974] [G loss: 1.000214]\n",
      "3135 [D loss: 0.999927] [G loss: 1.000060]\n",
      "3136 [D loss: 0.999907] [G loss: 0.999917]\n",
      "3137 [D loss: 0.999778] [G loss: 0.999827]\n",
      "3138 [D loss: 0.999996] [G loss: 1.000064]\n",
      "3139 [D loss: 0.999891] [G loss: 1.000029]\n",
      "3140 [D loss: 0.999654] [G loss: 0.999855]\n",
      "3141 [D loss: 0.999858] [G loss: 1.000141]\n",
      "3142 [D loss: 0.999750] [G loss: 0.999720]\n",
      "3143 [D loss: 0.999792] [G loss: 0.999868]\n",
      "3144 [D loss: 0.999695] [G loss: 0.999857]\n",
      "3145 [D loss: 0.999822] [G loss: 0.999936]\n",
      "3146 [D loss: 1.000049] [G loss: 1.000129]\n",
      "3147 [D loss: 0.999934] [G loss: 1.000230]\n",
      "3148 [D loss: 0.999926] [G loss: 1.000264]\n",
      "3149 [D loss: 0.999784] [G loss: 0.999859]\n",
      "3150 [D loss: 0.999936] [G loss: 1.000032]\n",
      "3151 [D loss: 0.999943] [G loss: 0.999876]\n",
      "3152 [D loss: 0.999915] [G loss: 0.999738]\n",
      "3153 [D loss: 0.999966] [G loss: 0.999939]\n",
      "3154 [D loss: 0.999951] [G loss: 1.000087]\n",
      "3155 [D loss: 0.999831] [G loss: 0.999955]\n",
      "3156 [D loss: 0.999939] [G loss: 1.000081]\n",
      "3157 [D loss: 0.999799] [G loss: 0.999850]\n",
      "3158 [D loss: 1.000067] [G loss: 0.999792]\n",
      "3159 [D loss: 0.999808] [G loss: 0.999961]\n",
      "3160 [D loss: 0.999854] [G loss: 0.999884]\n",
      "3161 [D loss: 0.999951] [G loss: 0.999820]\n",
      "3162 [D loss: 0.999976] [G loss: 1.000014]\n",
      "3163 [D loss: 0.999929] [G loss: 1.000023]\n",
      "3164 [D loss: 0.999870] [G loss: 0.999772]\n",
      "3165 [D loss: 0.999749] [G loss: 0.999838]\n",
      "3166 [D loss: 0.999790] [G loss: 0.999934]\n",
      "3167 [D loss: 0.999905] [G loss: 0.999882]\n",
      "3168 [D loss: 0.999940] [G loss: 0.999908]\n",
      "3169 [D loss: 0.999984] [G loss: 0.999890]\n",
      "3170 [D loss: 0.999814] [G loss: 0.999802]\n",
      "3171 [D loss: 1.000019] [G loss: 0.999750]\n",
      "3172 [D loss: 0.999876] [G loss: 0.999847]\n",
      "3173 [D loss: 0.999821] [G loss: 0.999981]\n",
      "3174 [D loss: 0.999780] [G loss: 0.999873]\n",
      "3175 [D loss: 0.999873] [G loss: 0.999982]\n",
      "3176 [D loss: 0.999783] [G loss: 0.999973]\n",
      "3177 [D loss: 0.999848] [G loss: 0.999846]\n",
      "3178 [D loss: 0.999944] [G loss: 0.999834]\n",
      "3179 [D loss: 0.999920] [G loss: 0.999871]\n",
      "3180 [D loss: 0.999961] [G loss: 1.000148]\n",
      "3181 [D loss: 0.999946] [G loss: 0.999705]\n",
      "3182 [D loss: 0.999825] [G loss: 0.999994]\n",
      "3183 [D loss: 0.999713] [G loss: 0.999973]\n",
      "3184 [D loss: 0.999843] [G loss: 1.000124]\n",
      "3185 [D loss: 0.999897] [G loss: 1.000235]\n",
      "3186 [D loss: 0.999890] [G loss: 1.000067]\n",
      "3187 [D loss: 0.999869] [G loss: 0.999816]\n",
      "3188 [D loss: 0.999923] [G loss: 0.999989]\n",
      "3189 [D loss: 0.999891] [G loss: 0.999988]\n",
      "3190 [D loss: 0.999939] [G loss: 0.999906]\n",
      "3191 [D loss: 0.999754] [G loss: 0.999770]\n",
      "3192 [D loss: 0.999825] [G loss: 0.999846]\n",
      "3193 [D loss: 0.999739] [G loss: 1.000018]\n",
      "3194 [D loss: 0.999842] [G loss: 1.000031]\n",
      "3195 [D loss: 0.999838] [G loss: 1.000086]\n",
      "3196 [D loss: 0.999807] [G loss: 1.000182]\n",
      "3197 [D loss: 0.999930] [G loss: 0.999947]\n",
      "3198 [D loss: 0.999782] [G loss: 1.000033]\n",
      "3199 [D loss: 0.999744] [G loss: 0.999944]\n",
      "3200 [D loss: 0.999948] [G loss: 0.999974]\n",
      "3201 [D loss: 0.999914] [G loss: 0.999876]\n",
      "3202 [D loss: 0.999982] [G loss: 1.000182]\n",
      "3203 [D loss: 0.999725] [G loss: 0.999944]\n",
      "3204 [D loss: 0.999887] [G loss: 1.000000]\n",
      "3205 [D loss: 1.000094] [G loss: 0.999846]\n",
      "3206 [D loss: 0.999963] [G loss: 0.999684]\n",
      "3207 [D loss: 0.999694] [G loss: 1.000157]\n",
      "3208 [D loss: 0.999764] [G loss: 0.999898]\n",
      "3209 [D loss: 0.999896] [G loss: 0.999870]\n",
      "3210 [D loss: 0.999875] [G loss: 0.999963]\n",
      "3211 [D loss: 0.999863] [G loss: 1.000148]\n",
      "3212 [D loss: 0.999766] [G loss: 0.999945]\n",
      "3213 [D loss: 0.999939] [G loss: 0.999896]\n",
      "3214 [D loss: 1.000000] [G loss: 0.999769]\n",
      "3215 [D loss: 0.999969] [G loss: 0.999768]\n",
      "3216 [D loss: 0.999684] [G loss: 0.999809]\n",
      "3217 [D loss: 0.999867] [G loss: 0.999945]\n",
      "3218 [D loss: 0.999918] [G loss: 1.000125]\n",
      "3219 [D loss: 0.999902] [G loss: 1.000135]\n",
      "3220 [D loss: 1.000067] [G loss: 0.999839]\n",
      "3221 [D loss: 0.999820] [G loss: 0.999943]\n",
      "3222 [D loss: 0.999887] [G loss: 1.000038]\n",
      "3223 [D loss: 0.999930] [G loss: 1.000170]\n",
      "3224 [D loss: 0.999885] [G loss: 0.999857]\n",
      "3225 [D loss: 0.999863] [G loss: 1.000078]\n",
      "3226 [D loss: 0.999831] [G loss: 0.999909]\n",
      "3227 [D loss: 0.999913] [G loss: 0.999896]\n",
      "3228 [D loss: 0.999784] [G loss: 0.999928]\n",
      "3229 [D loss: 0.999870] [G loss: 0.999926]\n",
      "3230 [D loss: 0.999802] [G loss: 0.999881]\n",
      "3231 [D loss: 0.999857] [G loss: 1.000009]\n",
      "3232 [D loss: 0.999720] [G loss: 1.000172]\n",
      "3233 [D loss: 0.999874] [G loss: 1.000156]\n",
      "3234 [D loss: 1.000030] [G loss: 0.999995]\n",
      "3235 [D loss: 0.999840] [G loss: 1.000025]\n",
      "3236 [D loss: 0.999969] [G loss: 1.000021]\n",
      "3237 [D loss: 0.999776] [G loss: 1.000113]\n",
      "3238 [D loss: 0.999822] [G loss: 1.000085]\n",
      "3239 [D loss: 0.999885] [G loss: 0.999875]\n",
      "3240 [D loss: 0.999806] [G loss: 0.999848]\n",
      "3241 [D loss: 0.999906] [G loss: 1.000098]\n",
      "3242 [D loss: 0.999952] [G loss: 0.999955]\n",
      "3243 [D loss: 0.999842] [G loss: 1.000036]\n",
      "3244 [D loss: 0.999853] [G loss: 0.999843]\n",
      "3245 [D loss: 0.999965] [G loss: 0.999864]\n",
      "3246 [D loss: 0.999736] [G loss: 0.999962]\n",
      "3247 [D loss: 0.999947] [G loss: 1.000085]\n",
      "3248 [D loss: 0.999845] [G loss: 0.999862]\n",
      "3249 [D loss: 1.000004] [G loss: 1.000012]\n",
      "3250 [D loss: 0.999836] [G loss: 0.999859]\n",
      "3251 [D loss: 0.999875] [G loss: 1.000166]\n",
      "3252 [D loss: 0.999897] [G loss: 1.000125]\n",
      "3253 [D loss: 0.999888] [G loss: 0.999885]\n",
      "3254 [D loss: 0.999988] [G loss: 1.000033]\n",
      "3255 [D loss: 0.999930] [G loss: 0.999881]\n",
      "3256 [D loss: 0.999912] [G loss: 0.999992]\n",
      "3257 [D loss: 0.999878] [G loss: 1.000082]\n",
      "3258 [D loss: 0.999929] [G loss: 0.999948]\n",
      "3259 [D loss: 0.999948] [G loss: 0.999971]\n",
      "3260 [D loss: 0.999884] [G loss: 0.999915]\n",
      "3261 [D loss: 0.999927] [G loss: 0.999964]\n",
      "3262 [D loss: 0.999838] [G loss: 0.999886]\n",
      "3263 [D loss: 0.999850] [G loss: 1.000022]\n",
      "3264 [D loss: 0.999970] [G loss: 1.000083]\n",
      "3265 [D loss: 0.999806] [G loss: 0.999843]\n",
      "3266 [D loss: 0.999881] [G loss: 0.999947]\n",
      "3267 [D loss: 0.999857] [G loss: 1.000009]\n",
      "3268 [D loss: 0.999958] [G loss: 1.000040]\n",
      "3269 [D loss: 0.999827] [G loss: 0.999982]\n",
      "3270 [D loss: 0.999922] [G loss: 0.999978]\n",
      "3271 [D loss: 0.999824] [G loss: 1.000062]\n",
      "3272 [D loss: 0.999840] [G loss: 0.999906]\n",
      "3273 [D loss: 1.000041] [G loss: 0.999914]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3274 [D loss: 0.999877] [G loss: 0.999987]\n",
      "3275 [D loss: 0.999905] [G loss: 1.000181]\n",
      "3276 [D loss: 0.999946] [G loss: 1.000009]\n",
      "3277 [D loss: 1.000049] [G loss: 0.999957]\n",
      "3278 [D loss: 0.999807] [G loss: 0.999996]\n",
      "3279 [D loss: 0.999903] [G loss: 1.000087]\n",
      "3280 [D loss: 0.999842] [G loss: 1.000076]\n",
      "3281 [D loss: 0.999888] [G loss: 1.000044]\n",
      "3282 [D loss: 0.999872] [G loss: 0.999988]\n",
      "3283 [D loss: 0.999940] [G loss: 1.000027]\n",
      "3284 [D loss: 0.999967] [G loss: 1.000079]\n",
      "3285 [D loss: 0.999830] [G loss: 1.000073]\n",
      "3286 [D loss: 0.999889] [G loss: 1.000015]\n",
      "3287 [D loss: 0.999874] [G loss: 1.000203]\n",
      "3288 [D loss: 0.999788] [G loss: 1.000018]\n",
      "3289 [D loss: 0.999765] [G loss: 1.000131]\n",
      "3290 [D loss: 0.999814] [G loss: 1.000058]\n",
      "3291 [D loss: 0.999925] [G loss: 1.000083]\n",
      "3292 [D loss: 0.999875] [G loss: 1.000113]\n",
      "3293 [D loss: 0.999924] [G loss: 1.000030]\n",
      "3294 [D loss: 0.999892] [G loss: 1.000014]\n",
      "3295 [D loss: 0.999878] [G loss: 0.999848]\n",
      "3296 [D loss: 0.999959] [G loss: 0.999971]\n",
      "3297 [D loss: 0.999921] [G loss: 1.000053]\n",
      "3298 [D loss: 0.999879] [G loss: 1.000149]\n",
      "3299 [D loss: 0.999953] [G loss: 0.999915]\n",
      "3300 [D loss: 0.999936] [G loss: 0.999910]\n",
      "3301 [D loss: 0.999889] [G loss: 0.999864]\n",
      "3302 [D loss: 0.999909] [G loss: 0.999958]\n",
      "3303 [D loss: 0.999894] [G loss: 0.999991]\n",
      "3304 [D loss: 0.999888] [G loss: 0.999968]\n",
      "3305 [D loss: 0.999911] [G loss: 0.999891]\n",
      "3306 [D loss: 0.999934] [G loss: 0.999991]\n",
      "3307 [D loss: 0.999933] [G loss: 0.999980]\n",
      "3308 [D loss: 0.999963] [G loss: 0.999954]\n",
      "3309 [D loss: 0.999757] [G loss: 0.999816]\n",
      "3310 [D loss: 0.999895] [G loss: 1.000058]\n",
      "3311 [D loss: 1.000020] [G loss: 1.000137]\n",
      "3312 [D loss: 0.999994] [G loss: 0.999949]\n",
      "3313 [D loss: 0.999863] [G loss: 1.000031]\n",
      "3314 [D loss: 0.999902] [G loss: 1.000019]\n",
      "3315 [D loss: 0.999709] [G loss: 0.999937]\n",
      "3316 [D loss: 0.999901] [G loss: 0.999963]\n",
      "3317 [D loss: 0.999858] [G loss: 1.000060]\n",
      "3318 [D loss: 0.999909] [G loss: 1.000090]\n",
      "3319 [D loss: 0.999881] [G loss: 0.999956]\n",
      "3320 [D loss: 0.999948] [G loss: 0.999935]\n",
      "3321 [D loss: 0.999863] [G loss: 0.999949]\n",
      "3322 [D loss: 1.000009] [G loss: 0.999871]\n",
      "3323 [D loss: 0.999935] [G loss: 1.000075]\n",
      "3324 [D loss: 1.000019] [G loss: 1.000016]\n",
      "3325 [D loss: 0.999930] [G loss: 0.999859]\n",
      "3326 [D loss: 0.999840] [G loss: 1.000181]\n",
      "3327 [D loss: 0.999930] [G loss: 0.999905]\n",
      "3328 [D loss: 0.999967] [G loss: 0.999980]\n",
      "3329 [D loss: 0.999793] [G loss: 0.999873]\n",
      "3330 [D loss: 0.999839] [G loss: 0.999907]\n",
      "3331 [D loss: 0.999765] [G loss: 0.999879]\n",
      "3332 [D loss: 0.999792] [G loss: 1.000060]\n",
      "3333 [D loss: 0.999913] [G loss: 0.999914]\n",
      "3334 [D loss: 0.999887] [G loss: 1.000088]\n",
      "3335 [D loss: 0.999856] [G loss: 0.999980]\n",
      "3336 [D loss: 0.999979] [G loss: 1.000038]\n",
      "3337 [D loss: 0.999853] [G loss: 0.999854]\n",
      "3338 [D loss: 0.999889] [G loss: 0.999989]\n",
      "3339 [D loss: 0.999920] [G loss: 1.000069]\n",
      "3340 [D loss: 0.999932] [G loss: 1.000075]\n",
      "3341 [D loss: 0.999766] [G loss: 0.999851]\n",
      "3342 [D loss: 0.999865] [G loss: 1.000003]\n",
      "3343 [D loss: 0.999909] [G loss: 0.999992]\n",
      "3344 [D loss: 1.000050] [G loss: 0.999927]\n",
      "3345 [D loss: 0.999928] [G loss: 1.000044]\n",
      "3346 [D loss: 0.999898] [G loss: 0.999999]\n",
      "3347 [D loss: 1.000009] [G loss: 0.999937]\n",
      "3348 [D loss: 0.999951] [G loss: 0.999991]\n",
      "3349 [D loss: 0.999765] [G loss: 0.999819]\n",
      "3350 [D loss: 0.999973] [G loss: 1.000013]\n",
      "3351 [D loss: 0.999948] [G loss: 0.999904]\n",
      "3352 [D loss: 0.999890] [G loss: 0.999936]\n",
      "3353 [D loss: 0.999930] [G loss: 0.999909]\n",
      "3354 [D loss: 0.999963] [G loss: 1.000095]\n",
      "3355 [D loss: 0.999945] [G loss: 0.999860]\n",
      "3356 [D loss: 0.999967] [G loss: 1.000076]\n",
      "3357 [D loss: 0.999803] [G loss: 1.000145]\n",
      "3358 [D loss: 0.999956] [G loss: 0.999952]\n",
      "3359 [D loss: 0.999795] [G loss: 0.999776]\n",
      "3360 [D loss: 0.999906] [G loss: 1.000036]\n",
      "3361 [D loss: 0.999980] [G loss: 0.999945]\n",
      "3362 [D loss: 1.000078] [G loss: 0.999826]\n",
      "3363 [D loss: 0.999906] [G loss: 1.000004]\n",
      "3364 [D loss: 0.999831] [G loss: 1.000035]\n",
      "3365 [D loss: 0.999901] [G loss: 0.999913]\n",
      "3366 [D loss: 0.999927] [G loss: 1.000037]\n",
      "3367 [D loss: 0.999836] [G loss: 0.999878]\n",
      "3368 [D loss: 1.000004] [G loss: 0.999874]\n",
      "3369 [D loss: 0.999839] [G loss: 1.000025]\n",
      "3370 [D loss: 1.000052] [G loss: 1.000020]\n",
      "3371 [D loss: 0.999894] [G loss: 1.000084]\n",
      "3372 [D loss: 0.999895] [G loss: 0.999915]\n",
      "3373 [D loss: 0.999996] [G loss: 0.999835]\n",
      "3374 [D loss: 0.999818] [G loss: 1.000020]\n",
      "3375 [D loss: 0.999952] [G loss: 0.999994]\n",
      "3376 [D loss: 0.999959] [G loss: 1.000243]\n",
      "3377 [D loss: 0.999888] [G loss: 0.999901]\n",
      "3378 [D loss: 0.999846] [G loss: 0.999947]\n",
      "3379 [D loss: 0.999984] [G loss: 0.999898]\n",
      "3380 [D loss: 0.999828] [G loss: 1.000093]\n",
      "3381 [D loss: 0.999984] [G loss: 1.000098]\n",
      "3382 [D loss: 0.999942] [G loss: 0.999927]\n",
      "3383 [D loss: 0.999847] [G loss: 1.000050]\n",
      "3384 [D loss: 0.999903] [G loss: 1.000012]\n",
      "3385 [D loss: 0.999922] [G loss: 1.000056]\n",
      "3386 [D loss: 0.999913] [G loss: 0.999981]\n",
      "3387 [D loss: 0.999792] [G loss: 1.000050]\n",
      "3388 [D loss: 0.999828] [G loss: 0.999963]\n",
      "3389 [D loss: 0.999866] [G loss: 0.999958]\n",
      "3390 [D loss: 0.999994] [G loss: 0.999985]\n",
      "3391 [D loss: 0.999953] [G loss: 1.000037]\n",
      "3392 [D loss: 0.999820] [G loss: 0.999920]\n",
      "3393 [D loss: 0.999990] [G loss: 0.999940]\n",
      "3394 [D loss: 0.999814] [G loss: 0.999883]\n",
      "3395 [D loss: 0.999889] [G loss: 1.000020]\n",
      "3396 [D loss: 0.999968] [G loss: 1.000079]\n",
      "3397 [D loss: 0.999943] [G loss: 1.000063]\n",
      "3398 [D loss: 0.999851] [G loss: 0.999985]\n",
      "3399 [D loss: 0.999859] [G loss: 0.999968]\n",
      "3400 [D loss: 0.999903] [G loss: 1.000036]\n",
      "3401 [D loss: 0.999883] [G loss: 0.999981]\n",
      "3402 [D loss: 0.999849] [G loss: 1.000018]\n",
      "3403 [D loss: 0.999974] [G loss: 0.999974]\n",
      "3404 [D loss: 0.999854] [G loss: 1.000036]\n",
      "3405 [D loss: 0.999907] [G loss: 1.000034]\n",
      "3406 [D loss: 0.999946] [G loss: 0.999852]\n",
      "3407 [D loss: 0.999826] [G loss: 0.999992]\n",
      "3408 [D loss: 0.999994] [G loss: 0.999935]\n",
      "3409 [D loss: 1.000008] [G loss: 0.999994]\n",
      "3410 [D loss: 0.999937] [G loss: 1.000052]\n",
      "3411 [D loss: 0.999900] [G loss: 1.000079]\n",
      "3412 [D loss: 0.999946] [G loss: 1.000050]\n",
      "3413 [D loss: 0.999988] [G loss: 1.000113]\n",
      "3414 [D loss: 0.999863] [G loss: 0.999924]\n",
      "3415 [D loss: 1.000016] [G loss: 1.000108]\n",
      "3416 [D loss: 0.999875] [G loss: 1.000030]\n",
      "3417 [D loss: 0.999926] [G loss: 0.999988]\n",
      "3418 [D loss: 0.999944] [G loss: 0.999878]\n",
      "3419 [D loss: 0.999933] [G loss: 0.999955]\n",
      "3420 [D loss: 0.999948] [G loss: 1.000003]\n",
      "3421 [D loss: 0.999987] [G loss: 0.999902]\n",
      "3422 [D loss: 0.999862] [G loss: 1.000078]\n",
      "3423 [D loss: 0.999958] [G loss: 1.000075]\n",
      "3424 [D loss: 0.999938] [G loss: 1.000048]\n",
      "3425 [D loss: 0.999921] [G loss: 1.000081]\n",
      "3426 [D loss: 0.999834] [G loss: 0.999999]\n",
      "3427 [D loss: 0.999917] [G loss: 0.999999]\n",
      "3428 [D loss: 0.999861] [G loss: 0.999966]\n",
      "3429 [D loss: 0.999900] [G loss: 1.000110]\n",
      "3430 [D loss: 0.999959] [G loss: 0.999891]\n",
      "3431 [D loss: 0.999966] [G loss: 1.000072]\n",
      "3432 [D loss: 0.999927] [G loss: 0.999890]\n",
      "3433 [D loss: 0.999898] [G loss: 1.000039]\n",
      "3434 [D loss: 0.999825] [G loss: 0.999922]\n",
      "3435 [D loss: 0.999862] [G loss: 1.000021]\n",
      "3436 [D loss: 0.999889] [G loss: 0.999935]\n",
      "3437 [D loss: 0.999932] [G loss: 1.000035]\n",
      "3438 [D loss: 0.999853] [G loss: 0.999975]\n",
      "3439 [D loss: 0.999970] [G loss: 1.000036]\n",
      "3440 [D loss: 0.999980] [G loss: 1.000023]\n",
      "3441 [D loss: 0.999805] [G loss: 1.000089]\n",
      "3442 [D loss: 0.999953] [G loss: 1.000029]\n",
      "3443 [D loss: 0.999769] [G loss: 1.000001]\n",
      "3444 [D loss: 0.999931] [G loss: 0.999948]\n",
      "3445 [D loss: 0.999838] [G loss: 0.999978]\n",
      "3446 [D loss: 0.999822] [G loss: 0.999893]\n",
      "3447 [D loss: 0.999992] [G loss: 0.999938]\n",
      "3448 [D loss: 0.999981] [G loss: 0.999912]\n",
      "3449 [D loss: 0.999926] [G loss: 1.000000]\n",
      "3450 [D loss: 0.999861] [G loss: 1.000007]\n",
      "3451 [D loss: 1.000062] [G loss: 1.000056]\n",
      "3452 [D loss: 0.999880] [G loss: 1.000042]\n",
      "3453 [D loss: 0.999927] [G loss: 1.000111]\n",
      "3454 [D loss: 0.999964] [G loss: 0.999996]\n",
      "3455 [D loss: 0.999924] [G loss: 1.000007]\n",
      "3456 [D loss: 0.999883] [G loss: 1.000033]\n",
      "3457 [D loss: 0.999956] [G loss: 0.999850]\n",
      "3458 [D loss: 0.999933] [G loss: 0.999980]\n",
      "3459 [D loss: 0.999991] [G loss: 0.999939]\n",
      "3460 [D loss: 0.999983] [G loss: 0.999912]\n",
      "3461 [D loss: 0.999975] [G loss: 1.000085]\n",
      "3462 [D loss: 0.999836] [G loss: 0.999809]\n",
      "3463 [D loss: 0.999965] [G loss: 0.999918]\n",
      "3464 [D loss: 0.999794] [G loss: 1.000082]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3465 [D loss: 0.999973] [G loss: 1.000012]\n",
      "3466 [D loss: 1.000032] [G loss: 1.000040]\n",
      "3467 [D loss: 0.999894] [G loss: 0.999935]\n",
      "3468 [D loss: 0.999984] [G loss: 0.999932]\n",
      "3469 [D loss: 0.999825] [G loss: 1.000073]\n",
      "3470 [D loss: 0.999868] [G loss: 0.999937]\n",
      "3471 [D loss: 0.999933] [G loss: 0.999978]\n",
      "3472 [D loss: 0.999940] [G loss: 0.999952]\n",
      "3473 [D loss: 0.999953] [G loss: 0.999982]\n",
      "3474 [D loss: 0.999874] [G loss: 0.999917]\n",
      "3475 [D loss: 0.999924] [G loss: 1.000003]\n",
      "3476 [D loss: 0.999914] [G loss: 0.999992]\n",
      "3477 [D loss: 0.999964] [G loss: 0.999968]\n",
      "3478 [D loss: 1.000002] [G loss: 1.000092]\n",
      "3479 [D loss: 0.999901] [G loss: 0.999968]\n",
      "3480 [D loss: 0.999828] [G loss: 1.000055]\n",
      "3481 [D loss: 0.999959] [G loss: 1.000110]\n",
      "3482 [D loss: 0.999919] [G loss: 1.000142]\n",
      "3483 [D loss: 0.999961] [G loss: 1.000051]\n",
      "3484 [D loss: 0.999912] [G loss: 0.999975]\n",
      "3485 [D loss: 1.000018] [G loss: 1.000014]\n",
      "3486 [D loss: 0.999860] [G loss: 1.000086]\n",
      "3487 [D loss: 0.999896] [G loss: 0.999934]\n",
      "3488 [D loss: 0.999949] [G loss: 0.999932]\n",
      "3489 [D loss: 1.000002] [G loss: 0.999908]\n",
      "3490 [D loss: 0.999904] [G loss: 0.999988]\n",
      "3491 [D loss: 0.999946] [G loss: 1.000040]\n",
      "3492 [D loss: 0.999890] [G loss: 1.000122]\n",
      "3493 [D loss: 0.999925] [G loss: 1.000146]\n",
      "3494 [D loss: 0.999839] [G loss: 0.999896]\n",
      "3495 [D loss: 0.999907] [G loss: 0.999933]\n",
      "3496 [D loss: 0.999926] [G loss: 1.000057]\n",
      "3497 [D loss: 0.999993] [G loss: 1.000113]\n",
      "3498 [D loss: 0.999839] [G loss: 1.000145]\n",
      "3499 [D loss: 1.000007] [G loss: 0.999973]\n",
      "3500 [D loss: 0.999981] [G loss: 1.000060]\n",
      "3501 [D loss: 0.999877] [G loss: 0.999865]\n",
      "3502 [D loss: 0.999921] [G loss: 1.000154]\n",
      "3503 [D loss: 0.999773] [G loss: 1.000027]\n",
      "3504 [D loss: 0.999846] [G loss: 1.000076]\n",
      "3505 [D loss: 0.999947] [G loss: 1.000113]\n",
      "3506 [D loss: 0.999971] [G loss: 1.000110]\n",
      "3507 [D loss: 0.999905] [G loss: 0.999975]\n",
      "3508 [D loss: 0.999856] [G loss: 1.000016]\n",
      "3509 [D loss: 0.999978] [G loss: 0.999942]\n",
      "3510 [D loss: 0.999907] [G loss: 1.000064]\n",
      "3511 [D loss: 0.999924] [G loss: 1.000043]\n",
      "3512 [D loss: 0.999970] [G loss: 0.999922]\n",
      "3513 [D loss: 0.999842] [G loss: 0.999859]\n",
      "3514 [D loss: 0.999830] [G loss: 0.999954]\n",
      "3515 [D loss: 0.999841] [G loss: 1.000024]\n",
      "3516 [D loss: 0.999993] [G loss: 1.000177]\n",
      "3517 [D loss: 0.999900] [G loss: 1.000055]\n",
      "3518 [D loss: 0.999833] [G loss: 1.000046]\n",
      "3519 [D loss: 0.999974] [G loss: 1.000009]\n",
      "3520 [D loss: 0.999827] [G loss: 0.999976]\n",
      "3521 [D loss: 1.000038] [G loss: 0.999989]\n",
      "3522 [D loss: 0.999887] [G loss: 0.999961]\n",
      "3523 [D loss: 0.999932] [G loss: 0.999993]\n",
      "3524 [D loss: 0.999878] [G loss: 0.999893]\n",
      "3525 [D loss: 1.000031] [G loss: 1.000007]\n",
      "3526 [D loss: 0.999967] [G loss: 0.999978]\n",
      "3527 [D loss: 0.999891] [G loss: 1.000083]\n",
      "3528 [D loss: 0.999978] [G loss: 1.000062]\n",
      "3529 [D loss: 0.999954] [G loss: 0.999921]\n",
      "3530 [D loss: 0.999857] [G loss: 1.000043]\n",
      "3531 [D loss: 0.999911] [G loss: 0.999838]\n",
      "3532 [D loss: 0.999941] [G loss: 0.999969]\n",
      "3533 [D loss: 0.999955] [G loss: 1.000103]\n",
      "3534 [D loss: 0.999989] [G loss: 0.999897]\n",
      "3535 [D loss: 0.999892] [G loss: 0.999930]\n",
      "3536 [D loss: 0.999970] [G loss: 1.000057]\n",
      "3537 [D loss: 0.999867] [G loss: 1.000020]\n",
      "3538 [D loss: 0.999840] [G loss: 1.000003]\n",
      "3539 [D loss: 0.999878] [G loss: 0.999991]\n",
      "3540 [D loss: 0.999847] [G loss: 0.999937]\n",
      "3541 [D loss: 0.999887] [G loss: 1.000000]\n",
      "3542 [D loss: 0.999939] [G loss: 0.999936]\n",
      "3543 [D loss: 0.999890] [G loss: 1.000097]\n",
      "3544 [D loss: 0.999932] [G loss: 1.000043]\n",
      "3545 [D loss: 0.999950] [G loss: 0.999957]\n",
      "3546 [D loss: 0.999918] [G loss: 0.999871]\n",
      "3547 [D loss: 0.999997] [G loss: 0.999867]\n",
      "3548 [D loss: 0.999845] [G loss: 1.000014]\n",
      "3549 [D loss: 0.999867] [G loss: 1.000079]\n",
      "3550 [D loss: 0.999946] [G loss: 0.999964]\n",
      "3551 [D loss: 0.999991] [G loss: 0.999912]\n",
      "3552 [D loss: 0.999871] [G loss: 1.000180]\n",
      "3553 [D loss: 0.999966] [G loss: 0.999870]\n",
      "3554 [D loss: 0.999923] [G loss: 0.999906]\n",
      "3555 [D loss: 1.000024] [G loss: 1.000053]\n",
      "3556 [D loss: 0.999985] [G loss: 0.999846]\n",
      "3557 [D loss: 0.999951] [G loss: 1.000040]\n",
      "3558 [D loss: 0.999918] [G loss: 0.999988]\n",
      "3559 [D loss: 0.999929] [G loss: 1.000027]\n",
      "3560 [D loss: 0.999931] [G loss: 1.000028]\n",
      "3561 [D loss: 0.999856] [G loss: 1.000074]\n",
      "3562 [D loss: 0.999952] [G loss: 1.000075]\n",
      "3563 [D loss: 1.000019] [G loss: 0.999968]\n",
      "3564 [D loss: 0.999975] [G loss: 0.999889]\n",
      "3565 [D loss: 0.999937] [G loss: 0.999951]\n",
      "3566 [D loss: 0.999944] [G loss: 0.999937]\n",
      "3567 [D loss: 0.999977] [G loss: 0.999937]\n",
      "3568 [D loss: 0.999896] [G loss: 1.000033]\n",
      "3569 [D loss: 0.999920] [G loss: 1.000014]\n",
      "3570 [D loss: 1.000037] [G loss: 0.999917]\n",
      "3571 [D loss: 0.999974] [G loss: 1.000050]\n",
      "3572 [D loss: 0.999968] [G loss: 1.000021]\n",
      "3573 [D loss: 1.000059] [G loss: 1.000151]\n",
      "3574 [D loss: 0.999822] [G loss: 1.000088]\n",
      "3575 [D loss: 0.999927] [G loss: 0.999997]\n",
      "3576 [D loss: 0.999997] [G loss: 1.000135]\n",
      "3577 [D loss: 0.999816] [G loss: 0.999970]\n",
      "3578 [D loss: 0.999886] [G loss: 1.000041]\n",
      "3579 [D loss: 0.999950] [G loss: 1.000044]\n",
      "3580 [D loss: 1.000016] [G loss: 1.000086]\n",
      "3581 [D loss: 0.999927] [G loss: 1.000201]\n",
      "3582 [D loss: 0.999914] [G loss: 1.000102]\n",
      "3583 [D loss: 0.999939] [G loss: 1.000048]\n",
      "3584 [D loss: 0.999926] [G loss: 1.000049]\n",
      "3585 [D loss: 0.999890] [G loss: 0.999883]\n",
      "3586 [D loss: 0.999883] [G loss: 1.000139]\n",
      "3587 [D loss: 0.999957] [G loss: 1.000086]\n",
      "3588 [D loss: 0.999932] [G loss: 1.000059]\n",
      "3589 [D loss: 1.000005] [G loss: 1.000083]\n",
      "3590 [D loss: 0.999871] [G loss: 0.999965]\n",
      "3591 [D loss: 0.999864] [G loss: 0.999960]\n",
      "3592 [D loss: 0.999901] [G loss: 1.000047]\n",
      "3593 [D loss: 0.999924] [G loss: 0.999967]\n",
      "3594 [D loss: 0.999901] [G loss: 1.000000]\n",
      "3595 [D loss: 0.999911] [G loss: 1.000034]\n",
      "3596 [D loss: 0.999904] [G loss: 1.000166]\n",
      "3597 [D loss: 0.999980] [G loss: 1.000001]\n",
      "3598 [D loss: 0.999952] [G loss: 1.000009]\n",
      "3599 [D loss: 0.999972] [G loss: 1.000008]\n",
      "3600 [D loss: 0.999958] [G loss: 0.999962]\n",
      "3601 [D loss: 0.999932] [G loss: 1.000091]\n",
      "3602 [D loss: 0.999899] [G loss: 1.000029]\n",
      "3603 [D loss: 0.999927] [G loss: 1.000102]\n",
      "3604 [D loss: 0.999987] [G loss: 0.999961]\n",
      "3605 [D loss: 0.999904] [G loss: 0.999982]\n",
      "3606 [D loss: 0.999929] [G loss: 0.999945]\n",
      "3607 [D loss: 1.000064] [G loss: 1.000034]\n",
      "3608 [D loss: 0.999770] [G loss: 1.000016]\n",
      "3609 [D loss: 0.999930] [G loss: 1.000024]\n",
      "3610 [D loss: 0.999897] [G loss: 0.999898]\n",
      "3611 [D loss: 0.999895] [G loss: 1.000095]\n",
      "3612 [D loss: 0.999909] [G loss: 1.000076]\n",
      "3613 [D loss: 0.999920] [G loss: 0.999960]\n",
      "3614 [D loss: 0.999937] [G loss: 0.999968]\n",
      "3615 [D loss: 0.999911] [G loss: 1.000062]\n",
      "3616 [D loss: 0.999955] [G loss: 0.999976]\n",
      "3617 [D loss: 0.999928] [G loss: 1.000040]\n",
      "3618 [D loss: 0.999947] [G loss: 0.999985]\n",
      "3619 [D loss: 0.999841] [G loss: 1.000102]\n",
      "3620 [D loss: 0.999989] [G loss: 0.999924]\n",
      "3621 [D loss: 0.999926] [G loss: 1.000137]\n",
      "3622 [D loss: 0.999858] [G loss: 1.000024]\n",
      "3623 [D loss: 0.999973] [G loss: 0.999922]\n",
      "3624 [D loss: 0.999900] [G loss: 1.000034]\n",
      "3625 [D loss: 0.999885] [G loss: 0.999891]\n",
      "3626 [D loss: 0.999908] [G loss: 1.000063]\n",
      "3627 [D loss: 0.999890] [G loss: 1.000069]\n",
      "3628 [D loss: 0.999945] [G loss: 1.000058]\n",
      "3629 [D loss: 0.999946] [G loss: 1.000103]\n",
      "3630 [D loss: 0.999883] [G loss: 0.999995]\n",
      "3631 [D loss: 1.000020] [G loss: 0.999984]\n",
      "3632 [D loss: 0.999984] [G loss: 1.000026]\n",
      "3633 [D loss: 0.999894] [G loss: 0.999965]\n",
      "3634 [D loss: 1.000004] [G loss: 0.999970]\n",
      "3635 [D loss: 0.999917] [G loss: 1.000024]\n",
      "3636 [D loss: 0.999930] [G loss: 0.999968]\n",
      "3637 [D loss: 0.999898] [G loss: 0.999991]\n",
      "3638 [D loss: 0.999935] [G loss: 0.999962]\n",
      "3639 [D loss: 0.999966] [G loss: 1.000124]\n",
      "3640 [D loss: 0.999973] [G loss: 1.000039]\n",
      "3641 [D loss: 0.999888] [G loss: 1.000020]\n",
      "3642 [D loss: 1.000028] [G loss: 1.000088]\n",
      "3643 [D loss: 0.999922] [G loss: 0.999934]\n",
      "3644 [D loss: 0.999977] [G loss: 0.999964]\n",
      "3645 [D loss: 0.999990] [G loss: 1.000088]\n",
      "3646 [D loss: 0.999955] [G loss: 0.999980]\n",
      "3647 [D loss: 0.999917] [G loss: 0.999962]\n",
      "3648 [D loss: 0.999892] [G loss: 1.000167]\n",
      "3649 [D loss: 0.999953] [G loss: 1.000026]\n",
      "3650 [D loss: 0.999882] [G loss: 0.999964]\n",
      "3651 [D loss: 0.999846] [G loss: 1.000123]\n",
      "3652 [D loss: 0.999974] [G loss: 1.000080]\n",
      "3653 [D loss: 0.999869] [G loss: 1.000049]\n",
      "3654 [D loss: 0.999986] [G loss: 1.000004]\n",
      "3655 [D loss: 0.999930] [G loss: 0.999937]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3656 [D loss: 0.999970] [G loss: 0.999996]\n",
      "3657 [D loss: 0.999925] [G loss: 1.000040]\n",
      "3658 [D loss: 0.999976] [G loss: 0.999985]\n",
      "3659 [D loss: 0.999943] [G loss: 1.000009]\n",
      "3660 [D loss: 0.999987] [G loss: 0.999968]\n",
      "3661 [D loss: 0.999947] [G loss: 1.000038]\n",
      "3662 [D loss: 0.999980] [G loss: 1.000050]\n",
      "3663 [D loss: 0.999939] [G loss: 1.000122]\n",
      "3664 [D loss: 0.999958] [G loss: 0.999959]\n",
      "3665 [D loss: 0.999944] [G loss: 1.000043]\n",
      "3666 [D loss: 0.999922] [G loss: 1.000067]\n",
      "3667 [D loss: 1.000000] [G loss: 1.000077]\n",
      "3668 [D loss: 0.999887] [G loss: 1.000024]\n",
      "3669 [D loss: 0.999952] [G loss: 1.000055]\n",
      "3670 [D loss: 0.999996] [G loss: 1.000020]\n",
      "3671 [D loss: 0.999916] [G loss: 1.000065]\n",
      "3672 [D loss: 0.999888] [G loss: 0.999952]\n",
      "3673 [D loss: 0.999970] [G loss: 1.000139]\n",
      "3674 [D loss: 0.999900] [G loss: 1.000040]\n",
      "3675 [D loss: 1.000005] [G loss: 1.000087]\n",
      "3676 [D loss: 0.999934] [G loss: 0.999913]\n",
      "3677 [D loss: 0.999909] [G loss: 0.999994]\n",
      "3678 [D loss: 0.999983] [G loss: 1.000013]\n",
      "3679 [D loss: 0.999846] [G loss: 0.999918]\n",
      "3680 [D loss: 0.999961] [G loss: 0.999941]\n",
      "3681 [D loss: 0.999965] [G loss: 1.000011]\n",
      "3682 [D loss: 0.999890] [G loss: 1.000080]\n",
      "3683 [D loss: 0.999994] [G loss: 1.000149]\n",
      "3684 [D loss: 0.999886] [G loss: 1.000060]\n",
      "3685 [D loss: 0.999942] [G loss: 1.000080]\n",
      "3686 [D loss: 0.999990] [G loss: 1.000036]\n",
      "3687 [D loss: 0.999966] [G loss: 1.000060]\n",
      "3688 [D loss: 0.999912] [G loss: 1.000023]\n",
      "3689 [D loss: 1.000005] [G loss: 1.000011]\n",
      "3690 [D loss: 0.999988] [G loss: 1.000053]\n",
      "3691 [D loss: 0.999913] [G loss: 0.999948]\n",
      "3692 [D loss: 0.999992] [G loss: 0.999902]\n",
      "3693 [D loss: 0.999883] [G loss: 1.000040]\n",
      "3694 [D loss: 0.999970] [G loss: 1.000232]\n",
      "3695 [D loss: 0.999940] [G loss: 1.000067]\n",
      "3696 [D loss: 0.999906] [G loss: 1.000085]\n",
      "3697 [D loss: 0.999907] [G loss: 1.000088]\n",
      "3698 [D loss: 0.999925] [G loss: 1.000029]\n",
      "3699 [D loss: 0.999917] [G loss: 1.000011]\n",
      "3700 [D loss: 0.999933] [G loss: 1.000047]\n",
      "3701 [D loss: 0.999960] [G loss: 0.999971]\n",
      "3702 [D loss: 0.999863] [G loss: 0.999948]\n",
      "3703 [D loss: 0.999949] [G loss: 1.000084]\n",
      "3704 [D loss: 0.999927] [G loss: 1.000018]\n",
      "3705 [D loss: 0.999900] [G loss: 0.999989]\n",
      "3706 [D loss: 0.999958] [G loss: 1.000068]\n",
      "3707 [D loss: 0.999934] [G loss: 0.999957]\n",
      "3708 [D loss: 0.999961] [G loss: 1.000185]\n",
      "3709 [D loss: 0.999984] [G loss: 1.000079]\n",
      "3710 [D loss: 0.999972] [G loss: 1.000086]\n",
      "3711 [D loss: 0.999975] [G loss: 1.000024]\n",
      "3712 [D loss: 0.999857] [G loss: 1.000034]\n",
      "3713 [D loss: 0.999965] [G loss: 1.000044]\n",
      "3714 [D loss: 0.999933] [G loss: 1.000056]\n",
      "3715 [D loss: 0.999955] [G loss: 1.000008]\n",
      "3716 [D loss: 0.999903] [G loss: 1.000048]\n",
      "3717 [D loss: 0.999925] [G loss: 0.999973]\n",
      "3718 [D loss: 0.999934] [G loss: 0.999975]\n",
      "3719 [D loss: 0.999924] [G loss: 1.000038]\n",
      "3720 [D loss: 0.999970] [G loss: 1.000080]\n",
      "3721 [D loss: 0.999934] [G loss: 1.000035]\n",
      "3722 [D loss: 0.999913] [G loss: 1.000052]\n",
      "3723 [D loss: 0.999964] [G loss: 1.000027]\n",
      "3724 [D loss: 0.999910] [G loss: 1.000010]\n",
      "3725 [D loss: 0.999923] [G loss: 1.000102]\n",
      "3726 [D loss: 1.000001] [G loss: 1.000130]\n",
      "3727 [D loss: 0.999939] [G loss: 1.000080]\n",
      "3728 [D loss: 0.999913] [G loss: 1.000010]\n",
      "3729 [D loss: 0.999987] [G loss: 1.000003]\n",
      "3730 [D loss: 0.999982] [G loss: 0.999987]\n",
      "3731 [D loss: 0.999931] [G loss: 1.000048]\n",
      "3732 [D loss: 1.000018] [G loss: 1.000055]\n",
      "3733 [D loss: 0.999942] [G loss: 1.000078]\n",
      "3734 [D loss: 0.999921] [G loss: 1.000084]\n",
      "3735 [D loss: 0.999949] [G loss: 1.000029]\n",
      "3736 [D loss: 0.999933] [G loss: 1.000087]\n",
      "3737 [D loss: 0.999920] [G loss: 0.999958]\n",
      "3738 [D loss: 0.999896] [G loss: 1.000029]\n",
      "3739 [D loss: 0.999981] [G loss: 1.000071]\n",
      "3740 [D loss: 0.999908] [G loss: 0.999987]\n",
      "3741 [D loss: 0.999911] [G loss: 1.000072]\n",
      "3742 [D loss: 0.999956] [G loss: 1.000026]\n",
      "3743 [D loss: 0.999920] [G loss: 1.000070]\n",
      "3744 [D loss: 0.999901] [G loss: 1.000050]\n",
      "3745 [D loss: 0.999968] [G loss: 1.000088]\n",
      "3746 [D loss: 0.999969] [G loss: 0.999995]\n",
      "3747 [D loss: 0.999991] [G loss: 1.000016]\n",
      "3748 [D loss: 0.999966] [G loss: 1.000083]\n",
      "3749 [D loss: 0.999969] [G loss: 1.000062]\n",
      "3750 [D loss: 0.999990] [G loss: 1.000085]\n",
      "3751 [D loss: 0.999937] [G loss: 1.000080]\n",
      "3752 [D loss: 0.999944] [G loss: 1.000009]\n",
      "3753 [D loss: 0.999947] [G loss: 1.000023]\n",
      "3754 [D loss: 0.999925] [G loss: 1.000039]\n",
      "3755 [D loss: 0.999972] [G loss: 1.000101]\n",
      "3756 [D loss: 0.999959] [G loss: 1.000036]\n",
      "3757 [D loss: 0.999939] [G loss: 1.000036]\n",
      "3758 [D loss: 0.999943] [G loss: 1.000028]\n",
      "3759 [D loss: 0.999954] [G loss: 1.000038]\n",
      "3760 [D loss: 0.999939] [G loss: 1.000066]\n",
      "3761 [D loss: 0.999944] [G loss: 1.000073]\n",
      "3762 [D loss: 0.999951] [G loss: 1.000052]\n",
      "3763 [D loss: 0.999966] [G loss: 1.000065]\n",
      "3764 [D loss: 0.999945] [G loss: 1.000091]\n",
      "3765 [D loss: 0.999967] [G loss: 1.000020]\n",
      "3766 [D loss: 0.999917] [G loss: 1.000070]\n",
      "3767 [D loss: 0.999953] [G loss: 0.999990]\n",
      "3768 [D loss: 0.999904] [G loss: 1.000064]\n",
      "3769 [D loss: 0.999958] [G loss: 1.000060]\n",
      "3770 [D loss: 0.999963] [G loss: 1.000041]\n",
      "3771 [D loss: 0.999925] [G loss: 1.000070]\n",
      "3772 [D loss: 0.999942] [G loss: 1.000027]\n",
      "3773 [D loss: 0.999924] [G loss: 1.000069]\n",
      "3774 [D loss: 0.999989] [G loss: 1.000057]\n",
      "3775 [D loss: 0.999979] [G loss: 1.000028]\n",
      "3776 [D loss: 0.999975] [G loss: 1.000061]\n",
      "3777 [D loss: 0.999970] [G loss: 0.999963]\n",
      "3778 [D loss: 0.999980] [G loss: 1.000052]\n",
      "3779 [D loss: 0.999946] [G loss: 0.999999]\n",
      "3780 [D loss: 0.999944] [G loss: 1.000083]\n",
      "3781 [D loss: 0.999981] [G loss: 1.000051]\n",
      "3782 [D loss: 0.999964] [G loss: 1.000090]\n",
      "3783 [D loss: 0.999967] [G loss: 1.000080]\n",
      "3784 [D loss: 0.999957] [G loss: 0.999970]\n",
      "3785 [D loss: 0.999915] [G loss: 1.000065]\n",
      "3786 [D loss: 0.999962] [G loss: 0.999972]\n",
      "3787 [D loss: 0.999900] [G loss: 1.000031]\n",
      "3788 [D loss: 0.999951] [G loss: 1.000021]\n",
      "3789 [D loss: 0.999986] [G loss: 1.000072]\n",
      "3790 [D loss: 0.999959] [G loss: 0.999940]\n",
      "3791 [D loss: 0.999880] [G loss: 1.000043]\n",
      "3792 [D loss: 0.999903] [G loss: 1.000049]\n",
      "3793 [D loss: 0.999975] [G loss: 1.000087]\n",
      "3794 [D loss: 0.999906] [G loss: 1.000072]\n",
      "3795 [D loss: 0.999947] [G loss: 1.000081]\n",
      "3796 [D loss: 0.999971] [G loss: 1.000047]\n",
      "3797 [D loss: 0.999929] [G loss: 1.000040]\n",
      "3798 [D loss: 0.999915] [G loss: 1.000013]\n",
      "3799 [D loss: 0.999943] [G loss: 1.000019]\n",
      "3800 [D loss: 0.999928] [G loss: 0.999983]\n",
      "3801 [D loss: 0.999971] [G loss: 1.000031]\n",
      "3802 [D loss: 0.999907] [G loss: 1.000045]\n",
      "3803 [D loss: 0.999925] [G loss: 1.000029]\n",
      "3804 [D loss: 0.999996] [G loss: 1.000061]\n",
      "3805 [D loss: 0.999989] [G loss: 1.000072]\n",
      "3806 [D loss: 0.999934] [G loss: 1.000100]\n",
      "3807 [D loss: 0.999930] [G loss: 1.000041]\n",
      "3808 [D loss: 0.999981] [G loss: 1.000045]\n",
      "3809 [D loss: 0.999963] [G loss: 1.000035]\n",
      "3810 [D loss: 0.999955] [G loss: 1.000009]\n",
      "3811 [D loss: 0.999985] [G loss: 1.000007]\n",
      "3812 [D loss: 0.999950] [G loss: 1.000095]\n",
      "3813 [D loss: 0.999944] [G loss: 1.000065]\n",
      "3814 [D loss: 0.999921] [G loss: 1.000021]\n",
      "3815 [D loss: 0.999932] [G loss: 0.999989]\n",
      "3816 [D loss: 0.999932] [G loss: 1.000048]\n",
      "3817 [D loss: 0.999945] [G loss: 1.000069]\n",
      "3818 [D loss: 0.999941] [G loss: 1.000098]\n",
      "3819 [D loss: 0.999922] [G loss: 1.000063]\n",
      "3820 [D loss: 0.999951] [G loss: 1.000053]\n",
      "3821 [D loss: 0.999944] [G loss: 1.000055]\n",
      "3822 [D loss: 0.999970] [G loss: 1.000012]\n",
      "3823 [D loss: 0.999961] [G loss: 1.000075]\n",
      "3824 [D loss: 0.999949] [G loss: 1.000077]\n",
      "3825 [D loss: 0.999987] [G loss: 1.000139]\n",
      "3826 [D loss: 0.999890] [G loss: 1.000042]\n",
      "3827 [D loss: 0.999938] [G loss: 1.000065]\n",
      "3828 [D loss: 0.999977] [G loss: 1.000095]\n",
      "3829 [D loss: 0.999927] [G loss: 1.000007]\n",
      "3830 [D loss: 0.999928] [G loss: 1.000101]\n",
      "3831 [D loss: 0.999949] [G loss: 1.000032]\n",
      "3832 [D loss: 0.999959] [G loss: 1.000030]\n",
      "3833 [D loss: 0.999936] [G loss: 1.000068]\n",
      "3834 [D loss: 0.999950] [G loss: 1.000012]\n",
      "3835 [D loss: 0.999898] [G loss: 1.000058]\n",
      "3836 [D loss: 0.999933] [G loss: 1.000056]\n",
      "3837 [D loss: 0.999966] [G loss: 1.000110]\n",
      "3838 [D loss: 0.999928] [G loss: 1.000023]\n",
      "3839 [D loss: 0.999999] [G loss: 1.000109]\n",
      "3840 [D loss: 0.999984] [G loss: 1.000017]\n",
      "3841 [D loss: 0.999860] [G loss: 1.000054]\n",
      "3842 [D loss: 0.999945] [G loss: 0.999986]\n",
      "3843 [D loss: 0.999980] [G loss: 1.000016]\n",
      "3844 [D loss: 0.999936] [G loss: 0.999992]\n",
      "3845 [D loss: 0.999971] [G loss: 1.000084]\n",
      "3846 [D loss: 0.999968] [G loss: 1.000020]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3847 [D loss: 0.999922] [G loss: 1.000047]\n",
      "3848 [D loss: 0.999981] [G loss: 1.000027]\n",
      "3849 [D loss: 0.999924] [G loss: 1.000010]\n",
      "3850 [D loss: 0.999920] [G loss: 1.000069]\n",
      "3851 [D loss: 0.999943] [G loss: 1.000041]\n",
      "3852 [D loss: 0.999930] [G loss: 1.000026]\n",
      "3853 [D loss: 0.999903] [G loss: 1.000052]\n",
      "3854 [D loss: 0.999917] [G loss: 1.000085]\n",
      "3855 [D loss: 0.999942] [G loss: 1.000090]\n",
      "3856 [D loss: 0.999934] [G loss: 1.000100]\n",
      "3857 [D loss: 0.999919] [G loss: 0.999993]\n",
      "3858 [D loss: 0.999986] [G loss: 1.000063]\n",
      "3859 [D loss: 0.999943] [G loss: 1.000007]\n",
      "3860 [D loss: 0.999976] [G loss: 1.000053]\n",
      "3861 [D loss: 0.999939] [G loss: 1.000082]\n",
      "3862 [D loss: 0.999966] [G loss: 0.999970]\n",
      "3863 [D loss: 0.999948] [G loss: 1.000081]\n",
      "3864 [D loss: 0.999970] [G loss: 1.000037]\n",
      "3865 [D loss: 0.999915] [G loss: 1.000083]\n",
      "3866 [D loss: 0.999975] [G loss: 1.000052]\n",
      "3867 [D loss: 0.999909] [G loss: 1.000059]\n",
      "3868 [D loss: 0.999964] [G loss: 1.000035]\n",
      "3869 [D loss: 0.999969] [G loss: 1.000038]\n",
      "3870 [D loss: 0.999906] [G loss: 0.999993]\n",
      "3871 [D loss: 0.999974] [G loss: 0.999939]\n",
      "3872 [D loss: 0.999959] [G loss: 1.000030]\n",
      "3873 [D loss: 0.999916] [G loss: 1.000103]\n",
      "3874 [D loss: 0.999952] [G loss: 1.000007]\n",
      "3875 [D loss: 0.999930] [G loss: 1.000078]\n",
      "3876 [D loss: 0.999967] [G loss: 1.000027]\n",
      "3877 [D loss: 0.999928] [G loss: 1.000029]\n",
      "3878 [D loss: 0.999928] [G loss: 1.000043]\n",
      "3879 [D loss: 0.999962] [G loss: 1.000028]\n",
      "3880 [D loss: 0.999946] [G loss: 1.000034]\n",
      "3881 [D loss: 0.999958] [G loss: 1.000028]\n",
      "3882 [D loss: 0.999963] [G loss: 1.000064]\n",
      "3883 [D loss: 0.999950] [G loss: 1.000124]\n",
      "3884 [D loss: 0.999981] [G loss: 1.000049]\n",
      "3885 [D loss: 0.999915] [G loss: 1.000015]\n",
      "3886 [D loss: 0.999953] [G loss: 1.000062]\n",
      "3887 [D loss: 0.999943] [G loss: 1.000030]\n",
      "3888 [D loss: 0.999956] [G loss: 1.000092]\n",
      "3889 [D loss: 0.999930] [G loss: 1.000007]\n",
      "3890 [D loss: 0.999969] [G loss: 1.000063]\n",
      "3891 [D loss: 0.999950] [G loss: 1.000073]\n",
      "3892 [D loss: 0.999923] [G loss: 1.000063]\n",
      "3893 [D loss: 0.999948] [G loss: 1.000054]\n",
      "3894 [D loss: 0.999919] [G loss: 1.000028]\n",
      "3895 [D loss: 0.999975] [G loss: 1.000040]\n",
      "3896 [D loss: 0.999945] [G loss: 1.000071]\n",
      "3897 [D loss: 0.999998] [G loss: 1.000037]\n",
      "3898 [D loss: 0.999949] [G loss: 1.000013]\n",
      "3899 [D loss: 0.999958] [G loss: 1.000060]\n",
      "3900 [D loss: 0.999949] [G loss: 1.000023]\n",
      "3901 [D loss: 0.999959] [G loss: 1.000026]\n",
      "3902 [D loss: 0.999977] [G loss: 1.000060]\n",
      "3903 [D loss: 0.999935] [G loss: 1.000066]\n",
      "3904 [D loss: 0.999928] [G loss: 1.000068]\n",
      "3905 [D loss: 0.999954] [G loss: 1.000048]\n",
      "3906 [D loss: 0.999939] [G loss: 1.000042]\n",
      "3907 [D loss: 0.999984] [G loss: 1.000072]\n",
      "3908 [D loss: 0.999960] [G loss: 1.000012]\n",
      "3909 [D loss: 0.999955] [G loss: 1.000016]\n",
      "3910 [D loss: 0.999897] [G loss: 1.000074]\n",
      "3911 [D loss: 0.999964] [G loss: 1.000050]\n",
      "3912 [D loss: 0.999945] [G loss: 1.000063]\n",
      "3913 [D loss: 0.999952] [G loss: 1.000061]\n",
      "3914 [D loss: 0.999942] [G loss: 1.000028]\n",
      "3915 [D loss: 0.999952] [G loss: 1.000041]\n",
      "3916 [D loss: 0.999985] [G loss: 1.000050]\n",
      "3917 [D loss: 0.999973] [G loss: 1.000040]\n",
      "3918 [D loss: 0.999970] [G loss: 1.000039]\n",
      "3919 [D loss: 0.999957] [G loss: 1.000114]\n",
      "3920 [D loss: 0.999963] [G loss: 1.000082]\n",
      "3921 [D loss: 0.999926] [G loss: 1.000104]\n",
      "3922 [D loss: 0.999988] [G loss: 1.000049]\n",
      "3923 [D loss: 0.999972] [G loss: 1.000038]\n",
      "3924 [D loss: 0.999953] [G loss: 1.000026]\n",
      "3925 [D loss: 0.999973] [G loss: 1.000046]\n",
      "3926 [D loss: 0.999947] [G loss: 1.000068]\n",
      "3927 [D loss: 0.999982] [G loss: 1.000070]\n",
      "3928 [D loss: 0.999979] [G loss: 1.000058]\n",
      "3929 [D loss: 0.999958] [G loss: 1.000100]\n",
      "3930 [D loss: 0.999963] [G loss: 1.000076]\n",
      "3931 [D loss: 0.999980] [G loss: 1.000062]\n",
      "3932 [D loss: 0.999966] [G loss: 1.000034]\n",
      "3933 [D loss: 0.999961] [G loss: 1.000048]\n",
      "3934 [D loss: 0.999949] [G loss: 1.000086]\n",
      "3935 [D loss: 0.999941] [G loss: 1.000063]\n",
      "3936 [D loss: 0.999956] [G loss: 1.000065]\n",
      "3937 [D loss: 0.999978] [G loss: 1.000055]\n",
      "3938 [D loss: 0.999964] [G loss: 1.000071]\n",
      "3939 [D loss: 0.999963] [G loss: 1.000059]\n",
      "3940 [D loss: 0.999995] [G loss: 1.000089]\n",
      "3941 [D loss: 0.999952] [G loss: 1.000055]\n",
      "3942 [D loss: 0.999956] [G loss: 1.000092]\n",
      "3943 [D loss: 0.999971] [G loss: 1.000059]\n",
      "3944 [D loss: 0.999974] [G loss: 1.000065]\n",
      "3945 [D loss: 0.999956] [G loss: 1.000043]\n",
      "3946 [D loss: 0.999946] [G loss: 1.000028]\n",
      "3947 [D loss: 0.999993] [G loss: 1.000030]\n",
      "3948 [D loss: 0.999967] [G loss: 1.000040]\n",
      "3949 [D loss: 0.999955] [G loss: 1.000043]\n",
      "3950 [D loss: 0.999955] [G loss: 0.999973]\n",
      "3951 [D loss: 0.999929] [G loss: 1.000038]\n",
      "3952 [D loss: 0.999963] [G loss: 1.000025]\n",
      "3953 [D loss: 0.999984] [G loss: 1.000042]\n",
      "3954 [D loss: 0.999958] [G loss: 1.000036]\n",
      "3955 [D loss: 0.999920] [G loss: 1.000055]\n",
      "3956 [D loss: 0.999951] [G loss: 1.000042]\n",
      "3957 [D loss: 0.999950] [G loss: 1.000034]\n",
      "3958 [D loss: 0.999932] [G loss: 1.000050]\n",
      "3959 [D loss: 0.999932] [G loss: 1.000049]\n",
      "3960 [D loss: 0.999953] [G loss: 1.000031]\n",
      "3961 [D loss: 0.999952] [G loss: 1.000051]\n",
      "3962 [D loss: 0.999916] [G loss: 1.000034]\n",
      "3963 [D loss: 0.999985] [G loss: 1.000035]\n",
      "3964 [D loss: 0.999978] [G loss: 1.000038]\n",
      "3965 [D loss: 0.999954] [G loss: 1.000028]\n",
      "3966 [D loss: 0.999941] [G loss: 1.000042]\n",
      "3967 [D loss: 0.999939] [G loss: 1.000023]\n",
      "3968 [D loss: 0.999965] [G loss: 1.000016]\n",
      "3969 [D loss: 0.999913] [G loss: 1.000021]\n",
      "3970 [D loss: 0.999956] [G loss: 1.000032]\n",
      "3971 [D loss: 0.999932] [G loss: 1.000082]\n",
      "3972 [D loss: 0.999954] [G loss: 1.000000]\n",
      "3973 [D loss: 0.999943] [G loss: 1.000055]\n",
      "3974 [D loss: 0.999933] [G loss: 1.000080]\n",
      "3975 [D loss: 0.999944] [G loss: 1.000047]\n",
      "3976 [D loss: 0.999903] [G loss: 1.000053]\n",
      "3977 [D loss: 0.999947] [G loss: 1.000046]\n",
      "3978 [D loss: 0.999934] [G loss: 1.000058]\n",
      "3979 [D loss: 0.999922] [G loss: 1.000038]\n",
      "3980 [D loss: 0.999969] [G loss: 1.000003]\n",
      "3981 [D loss: 0.999925] [G loss: 1.000038]\n",
      "3982 [D loss: 0.999993] [G loss: 1.000030]\n",
      "3983 [D loss: 0.999969] [G loss: 1.000071]\n",
      "3984 [D loss: 0.999948] [G loss: 1.000040]\n",
      "3985 [D loss: 0.999948] [G loss: 1.000095]\n",
      "3986 [D loss: 0.999967] [G loss: 1.000042]\n",
      "3987 [D loss: 0.999958] [G loss: 1.000091]\n",
      "3988 [D loss: 0.999961] [G loss: 0.999978]\n",
      "3989 [D loss: 0.999957] [G loss: 1.000072]\n",
      "3990 [D loss: 0.999977] [G loss: 1.000048]\n",
      "3991 [D loss: 0.999961] [G loss: 1.000026]\n",
      "3992 [D loss: 0.999920] [G loss: 1.000086]\n",
      "3993 [D loss: 0.999942] [G loss: 1.000078]\n",
      "3994 [D loss: 0.999945] [G loss: 1.000031]\n",
      "3995 [D loss: 0.999946] [G loss: 1.000049]\n",
      "3996 [D loss: 0.999952] [G loss: 1.000040]\n",
      "3997 [D loss: 0.999928] [G loss: 1.000043]\n",
      "3998 [D loss: 0.999947] [G loss: 0.999993]\n",
      "3999 [D loss: 0.999948] [G loss: 1.000094]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class WGAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        # Following parameter and optimizer set as recommended in paper\n",
    "        self.n_critic = 5\n",
    "        self.clip_value = 0.01\n",
    "        optimizer = RMSprop(lr=0.00005)\n",
    "\n",
    "        # Build and compile the critic\n",
    "        self.critic = self.build_critic()\n",
    "        self.critic.compile(loss=self.wasserstein_loss,\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.critic.trainable = False\n",
    "\n",
    "        # The critic takes generated images as input and determines validity\n",
    "        valid = self.critic(img)\n",
    "\n",
    "        # The combined model  (stacked generator and critic)\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss=self.wasserstein_loss,\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=4, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=4, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=4, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_critic(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = -np.ones((batch_size, 1))\n",
    "        fake = np.ones((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for _ in range(self.n_critic):\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Select a random batch of images\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "                \n",
    "                # Sample noise as generator input\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "                # Generate a batch of new images\n",
    "                gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "                # Train the critic\n",
    "                d_loss_real = self.critic.train_on_batch(imgs, valid)\n",
    "                d_loss_fake = self.critic.train_on_batch(gen_imgs, fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
    "\n",
    "                # Clip critic weights\n",
    "                for l in self.critic.layers:\n",
    "                    weights = l.get_weights()\n",
    "                    weights = [np.clip(w, -self.clip_value, self.clip_value) for w in weights]\n",
    "                    l.set_weights(weights)\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, 1 - d_loss[0], 1 - g_loss[0]))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    wgan = WGAN()\n",
    "    wgan.train(epochs=4000, batch_size=32, sample_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/charming/Python/0_Paper_Review/08. From GAN to WGAN'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "path2 = os.getcwd()\n",
    "path2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_image_array = [imageio.imread(generated_image) for generated_image in f]\n",
    "imageio.mimsave(path + 'WGAN_MNIST2.gif', generated_image_array, fps=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
